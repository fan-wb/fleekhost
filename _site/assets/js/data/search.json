[
  
  {
    "title": "Avalanche9000 升级总览",
    "url": "/posts/ava9000/",
    "categories": "Blockchain",
    "tags": "l1, avalanche",
    "date": "2024-12-19 12:00:00 +0800",
    





    
    "snippet": "在当前市场周期中，智能合约平台板块整体呈现强势上涨态势，多个项目代币价格创下历史新高，但作为头部平台之一的 Avalanche 却始终表现平平。不过该平台即将迎来的具有战略意义的协议升级—— Avalanche9000 有望改变这一局面。该升级在可扩展性、成本效率以及去中心化程度等核心维度均展现出突破性创新，或将重塑行业竞争格局。本文将重点剖析此次升级的关键亮点，包括验证者运营成本的显著降低...",
    "content": "在当前市场周期中，智能合约平台板块整体呈现强势上涨态势，多个项目代币价格创下历史新高，但作为头部平台之一的 Avalanche 却始终表现平平。不过该平台即将迎来的具有战略意义的协议升级—— Avalanche9000 有望改变这一局面。该升级在可扩展性、成本效率以及去中心化程度等核心维度均展现出突破性创新，或将重塑行业竞争格局。本文将重点剖析此次升级的关键亮点，包括验证者运营成本的显著降低、L1 的完全主权化等，并就其对 Avalanche 生态系统的潜在影响进行探讨。图1 AVAX代币价格表现 | 图源：CoinGeckoAvalanche 简介作为全球市值第六大的智能合约平台，Avalanche 凭借其创新的多链架构设计在 Layer 1 赛道中确立了独特优势。该架构由三条专门化主链构成，使 Avalanche 有效实现了功能分层并显著提升了可扩展性：  C-Chain：负责处理智能合约（EVM 兼容）  P-Chain：负责验证者和质押管理  X-Chain：处理资产转移图2 Avalanche 网络架构 | 图源：Avalanche Docs依靠这种架构，Avalanche 构建了作为其核心竞争壁垒的子网模型。子网是可定制化的独立网络，既能保持自主运营能力，又能继承主网的安全性与可扩展性，为机构级应用提供了理想的基础设施选择。例如专注于机构与企业场景的 Evergreen 子网，能够提供可控的私有环境，使组织机构可以在符合合规性和安全要求的条件下使用区块链技术。摩根大通与 Apollo Global 合作于 2023 年 11 月在 Avalanche 上部署了 Kinexys（原 Onyx）子网，提供外汇交易服务，全球客户年增长率达1000%。Spruce 子网也成功吸引 T. Rowe Price Associates、WisdomTree、Wellington Management 和 Cumberland 等顶级 TradFi 机构入驻。除金融领域外，Avalanche 在多个垂直领域均实现突破：德勤使用 Avalanche 构建了联邦灾害赔偿平台，以提升理赔效率与透明度；GUNZ 子网的游戏《Off the Grid》在 30 天内吸引了 1000 万玩家创建钱包，展现出巨大的用户增长潜力。Avalanche 的技术架构在实现子网间原生互操作性的同时，通过与以太坊的完全兼容性（所有智能合约交易均通过 EVM 兼容的 C-Chain 执行），带来了强大的网络效应。这一战略选择得到了市场验证：当前 Avalanche 是全球总锁定价值第十的区块链，TVL 达 16 亿美元，凸显出其生态的活跃度与用户信任度。图3 Avalanche TVL | 图源：ArtemisAvalanche 的原生代币 AVAX 在网络中充当交换媒介，用于各种交易。用户可以通过质押 AVAX 来保护生态系统，并支付交易费用。此外，AVAX 也用作治理代币，节点质押的 AVAX 数量与其在网络决策中的投票权成正比。现状优势前文提到 Avalanche 的多链架构是其核心竞争力之一，但市场上也存在采用类似架构的其他平台，与之相较，Avalanche 又有哪些优势呢？在区块链基础设施赛道具备三大核心竞争优势：网络性质的可定制性Arbitrum Orbit、Optimism OpStack 等 Layer 2 解决方案，以及 Polkadot、Cosmos 等 Layer 0 网络普遍要求构建与主链互联的公共网络，相较之下，Avalanche 子网在隐私保护与互操作性之间实现了独特的平衡。企业可基于业务需求灵活选择构建私有链或公链，通过 P-Chain 锚定机制确保安全性，能满足严格的保密需求，同时还可选择与其他链进行交互。该架构的市场认可度显著：目前已孵化近 150 个子网，其中 42 个展现出较高的商业活跃度。图4 Avalanche 子网类别 | 图源：SnowPeer治理、安全模型和操作系统的可定制性子网架构为企业级应用提供了全方位的定制空间，企业可根据自身的业务场景和运营需求调灵活整治理机制及安全模型，如游戏应用与大型金融平台可采用不同级别的安全措施，满足不同的安全性期望。此外，除原生的高性能 Avalanche 虚拟机 (AVM) 外，企业还可部署定制化 VM 以满足特定应用场景需求。合规导向的设计理念在监管合规方面，Avalanche 子网在网络层面支持 KYC/AML 流程的集成，还可原生实现地理围栏、白名单等合规必需功能。子网能够构建符合特定监管要求的许可环境，同时还可利用区块链技术的优势，从而激发了 Avalanche 上的多样化实验。这种以合规为导向的架构设计，突破了传统公链生态系统中以代币化为核心的发展路径，为更广泛的商业应用探索提供了可能性。挑战2024 年，区块链行业格局出现了重要转变。Avalanche 等传统 L1 公链的主导地位受到挑战，市场焦点逐步转向 Sui、Aptos 和 TON 等号称“Solana 杀手”的新生力量。这些新兴公链凭借技术创新与优秀的用户体验获得了市场的认可。  TON 与 Telegram 的深度整合使其在生态建设方面具备了独特优势。依托 Telegram 9 亿的用户基础，TON 构建了集加密支付、小程序及去中心化服务为一体的类微信应用生态。  Sui 和 Aptos 采用 Move 语言进行底层架构创新，在性能方面取得突破性进展，理论吞吐量突破 12 万 TPS，在显著提升可扩展性的同时，也有效规避了传统区块链编程语言的安全隐患。2024年，Rollup 即服务 (RaaS) 赛道也呈现快速发展态势。Gelato、Conduit 和 AltLayer 等服务提供商通过简化 L2 部署流程，降低了开发者进入 Arbitrum 和 Optimism 等网络的门槛。Dencun 升级显著降低了 L2 交易成本，增强了其相对替代 L1 的竞争力。但 L2 生态的快速扩张也导致了流动性分散，生态系统整合需求上升，可能促使用户回流至 Avalanche 这类基础设施完善的成熟公链。链上指标Avalanche 的网络架构发展轨迹与以太坊的扩容路径相似，均经历了从单体架构向分布式架构的战略转型。Avalanche 子网作为独立运行的区块链网络，在功能定位上与以太坊生态中的  L2 方案类似，为高吞吐量应用提供了专属的运行环境，有效降低了主链的负载压力。子网战略虽然在技术层面取得了成效，但同时也可能导致网络价值分散化趋势加剧，流向主链的经济利益减少，手续费收入出现结构性下降。这也凸显了区块链扩容的共同挑战：推动整体生态系统的持续扩张与创新的同时，维持主链的经济活力与价值捕获能力。如下所示，主网 gas 费的显著下降趋势显示了 Avalanche 生态系统的演进方向。图5 Avalanche C-Chain 平均交易费用 | 图源：Artemis虽然 C-Chain 上的直接收入有所下降，但 Avalanche 的生态通过子网架构实现了扩张，为其经济注入了新的活力。交易量数据显示出强劲的增长势头：每日交易量激增，月度交易量创历史第三高。这主要是因为 Avalanche 在其子网生态中扮演着最终结算层的角色，类似于以太坊在其 L2 扩容方案中的定位。图6 Avalanche 月度交易量 | 图源：Avalanche Stats尽管如下图所示，C-Chain 上的总用户数仍低于其 2021 年的峰值，但该指标并不能完全反映网络利用率和用户参与度。Avalanche 150 个子网日均贡献约 10 万活跃用户，因而整个生态总日活用户数至少在 15 万以上。图7 Avalanche C-Chain 日活跃地址数 | 图源：Artemis如图 8 所示，主网用户数下降的同时，结算价值显著提升，可见留存的用户群体更加活跃，用户行为有明显的高价值化趋势。这一现象说明 Avalanche 的经济活动正呈现整合态势，某些 Dapp 可能已实现产品与市场的有效匹配，为用户参与度和采用的潜在复苏奠定了基础。其中贝莱德的代币化货币市场基金 BUIDL 以及 Franklin Templeton 4.2 亿美元的链上美国政府货币基金 BENJI 登录 Avalanche 主网是重要标志。图8 Avalanche C-Chain 结算量 | 图源：Artemis如图 9 所示，自 2021 年高峰期以来，Solana 和以太坊 L2 依靠 memecoin 投机活动吸引了大量个人用户，使得 Avalanche 的 DeFi 活跃度有所下降。但即将到来的 9000 升级将显著降低交易成本，提升 DeFi 专用链的运营效率，有望重振 Avalanche 的 DeFi 生态。图9 Avalanche C-Chain DEX 交易量 | 图源：Artemis此外，有迹象表明，该网络在稳定币领域也发挥着越来越重要的作用。如下图所示，P2P 稳定币交易量总体上始终保持稳步增长，持续在为 Avalanche 生态注入流动性。增长背后有多种原因，其中 BOOST 等活动在吸引流动性方面发挥了关键作用。这些活动通过提供 AVAX 代币奖励显著增加了生态中稳定币的供应量，推动了资本流入和整体增长。图10 Avalanche C-Chain P2P 稳定币交易量 | 图源：ArtemisAvalanche9000 升级于 2024 年 11 月 25 日在 Fuji 测试网上线，并于 2024 年 12 月 16 日集成到了 Etna 主网中。核心问题与解决方案验证者在 Avalanche 生态系统中起着至关重要的作用，现有架构允许验证者同时参与主网络及其选定子网的验证，这一设计初衷是为了实现 Avalanche 的横向扩展。然而，实践表明，这种架构在释放新机遇的同时也带来了显著挑战，影响了生态系统的可扩展性、经济效率及采用程度。下文将列举 Avalanche 当前遇到的问题，以及 9000 升级相应的解决方案。  高资本门槛：当前验证者模型面临的首要瓶颈在于其较高的资本门槛。验证者需要在主网质押 2000 AVAX（约合 10 万美元，以 AVAX 币价 50 美元计），此外还需承担子网特定的质押要求。高额的财务门槛极大地限制了中小型项目及创新者的参与度，从而制约了 Avalanche 生态的整体发展。          解决方案：基于订阅的验证者模型。Avalanche9000 通过 ACP-77 提案引入了基于订阅的验证者模型，将巨额的初始质押转变为每月1-10 AVAX 的灵活订阅机制。这种按需付费模式大幅降低了准入门槛，同时借助连续 P-Chain 费用机制优化了子网管理的经济性，为各类规模的项目打开了构建独立 L1 的机会。        高运营成本：运营成本高企是现有模型的另一重要挑战。验证者需同时维护主网络和子网运营，这不仅显著提升了硬件要求，还推高了整体运营成本。在高额质押要求的压力下，大量验证者转向采用 AWS 等中心化托管服务，AWS 目前托管了 38.61% 的 Avalanche 节点。这种中心化趋势对网络安全性与去中心化理念造成了潜在威胁。          解决方案：独立 L1 验证。Avalanche9000 提出了独立 L1 验证机制，通过 ValidatorManager 智能合约赋予 L1 创建者更大的验证者管理自主权。 L1 创建者可建立自己的去中心化验证者管理系统，网络规则也可按需自定义。这一升级不仅简化了系统架构，还有效降低了节点的硬件门槛与运营支出，从而推动建立更具分散性、韧性与经济可持续性的验证者生态。      图11 Avalanche9000 实施前后的硬件要求和运营成本对比 | 图源：Avalanche  合规障碍：在 Avalanche 现有架构下，合规性问题成为制约机构级应用落地的重要因素。尽管子网可配置为私有许可链模式运营，但验证者仍需在公共无许可的主网中保持参与度，这一要求与许多机构对完全隔离环境的合规诉求相矛盾。虽然业界已开发出主网部分同步等解决方案，但这些方案的完整性与成熟度尚未达到企业级应用的标准。          解决方案：独立 L1 验证。Avalanche9000 通过 ACP-77 提案提出的新架构支持 L1 完全独立于主网运行，验证者可以专注于特定 L1 的维护而无需参与主网活动，从而为机构用户提供了严格意义上的网络隔离能力，满足其合规与运营需求。值得注意的是，这种独立性并未割裂生态系统的整体性，各个 L1 仍然通过跨链消息传递 (ICM) 机制保持互联互通，在确保必要隔离的同时实现跨链通信与流动性共享。        缺少故障隔离：在可靠性层面，当前子网验证者对主网的依赖也带来了挑战。主网的拥堵或故障势必波及子网运营，这对追求高可用性的关键应用造成了潜在威胁。          解决方案：完全故障隔离。Avalanche9000 通过将 L1 验证者与主网解耦，在架构层面实现了完全的故障隔离。每个 L1 的独立运营确保了其性能不受主网络波动影响，为机构金融、实时支付等高风险场景提供了强有力的可靠性保障。      其他升级除上述的 ACP-77 以外，Avalanche9000 的 Etna 升级还包含多项  ACP：  ACP-125：在优化网络经济性方面，Avalanche9000 借助 ACP-125 提案取得了突破性进展。通过将 C-Chain 基准 gas 费从 25 nAVAX 大幅下调至 1 nAVAX，交易成本实现了 96% 的显著降幅，Avalanche 社区成员创建了 Dune 仪表盘模拟 ACP-125，如下图所示。这一改进极大地降低了平台的使用门槛，为开发者与用户创造了更具吸引力的经济环境，有望加速生态的扩张与繁荣。图12 ACP-125 实施前后的 gas 费对比 | 图源：Dune      ACP-103：继 C-Chain 动态费用模型取得成功后，ACP-103 提案计划将其应用于 P-Chain 和 X-Chain，通过需求导向的定价机制优化网络资源配置效率。当前的固定费用体系难以反映实时网络负载状况，在高峰期可能导致资源争用，在低谷期则存在定价过高问题。    ACP-103 提出的动态费用方案将实现 gas 价格与网络负载的联动调整。从技术架构来看，ACP-103 采用多维度计量模型评估资源消耗，包括交易使用的网络带宽 (B)、数据库的读/写频次 (R/W) 以及计算时间 (C) 四个核心指标。这些维度的综合计算 gas 消耗，费用则根据网络整体的 gas 消耗量动态调整。值得关注的是，该方案预留了向多维费用模型演进的空间，未来有望实现对不同资源类型的独立定价，为更精细化的费用分配管理奠定基础。    总体而言，ACP-103 的实施将进一步增强平台的性能与可扩展性。通过建立更灵活的费用调节机制，平台可以更有效地应对交易需求波动，降低拥堵风险，同时提供更优质的用户体验。这一改进对于吸引机构与企业用户具有重要意义。        ACP-113：ACP-113 提案聚焦于解决 Avalanche 平台在可验证随机性生成方面的技术瓶颈，旨在为智能合约提供非密码学随机数种子的原生支持，这点对于游戏、彩票等依赖随机性的 Dapp 至关重要。当前区块链系统普遍面临随机性实现的挑战。由于区块链固有的确定性执行特征，相同输入必然产生相同输出，这一特性虽然保证了交易的可预测性，但也为随机数生成带来了天然障碍。在缺乏可靠外部随机源（如预言机）的情况下，智能合约难以实现真正意义上的公平随机性。    具体到 Avalanche 生态，L1 及其 EVM 兼容的智能合约环境目前尚未具备安全可验证的随机数生成能力，这一局限显著制约了平台的应用场景拓展。ACP-113 正是针对这一痛点，提出在智能合约层面引入可验证随机数种子生成机制。该方案不仅能够满足应用对随机性的需求，更重要的是通过验证机制的设计确保了随机过程的公平性与透明度。    ACP-113 提案在随机性生成机制设计上采用了 BLS 签名与可验证随机函数 (VRF) 的组合方案，实现了难以预测且可验证的随机性生成过程。            基于 BLS 的 VRF：区块提议者使用其 BLS 密钥对前序区块信息的签名生成随机种子，用于在后续区块中生成随机性。这一过程在密码学上是安全且可验证的，因此可以认为随机种子是无偏且可靠的。              递归签名：系统通过递归签名链接机制，将前一区块的签名传递用于后续区块的随机性生成，在维持随机值完整性的同时建立起连贯的验证体系。              启动机制：为了应对区块提议者没有 BLS 密钥等特殊场景，该方案还设计了使用预定义种子的启动机制，确保了随机性生成过程的连续性与稳定性。        ACP-113 提供了如下可视化说明。左侧是区块 n，在右侧是区块 n+1，每个区块都包含一个标记为 VRF-Sig 的值，代表该区块生成的 VRF 签名。随机性过程的输出由 VRF-Out(n) 表示，由对区块 n 的 VRF-Sig 进行哈希计算得出。该机制为去中心化应用提供了安全的随机性，同时确保了随机性不会被篡改。  \t+-----------------------+           +-----------------------+\t|  Block n              | &lt;-------- |  Block n+1            |\t+-----------------------+           +-----------------------+\t|  VRF-Sig(n)           |           |  VRF-Sig(n+1)         |\t|  ...                  |           |  ...                  |\t+-----------------------+           +-----------------------+\t\t\t\t+-----------------------+           +-----------------------+\t|  VM n                 |           |  VM n+1               |\t+-----------------------+           +-----------------------+\t|  VRF-Out(n)           |           |  VRF-Out(n+1)         |\t+-----------------------+           +-----------------------+\t\t\t\tVRF-Sig(n+1) = Sign(VRF-Sig(n), Block n+1 proposer's BLS key)\t\t\tVRF-Out(n) = Hash(VRF-Sig(n))        ACP-20：ACP-20 提案建议引入更轻量高效的 Ed25519 密钥替代现有的 RSA 或 ECDSA 密钥体系，提升 Avalanche 通信基础设施安全性的同时优化系统架构的复杂度与运行效率。    Ed25519 公钥仅 32 字节，签名 64 字节，相较于 RSA 或 ECDSA 的大尺寸密钥，这一改变显著降低了生成 NodeID 和进行 P2P 通信所需的带宽。同时，通过支持 TLS 证书的内存态生成机制，新方案能简化节点运维工作，降低运营成本。Ed25519 已在众多区块链与分布式系统中得到广泛验证，其在性能、安全性与资源占用等维度的优势已获得充分认可。Ed25519 公钥可与 ACP-113 提案引入的 VRF 机制相结合，为平台带来更多技术创新的可能性。        ACP-118：ACP-118 提案针对性地解决了 Warp 消息签名处理机制的标准化问题。该方案通过优化跨 L1 通信的签名流程，为不同虚拟机间的互操作性提供了统一框架，同时简化了 Warp 消息中继等基础服务的签名聚合流程。    在当前架构下，Warp 消息承载着 Avalanche 生态内不同 L1 网络间的跨链通信功能，其可靠性主要依托于验证者签名与 BLS 签名聚合机制的保障。然而，各虚拟机（如 L1 EVM、Coreth 等）采用独立的签名请求处理机制，这种异构性在跨 VM 应用开发过程中引入了额外的复杂度。    ACP-118 与 ACP-77 密切相关，通过提供标准化的跨 L1 区块链签名请求与聚合工具，该方案确保了跨链消息传递在面对不同验证者集合与治理机制时仍能保持顺畅和安全。        ACP-131：ACP-131 提案为 Avalanche C-Chain 及 L1-EVM 链引入了对特定 EIP 的支持。其目的是使Avalanche与以太坊的 Cancun 升级计划保持一致，以确保开发工具链和基础设施的兼容性。不过该方案选择性地排除了 EIP-4844 中的 blob 交易功能，以规避潜在的技术复杂性。    此升级确保了 Avalanche 与以太坊 EVM 保持兼容，使得开发者能够无缝使用各种工具和 Solidity 版本，优化了整体的开发者体验。在提升平台性能和功能的同时，也保障了 Avalanche 的稳定性和未来的可扩展性。  结论作为 Avalanche 自 2020 年主网上线以来最具里程碑意义的技术革新，Avalanche9000 升级方案针对性地解决了平台发展过程中的核心痛点。该升级通过优化可定制 L1 区块链的构建门槛，完善跨链消息传递机制，以及借助 ACP-77 提案降低质押成本等一系列举措，全面提升了网络的可扩展性、灵活性与开发友好度。升级引入的分散治理机制、完备的故障隔离能力以及合规友好的独立链架构，也显著增强了 Avalanche 作为企业级区块链解决方案的核心竞争力。在生态激励层面，Avalanche9000 配套推出的 Bounty9000 计划为新 L1 链的早期采用提供了有力支持，有望加速开发者群体对 Avalanche 基础设施的广泛应用。从市场反馈来看，升级效果已初步显现。目前已有超过 500 条 L1 链（即原子网）在积极开发中，应用场景横跨游戏、金融科技与企业服务等多个重点领域。这种蓬勃的开发态势，叠加平台基本面的持续改善，有望在未来数月内传导至资产价格表现。值得注意的是，这一轮生态扩张不仅体现在数量层面，更重要的是通过模块化的设计理念和低成本高性能的网络特性支持更丰富的应用创新，进一步巩固了 Avalanche 在行业中的战略地位。"
  },
  
  {
    "title": "Make L1 Great Again",
    "url": "/posts/ml1ga/",
    "categories": "Blockchain",
    "tags": "l1, solana, ethereum",
    "date": "2024-11-20 12:00:00 +0800",
    





    
    "snippet": "      Layer 1 生态呈现多元化发展态势，各平台已形成差异化定位，未来更可能是多链共存而非单一主导的局面。    市场格局正在重塑，一方面新兴平台在关键指标上开始挑战传统领导者，另一方面传统金融机构的加速进场为行业注入了新的增长动力。  截至 2024 年 12 月，加密货币市场的总市值已成功突破 3 万亿美元大关，其中智能合约平台的表现尤为突出，成为推动市场增长的主要力量。投资者...",
    "content": "      Layer 1 生态呈现多元化发展态势，各平台已形成差异化定位，未来更可能是多链共存而非单一主导的局面。    市场格局正在重塑，一方面新兴平台在关键指标上开始挑战传统领导者，另一方面传统金融机构的加速进场为行业注入了新的增长动力。  截至 2024 年 12 月，加密货币市场的总市值已成功突破 3 万亿美元大关，其中智能合约平台的表现尤为突出，成为推动市场增长的主要力量。投资者的风险偏好逐渐从比特币转向了这些新兴的智能合约平台。高效、低成本的交易解决方案成为了吸引用户参与、促进链上活动的关键因素。  自美国大选以来，Solana 的价格涨幅达 57%，这一成绩主要得益于 memecoin 热潮所带来的零售投资者兴趣激增，以及 DeFi 领域前所未有的活跃度，其产生的收入已经超越了以太坊。  与此同时，Sui 的价格创下了接近 4 美元的历史新高，这主要归因于其游戏设备的预发布和对消费者友好的特性所引发的市场关注。  此外，Aptos 在代币化和支付等机构级解决方案上的进展，也为其带来了显著的市场动力，近期价格涨幅达到了 71%。鉴于 Layer 1 区块链的迅速崛起，有必要对行业如何达到这一发展阶段进行更深入的考察：      L1 平台在解决区块链三难问题时采用了不同的策略。比特币和以太坊作为早期的代表，优先考虑了去中心化和安全性，奠定了区块链技术的信任基础。然而，随着用户需求的增长和技术的进步，后续的迭代开始更加注重优化可扩展性。        以太坊凭借其先发优势，巩固了 EVM 作为智能合约平台标准的地位，还推动了广泛的开发者社区和应用生态系统的形成。尽管如此，EVM 在性能上的局限性也促使了行业内的创新，催生了诸如 Solana 的 SVM 等新型虚拟机。  尽管各 L1 平台在设计理念和技术实现上存在显著差异，但都在链上生态中扮演着不可或缺的角色。每个平台根据其特定的架构特点和下表中概述的关键特性，为不同的应用场景提供了定制化的解决方案。            网络特性      Ethereum      Cardano      Solana      Avalanche      TON      Aptos      Sui                  出块时间 (s)      12      20      0.4      2      5      0.2      0.48              TPS      13      1.2      1035      54      60      25      45              验证者数      1.074M      3000+      1421      1429      400      150      108              平均交易成本      $20      $0.25      $0.12      $0.11      $0.025      $0.0015      $0.007      发展概况以下将对各个区块链网络独特优势的详细阐述，并按其发布的时间顺序探讨其在不同应用场景中的解决方案。Ethereum - 2015 年 7 月作为去中心化应用和智能合约技术的先行者，以太坊是首个引入智能合约功能的区块链平台。  以太坊为 DApp 和 DeFi 奠定了基础，拥有最高的流动性，TVL 达 600 亿美元。  以太坊专注于去中心化和安全性，拥有超过 100 万验证者，使其成为代币化政府证券发行的理想选择，管理总资产已超过 15 亿美元。  该平台托管了超过 4000 个 DApp，包括 Uniswap 和 Aave 等知名 DeFi 平台，充分利用了其强大网络效应。尽管 L2 解决方案显著提高了交易处理速度和效率，但同时也带来了碎片化问题，对收入产生了一定影响。Cardano - 2017 年 9 月自推出以来，Cardano 始终以其严谨的学术研究和科学方法论为基础，专注于构建更加稳健、可持续发展的区块链生态系统。  该平台率先将形式化验证应用于智能合约开发，显著增强了智能合约的安全性和可靠性，并通过委托权益证明 (DPoS) 机制改进了以太坊的权益证明 (PoS) 系统，实现了更广泛的社区参与和民主化的交易验证过程。  Cardano 在全球范围内推动了多项社会影响项目，例如与埃塞俄比亚合作部署 Atala PRISM，为当地居民提供数字身份服务，从而改善教育和金融服务的可及性。此外，Cardano 还致力于建立透明高效的援助分配系统，旨在提高人道主义援助的效果和效率。  由于开发进度相对缓慢、缺乏 EVM 兼容性以及链上治理结构的缺失，Cardano 的市场表现受到一定影响，目前其 TVL 仅为 4.56 亿美元。近期，Cardano 推出了 Chang 硬分叉等一系列升级措施来促进由社区主导的网络发展，而 BitcoinOS 的集成则有望通过增强 ADA 驱动的比特币交易来提升跨链操作的实用性。Solana - 2020 年 3 月Solana 网络结合历史证明 (PoH) 与权益证明 (PoS) 的双重共识模型使其能够实现超过 4000 TPS 的交易处理能力，成为了目前最快的区块链之一。  本月，Solana 的 DEX 交易量大幅超越以太坊，达到了 400 亿美元的规模。这主要归功于其高效低成本的区块链架构，以及由散户驱动的活跃 DeFi 生态。交易量的激增也推动 Solana TVL 达到了 83 亿美元。  Solana 还积极支持各类去中心化物理基础设施网络 (DePIN) 项目，如 Helium（宽带）、Hivemapper（地图）和 Render（3D 渲染）等依赖大规模、高速、低成本交易处理能力的应用。Solana  快速高效的区块链为这些应用提供了必要的基础设施，确保其能顺畅、高效地运行，进而推动物理世界与数字世界的深度融合。  Solana 已获得 PayPal 旗下稳定币 PYUSD 的支持，并与 Visa 和 Shopify 等知名企业建立了合作关系。但要进一步深化与传统金融体系的整合，Solana 必须解决其网络可靠性方面的问题。Solana 今年经历了一次停机事件，类似情况在过去三年中也多次出现。为此，Solana 推出了 Firedancer 验证器客户端。该客户端在测试网上展现了超过 100 万 TPS 的处理能力，有望显著减少停机并提高网络的稳定性，进而帮助 Solana 巩固其作为顶级区块链解决方案的地位，进一步提升其用户体验和市场竞争力。Avalanche - 2020 年 9 月Avalanche 是 EVM 兼容的区块链平台，其独特的子网架构支持创建定制化、有许可的网络，并可将其无缝连接至 Avalanche 主网。  Avalanche 的可扩展子网架构是其核心竞争力之一。通过这一架构，用户可以创建具有高度定制化的区块链网络，支持灵活的燃料费结构、数据隐私保护和验证者激励机制。每个子网可以根据具体需求进行配置，满足不同应用场景的要求。2023 年推出的 Avalanche9000 升级进一步增强了这一功能，通过引入共享流动性、降低验证者的运营成本以及提供完全的可定制性（包括地理限制），显著提升了子网的实用性和吸引力。这些改进使 Avalanche 成为企业级区块链解决方案的理想选择，尤其适合需要高度定制化和安全性的企业应用。  Avalanche 的 Evergreen 子网吸引了众多传统金融机构的关注和参与。富兰克林邓普顿利用 Avalanche 的子网对其 4.2 亿美元的政府货币市场基金进行了代币化。花旗银行和威灵顿管理公司也在积极探索基于 Avalanche 的金融应用，旨在提升金融服务的效率和透明度。  Avalanche 在多个行业中得到广泛应用。德勤在 Avalanche 上构建了联邦灾难赔偿平台，该平台旨在加速理赔流程并提高透明度，从而更好地服务于受灾群众。基于 GUNZ 子网开发的游戏项目 Shrapnel 展示了 Avalanche 在娱乐和游戏行业的潜力。Avalanche 的 EVM 兼容性及强大的开发者社区推动其 TVL 超过了 10 亿美元。TON - 2021 年 9 月The Open Network (TON) 是专为无缝用户接入设计的高吞吐量区块链平台，其目标是为用户提供类似 Web3 版微信的体验。与拥有 9 亿月活用户的 Telegram 建立的战略合作伙伴关系为 TON 带来了广泛的用户基础和市场准入机会，也使其成为了推动区块链主流采用的关键力量。      TON 的小程序是其链上应用生态的重要入口，吸引了大量普通用户，例如 Hamster Kombat 曾拥有近 3 亿用户，展示了 TON 在吸引大规模用户群体方面的潜力。然而，TON 的金融生态系统仍处于早期阶段，TVL 约为 3 亿美元，主要集中在更简单、对散户友好的用例上，DeFi 领域发展相对有限。        本月，TON 处理了近 2800 万笔交易，在推动基于区块链的支付方面展现出强大的竞争力。其最近的 USDT 的集成，在短短七个月内供应量超过 10 亿美元，进一步增强了 TON 在支付领域的潜力。        尽管 Telegram 最近遇到了一些挫折，但 TON 作为独立运营的网络并未受到太多影响，保持了先前强劲的增长势头。TON 的基础设施具备高度的可扩展性，能够支持大量用户和交易，同时其广阔的潜在市场使也为吸引下一波用户带来了巨大优势。未来，随着技术的不断进步和生态系统的逐步完善，TON 有望在支付、社交和娱乐等领域继续推动区块链技术的广泛应用。  Aptos - 2022 年 10 月Aptos 是一个高性能区块链平台，采用了由 Meta 为其 Diem 项目开发的 Move 编程语言。Move 语言在设计上优先考虑了安全性和可扩展性，这也使其成为机构应用的理想选择。通过利用 Move 语言的独特特性，Aptos 不仅能够确保智能合约的安全执行，还能提供高效的交易处理能力，满足企业级应用的需求。      Aptos 的并行处理技术实现了亚秒级的结算时间和理论上 16 万 TPS 的处理能力，确保了企业级应用所需的高吞吐量和低延迟，为大规模商业应用提供了坚实的技术基础。这种高性能使得 Aptos 在处理复杂金融交易、供应链管理和其他需要高效处理大量数据的应用场景中具备显著优势。        Aptos 由 Meta 的 Diem 项目前高管领导，吸引了众多传统金融巨头和机构级用户的关注和支持。微软、富兰克林邓普顿和 NBC 环球等知名企业在 Aptos 上进行了投资和合作，进一步巩固了其在市场中的地位。此外，Ondo 的代币化平台等机构级用例也选择了 Aptos，证明了其在企业级应用中的可行性和可靠性。随着 DeFi 生态系统的不断增长，Aptos 的 TVL 已接近 10 亿美元。        Aptos 致力于简化 Web3 的接入体验，推出了无密钥账户、无密码认证和交易预览等功能，大大降低了用户上链的门槛。为了进一步扩大其影响力，Aptos 还面向新兴市场推出了成本效益型设备，如预装了区块链工具的 Jambo Phone。通过结合以用户为中心的设计与高可扩展性，Aptos 成功在当前市场环境下占据了有利位置。  Sui - 2023 年 5 月Sui 是 Aptos 的姊妹链，同样基于 Move 语言构建，旨在为 Web3 带来类似 Web2 的简洁用户体验。但 Sui 主要面向消费者 DApp 而非机构用例。  为了迎合零售用户的需求，Sui 推出了一系列创新产品和服务，如连接 Web3 与传统游戏体验的游戏设备 SuiPlay0X1 等。Sui 这种以用户体验为中心的战略定位，使其在面向消费者的 DApp 领域中占据了领先地位。  Sui 融合了亚秒级的交易确认速度和并行处理能力，并提供了多项便捷功能：允许应用程序代付 gas 费以简化用户交互、支持通过 Google 或 Face ID 等第三方身份验证快速创建钱包的 zkLogin、以及基于二维码的支付工具 zkSend。通过将高可扩展性与卓越的用户体验相结合，Sui 有望加速零售市场的采用，架起 Web2 与 Web3 之间的桥梁，吸引新一代加密用户。  近期，Sui 上线了以太坊跨链桥，并引入了对 USDC 的支持，这些举措极大地增强了其 DeFi 生态的活力，目前平台上的 TVL 已超过 15 亿美元，Sui 的代币价格也接近 4 美元的历史高点，显示了市场对 Sui 的高度认可。在明确了每个区块链的独特优势和特点之后，接下来我们将深入分析这些特性如何在实际的链上活动中体现。为了更准确地评估各个网络的表现，本文将重点关注以下几个关键指标：活跃地址数量、DEX 交易量、产生的费用以及 TVL。这些数据不仅能够反映网络的活跃度和用户参与度，还能揭示其经济模型的健康状况和发展潜力。关键指标日活跃地址数图1 2024 年各智能合约平台日活跃地址数 | 图源：Artemis如图 1 所示，Solana 在用户参与度方面显著领先，凭借其低廉的交易费用、高效的处理速度和直观的用户界面，成功吸引了 650 万用户的青睐，成为目前用户基础最为庞大的平台之一。这一优势也为今年 memecoin 热潮的兴起创造了理想条件，年初至今，仅在 pump.fun 平台上就发行了超过 350 万种代币。此外，Solana 已成为了包括 AI 代理、DePIN 在内的新叙事代币的首选平台。Aptos 相比 Avalanche 等 L1 区块链，在用户增长方面表现亮眼，这主要归功于其日益壮大的 DeFi 生态。贝莱德 BUIDL 产品的扩展、Tether 的 USDT 和 Circle 的 USDC 稳定币在网络上的部署，以及 Stacks 的 sBTC 整合等关键进展，共同推动了 Aptos 网络用户活跃度和参与度的显著提升。以太坊依然是不可忽视的强大竞争者。通过将大部分活动迁移至 L2 扩展，以太坊有效地应对了可扩展性挑战。如今，以太坊生态中 90% 以上的交易量都发生在 L2 上，这一方面凸显了采用扩容方案作为传统网络执行层的重要性，另一方面也表明了以太坊对技术进步的适应能力。L2 大幅提升了以太坊的处理能力，实现了 382 TPS 的吞吐量，较之基础层的 14 TPS 提高了 26 倍，并吸引了 260 万用户，这一数字是以太坊主网平均 35 万用户数的七倍多。DEX 交易量图2 2024 年各智能合约平台 DEX 交易量 | 图源：Artemis总体来看，以太坊和 Solana 凭借其蓬勃发展的 DeFi 生态在交易所活动中占据了主导地位。以太坊长期以来一直保持领先地位，拥有规模最大、最为成熟的 DeFi 生态，TVL 接近 600 亿美元。自 2020 年以来，其金融应用展现了强大的韧性，经受住了多次市场挑战。如图 2 所示，在今年的前三个季度，以太坊贡献了近 50% 的 DEX 交易量。Solana 最近在多个关键指标上超越了以太坊。例如，11 月 Solana 的 DEX 交易量比以太坊高出 400 亿美元；而在 10 月的最后一周，Solana 的周 DEX 交易量甚至超过了以太坊及其所有 L2 的总和。此外，收入最高的前十个 Dapp 中有三个运行在 Solana 网络上。值得注意的是，基于 Solana 的 DEX Raydium 在上周的交易量达到了 290 亿美元，比 Uniswap 的 200 亿高出 45%。Solana 平台如今占据了前十大 DEX 中的三个，控制了整个加密货币生态 24 小时总交易量的 40% 以上。虽然 TON 当前的 DEX 交易量较低，DeFi 生态也相对不成熟，但其专注于简单用例的战略使其 TVL 保持在了一个适中的水平。不过即将到来的 Curve 集成预计将大幅增加稳定币流动性，进而推动增长。此外，TON 最近推出了其跨链桥解决方案的测试网，支持原生比特币转移至其网络。这一突破性进展使 TON 能够触及到价值约 1300 亿美元的休眠比特币，将为其开拓出庞大的潜在市场。费用图3 2024 年各智能合约平台产生的费用 | 图源：ArtemisTON 依托于 Telegram 庞大的小程序生态，网络费用在新兴加密平台中脱颖而出。TON 的应用场景不仅限于加密货币领域，还成功整合了打车、电子商务等传统服务，用户可以在这些服务中使用基于 TON 的数字资产进行支付。此外，以 Telegram 为中心的产品，如价格优惠的全球 eSIM、VPN 以及去中心化存储解决方案等，也进一步增强了其对非加密原生用户的吸引力。如图 3 所示，TON 已成为仅次于其他两大网络的第三大费用生成网络。Solana 已将自己打造成了费用巨头，如图 3 所示。该平台凭借其高速低成本的优势，吸引了大量领先的 Dapp。例如，无代码 memecoin 创建平台 Pump.fun  已产生 2.2亿美元的费用并吸引了 15 万用户。此外，基于 Solana 的协议占所有区块链前 15 大费用生成应用的半壁江山。交易机器人 Photon 和 BonkBot 每月总共赚取 7500 万 美元，而 Radium 和质押服务提供商 Jito 共同贡献了 3 亿美元的收入。Solana 目前产生的实际经济价值已经达到了 Ethereum 的 110%。以太坊最近的 Dencun 升级引入了高效的 L2 数据存储机制 blobspace，使得 L2 交易成本降低了 90% 以上。随着这一新系统逐渐获得市场认可并接近容量上限，L2 交易成本预计将逐步回升。如图 4 所示，这一趋势已经显现，预示着以太坊主网收入在未来几个月内可能会出现复苏。图4 以太坊区块平均 Blob 数 | 数据源：DuneTVL图5 2024 年各智能合约平台 TVL | 图源：ArtemisTVL 是加密领域金融生态中的一个重要指标，其作用类似于传统金融中的资产管理规模 (AUM)。作为 DeFi 的先驱，以太坊目前保持着所有网络中最高的 TVL。但随着行业的快速发展，其他区块链平台也在迅速崛起，改变现有格局。Solana 已成为 DeFi 领域的重要参与者，目前占据了整个市场 TVL 的近 10%。其增长主要得益于与传统金融 (TradFi) 的无缝集成、代币化领域的扩张以及支付用例的激增。此外，如前所述，Solana DeFi 生态系统的快速发展也是推动其 TVL 增长的关键因素。同时，Avalanche 凭借其独特的子网模型也保持着较高的 TVL。该模型支持企业用户创建高度定制化的网络，带来了前所未有的控制力和灵活性。Avalanche 因此成为了代币化项目和机构金融应用的中心。其网络设计不仅有效地满足了 TradFi 对商业和隐私的需求，还为各类金融创新提供了基础。指标清晰呈现了各链的现状，如下表所示。这些 L1 平台充分展现了区块链技术应用的多样性和灵活性，各条链在区块链生态中都有着独特的定位和竞争优势。可以预见的是，未来不太可能出现“至尊链，驭众链”的局面；多元化的多链生态正在逐渐形成，每条链都将因其特定的优势和应用场景而获得市场的认可与青睐。这种多样化的发展模式不仅能促进技术创新，也为开发者和用户提供了更多选择，推动整个区块链行业的健康发展。            指标      Ethereum      Cardano      Solana      Avalanche      TON      Aptos      Sui                  活跃地址数      404.8K      53.4K      6.5M      33.8K      482.8K      694.8K      668.9K              DEX 交易量      $648.4M      $13.8M      $6.1B      $291.6M      $31.8M      $40.6M      $288.8M              费用      $5.5M      $26.7K      $6.8M      $42.9K      $68.9K      $4.9K      $46.7K              TVL      $58.4B      $431.4M      $7.9B      $1.3B      $306.2M      $917.8M      $1.6B      "
  },
  
  {
    "title": "稳定币：金融生态的跨界之桥",
    "url": "/posts/stablecoin/",
    "categories": "RWA",
    "tags": "stablecoin, rwa",
    "date": "2024-10-23 12:00:00 +0800",
    





    
    "snippet": "      稳定币已成为加密经济的重要支柱，显著降低了加密货币的使用门槛。主导当前市场的法币抵押型稳定币提供了可靠的价值锚定和低成本的跨境交易解决方案，在新兴经济体中尤其受欢迎。    稳定币在金融生态中的应用日益广泛，不仅在交易和支付领域发挥重要作用，还在 DeFi 生态中占据重要份额。从汇款到借贷，稳定币正逐步成为连接传统金融和加密经济的关键桥梁。    稳定币面临的主要挑战包括中心化风...",
    "content": "      稳定币已成为加密经济的重要支柱，显著降低了加密货币的使用门槛。主导当前市场的法币抵押型稳定币提供了可靠的价值锚定和低成本的跨境交易解决方案，在新兴经济体中尤其受欢迎。    稳定币在金融生态中的应用日益广泛，不仅在交易和支付领域发挥重要作用，还在 DeFi 生态中占据重要份额。从汇款到借贷，稳定币正逐步成为连接传统金融和加密经济的关键桥梁。    稳定币面临的主要挑战包括中心化风险、透明度问题和潜在的价格脱钩。随着监管的逐步明确和市场的不断成熟，稳定币正在向更加安全、透明和创新的方向发展，可能会在未来金融生态系统中扮演越来越重要的角色。  稳定币已迅速成为加密经济的重要支柱，推动了近半数的链上交易活动。稳定币巧妙地弥合了高波动的加密货币与传统法定货币之间的鸿沟，为用户提供了稳定可靠的交易媒介。通过与 Stripe 和 PayPal 等主流支付平台无缝整合，稳定币降低了加密货币的使用门槛，使普通用户无需深入技术细节即可参与加密经济。其快速、低成本的支付特性正在重塑汇款和跨境交易模式，在非洲等面临货币挑战的地区尤为显著。稳定币对加密货币新老用户的独特吸引力，正推动着数字金融的广泛普及，成为金融生态系统演进的关键驱动力。何谓稳定币？稳定币是一种旨在维持恒定价值的加密资产，通常与黄金或美元等资产按 1:1 比例锚定。这种设计使其非常适合日常交易和资金流转。稳定币主要分为三种类型：  法币抵押型：由法定货币直接背书，目前占据了稳定币市场的主导地位。代表性产品包括 Tether 的 USDT 和 Circle 的 USDC。  加密抵押型：由加密资产背书，为对冲波动性，往往采用超额抵押机制。代表性产品如 MakerDAO 的 DAI。  算法型：完全依靠算法和智能合约维持价格稳定，无需传统的资产储备，其价格锚定通过铸造和销毁机制来实现。代表性产品如 Aave 的 GHO。抵押方式稳定币通过特定的抵押策略和市场机制来抵御价格波动，以保持其价格稳定。法币抵押型法币抵押稳定币由 Tether 和 Circle 等中心化机构发行，其核心特征是由链下银行账户中的法定货币储备提供背书。以 Tether 为例，据其透明度报告所述：约 15% 的储备投资于比特币等资产及担保贷款，以获取额外收益，同时保持大部分资产为现金或等价物，确保随时满足赎回需求。  遵循严格的 1:1 锚定模式，每个代币均由等值的法币或现金等价物支持。1 USDC 即代表 1 美元的价值。  发行方保留随时清算储备的能力，以确保拥有充足的抵押品覆盖已发行代币，维护币值稳定。  设计的简洁性使法币抵押稳定币很适合在交易和 DeFi 抵押等需要高流动性的场景下使用。凭借其简单可靠的特点，法币抵押稳定币在市场上占据主导地位，仅 USDT 和 USDC 的市场份额就超过 1400 亿美元，占比超过 90%，如图 1 所示。图1 法币抵押稳定币市场份额 | 图源：TokenTerminal加密抵押型加密抵押稳定币是去中心化金融生态系统中的创新产物，在无中央监管的环境中运作，依托智能合约和加密资产构建其稳定机制。  这类稳定币的核心策略是过度抵押：用户必须锁定超过铸造稳定币价值的加密资产，在市场波动中建立缓冲。  若抵押品价值跌破预设阈值，用户资产将自动划归协议，以保护币值锚定。价格预言机负责实时监控并触发自动清算，维护整体偿债能力。  尽管这种模式体现了去中心化的理想，但同时也面临复杂的技术挑战，对智能合约系统的健壮性要求极高。多重智能合约间的复杂交互存在潜在技术风险，需要精细的风险管理和技术审查。MakerDAO 的 DAI 是此模式的典型代表。用户用 ETH 作为抵押品，协议通过链上治理动态调整利率，平衡供需以维持币价稳定。凭借其去中心化治理和内部管理机制，DAI 在上月处理了逾 1130 亿美元的资金流转，展现了其在加密金融生态中的重要地位，如图 2 所示。图2 DAI 去中心化运营带来的供应流入与流出 | 图源：Artemis算法型算法稳定币的核心理念是在不依赖传统抵押品的情况下，通过智能合约和供应动态调整来维持币值稳定。  以 Ampleforth (AMPL) 为代表的协议根据市场价格的变化来调整代币的供应量来维持与 CPI 调整的美元等值。当代币价格突破锚定价格时，协议将增加总供应量；反之，则通过所谓的”rebasing”机制收缩供应。  还有一些设计，如曾经的 UST，则引入了另一种“债券”代币（如 LUNA）作为供应调节的辅助机制，来维持币值稳定。然而，这些创新性尝试的可靠性远低于传统抵押模式。算法的细微缺陷或系统设计中的潜在问题，都可能导致不可预期的黑天鹅事件，UST 的惨烈崩盘就是最鲜明的警示。使用情况交易及兑换稳定币凭借其可靠性，已成为加密货币交易生态系统中不可或缺的要素，兼具价值储存和交换媒介的双重功能。以市值最大的两种法币抵押稳定币为例，其应用趋势颇为明确。USDC 总供应量中约 60% 存储在外部账户 (EOA) 中，另有 11% 分布在中心化金融 (CeFi) 平台上，如图 3 所示。图3 USDC 在加密生态中的应用 | 数据源：DuneUSDT 的分布呈现更为显著的集中态势，其中 70% 位于 EOA 中，25% 驻留在 CeFi 平台上。USDT 的广泛采用源于 Tether 在区块链部署上的灵活性和前瞻性，其迅速进入新兴区块链并与众多中心化平台建立深度整合，使其在稳定币市场中占据优势地位。图4 USDT 在加密生态中的应用 | 数据源：Dune这种分布提现了稳定币的核心价值：提供金融稳定性。EOA（如 Metamask 和 Phantom 等非托管钱包）本质上是用户规避加密货币波动风险的避风港，让投资者能快速将资产转换为相对稳定的形态。而像 Binance 和 Coinbase 这样的 CeFi 平台不仅提供资产转换渠道，更为跨境交易构建了便捷桥梁。这点对 USDT 的众多网络上（尤其是 Tron）的广泛部署十分关键，使其成为了拉丁美洲和非洲新兴经济体用户的理想选择。由此不难看出稳定币在保护投资和提供全球金融可及性两方面都扮演了重要角色。DeFi在 DeFi 领域，法币抵押稳定币相较于去中心化稳定币存在感偏低，原因主要在于后者采用激励机制（如分发 AAVE、MKR 等原生协议代币）来推动其生态的发展。但 PayPal 的 PYUSD 是一个特例。在进军 Solana 生态系统的过程中，PayPal 与 Kamino Finance 携手合作，为 PYUSD 存款提供高达 17% 的年收益率，显著高于 USDC 在 Solana 上年化 9% 的收益率。这一策略在初期颇有成效，推动 PYUSD 的市值迅速攀升至近 10 亿美元，其中近 20% 流通于去中心化交易所，用户通过提供流动性赚取收益。然而，当激励措施收缩时， PYUSD 的市值从 8 月的峰值骤降近 40%，凸显了收益驱动策略在挑战现有主流稳定币市场地位、塑造采用路径中的关键作用。图5 PYUSD 在加密生态中的应用 | 数据源：Dune同样，得益于精心设计的激励机制，Aave 推出的超额抵押稳定币 GHO 也在 DeFi 领域引起了广泛关注。数据显示，约 80% 的 GHO 供应被用于借贷，另有5%在各大 DEX 中流通。这种独特的分布与 Aave 将 GHO 定位为其借贷市场核心的战略高度契合，进一步巩固了稳定币在整个 DeFi 生态中的地位，并促进了其更广泛的应用。图6 GHO 在加密生态中的应用 | 数据源：Dune总体来看，如图 7 所示，稳定币在 DeFi 中的应用范围相当广泛，约占整个生态的 25%，涵盖去中心化交易所、借贷以及其他金融活动，为用户提供了多元化的价值捕获途径。图7 按金融活动类型分类的稳定币交易量 | 图源：Visa汇款及支付稳定币在跨境支付领域展现出显著的低成本优势，相较于传统国际汇款服务动辄 6% 的高额手续费，其成本优势尤为突出。图8 不同渠道的平均汇款成本（占汇款金额的百分比）| 图源：Coinbase区块链网络的交易费用差异显著，从几分之一美分到几美元不等。像 Solana、Fantom、Polygon 和 TON 等新一代区块链平均交易费用在一美分以下，而 BNB、Tron 和 Ethereum 等老一代网络的费用则可能需要数美分到数美元的费用。如图 9 所示。不过随着 Layer 2 解决方案的发展，Base、Arbitrum 和 Optimism 等基于 Ethereum 的平台在 Dencun 升级后，也已将交易成本压低至不到一美分，极大地改善了用户体验。图9 主流 Layer-1 区块链的平均交易成本 | 图源：TokenTerminal即便是在费用相对较高的网络中，加密货币交易的成本仍远低于传统支付系统，这种成本效益直接反映在稳定币使用量的稳步增长上。过去四年间，尽管加密货币市场经历周期性波动，但稳定币的使用量却在持续攀升。从 2020 年第三季度至 2021 年底的牛市期间，稳定币使用量显著增长；随后即便进入熊市，从 2022 年到 2023 年第三季度，增长势头依然不减。尤为瞩目的是，每月稳定币发送者数量从 2020 年 10 月的约 230 万激增至如今的 1200 万，增长近 600%，充分显示了稳定币在加密生态之外的持续吸引力和实际价值。图10 每月稳定币发送者数量 | 图源：TokenTerminal稳定币的应用正在发展中经济体掀起新的变革。一项针对巴西、印度、印度尼西亚、尼日利亚和土耳其 2541 名成年人的调查显示，47% 的受访者使用稳定币以获得更高储蓄利率，43% 的人看重其便捷的货币兑换功能，另有 37% 的人将其视为获取美元资产的途径。尽管调查样本有限，但这些数据仍表明稳定币在新兴市场正逐步发展成为一种跨越传统加密货币应用边界的多功能金融工具，在满足复杂经济需求方面发挥着越来越重要的作用。图11 用户使用稳定币的主要目的 | 图源：CastleIsland影响Tether 对加密生态系统的影响显著，如图 12 所示，其用户数已达 6000 万，占到了整个行业 2.2 亿活跃用户基数的 27% 以上，体现了稳定币在推动加密货币普及中的重要作用。图12 稳定币持有者总数 | 图源：TokenTerminalUSDT 作为网络活动的重要引擎，其影响力巨大：仅在 10 月 21 日单日，就在各个区块链上支付了超过 700 万美元的费用，由此也能看出稳定币在推动区块链生态系统增长中的重要作用。更为显著的是，过去一个月中，USDT 在以太坊网络上的 gas 消耗位居第三，贡献了网络总费用（约 40270 ETH）的 5% 以上（约 2310 ETH）。在 Tron 网络上，USDT 更是占据了每周 96% 的网络活动。如此大规模的费用通过补偿验证者直接支持了网络安全，更凸显了稳定币对区块链经济生态的深远影响。图13 稳定币发行商每日 gas 费 | 图源：TokenTerminal监管格局稳定币发行商已超越德国和韩国成为美国国债第 18 大持有者。随着应用的深化，稳定币的角色可能延伸至购买国债等领域，从而进一步融入主流金融体系。其日益增长的影响力凸显了对明确监管的迫切需求，如去年引入的《支付稳定币透明度法案》和《Lummis-Gillibrand 支付稳定币法案》，以确保这一新兴领域的安全性和可信度。美国监管政策的模糊不清，加之地缘政治和宏观经济因素的影响，可能逐渐削弱美元计价稳定币的主导地位。      欧洲的《加密资产市场法规》(MiCA) 将于 2024 年 12 月 30 日全面生效。该法规对中心化发行商进行了严格限制，要求发行商必须获得 MiCA 许可证，方可在欧盟内公开提供或交易资产参考代币或电子货币代币，且不设过渡期。如果 Tether 等公司未在 2024 年底前达到合规要求，则 USDT 等稳定币可能在欧盟交易所被下架。此外，为保护货币主权，MiCA 对与外币挂钩的稳定币使用进行了限制，当每日使用量“作为单一货币区内的交换手段超过 100 万笔交易和 2 亿欧元”时，必须停止发行。    泰国最古老的银行推出该国的首个稳定币：暹罗商业银行与金融科技公司 Lightnet 合作，推出了泰国的首个用于跨境支付和汇款的稳定币。  阿联酋央行已原则上批准了 AED 稳定币在其《支付代币服务监管框架》下的申请。若最终获准，AE Coin 将可在交易所和去中心化平台作为本地交易对，并可用于商品和服务支付。值得注意的是，这一监管进展发生在 Tether 宣布推出 AED 稳定币的数月之后，其 CEO 当时表示，这将“提供美元的替代选择”。风险与挑战      中心化：USDT 和 USDC 等法币抵押稳定币由中心化机构发行，用户需信任这些机构能保持足够的储备金。这种模式引入了显著的交易对手风险：储备不足可能导致流动性危机，使用户面临资金无法兑付的问题。此外，中心化还可能带来监管干预、管理不善或资金被冻结等风险。        透明度：由于法币抵押稳定币的储备在链下，由现金或现金等价物支持，因此透明度对于维护信任至关重要。Tether 和 Circle 等发行商需要定期进行审计，并公布储备文件，如Tether 的 透明度报告，以证明其偿债能力。对于加密抵押稳定币来说，透明度问题较小，因为所有资产都可以在链上验证。协议通常会提供仪表板，如 MakerDAO 的 DAI 统计仪表板，详细显示抵押率、总债务、抵押类型和贷款稳定性。        脱钩：脱钩是指稳定币价值显著偏离预期锚定价格的情况。最著名的脱钩事件是 2022 年 5 月 UST 和 LUNA 的崩盘，此次事件也暴露出了算法稳定币的潜在风险。目前，尽管脱钩仍偶有发生，但稳定币整体的稳定性正在逐步提升，如图 14 所示，这主要归功于流动性的持续涌入、行业的成熟以及协议的久经考验。得益于充足的储备和活跃的链上治理社区，稳定币已具备了更强的流动性冲击抵御能力。图14 主流稳定币的价格稳定性 | 图源：CoinMetrics  展望央行利率下降催生了对稳定币的深入探讨。投资者追求更高收益的趋势已初露端倪，如下图所示，加密货币无抵押贷款平台 Maple Finance 的贷款总额已攀升至历史新高。图15 MapleFinance 未偿贷款总额 | 数据源：Dune稳定币市场在收益模型方面正在酝酿创新。Ondo Finance 的 USDY 和 Angle 的 stEUR 等代币化货币市场基金通过每日分红获得成功，可能激励其他发行商采用类似的利润分享模型。这一趋势旨在挑战 Tether 和 Circle 等既有玩家，后者目前主要通过将用户存款再投资于政府证券获利。新兴竞争者可能借助收入分享机制抢占市场份额，进而重塑稳定币生态，为用户提供更具吸引力的选择，例如 BitGo 即将推出的稳定币。但由于监管机构正致力于构建平衡创新、消费者保护和金融稳定的明确框架，这些创新模型短期内可能会面临监管审查压力。受 Ethena 模型启发的 delta 中性策略也将持续演进。尽管在市场低迷期维持策略仍存挑战，但后续迭代可能将充分利用日益成熟的期货和期权市场。其目标是简化复杂策略，降低准入门槛，利用复杂的加密衍生品使高级金融产品大众化。央行数字货币 (CBDC) 的发展可能将迎来转折。从提升金融包容性到改进支付效率，各国动机各异，但都面临复杂的设计抉择。考虑到法币抵押稳定币的简单性和有效性（过去十年中促成近 35 万亿美元的交易），各国央行可能会暂时选择将这类成熟解决方案整合至传统金融体系。正如如加拿大、新加坡和英国的实践所示，在就最佳 CBDC 模型达成共识前，这种务实方法或将成为权宜之计。"
  },
  
  {
    "title": "隐私与透明：密码朋克运动的多维度探讨",
    "url": "/posts/cypherpunk/",
    "categories": "Privacy",
    "tags": "cypherpunk, privacy",
    "date": "2024-09-16 12:00:00 +0800",
    





    
    "snippet": "  本文系统探讨了密码朋克运动的核心伦理观、认识论及实践策略，旨在深入剖析其在数字时代对个体隐私保护与权力机构透明度提升所作出的重要贡献。密码朋克运动始于 20 世纪 90 年代，主张通过加密技术的广泛应用来维护个人隐私，对抗政府和企业日益增长的监控能力。其核心伦理观可概括为“弱者要隐私，强者要透明”，强调个人隐私权的同时，呼吁政府及大型机构提高透明度并接受公众监督。密码朋克认识论将加密技术...",
    "content": "  本文系统探讨了密码朋克运动的核心伦理观、认识论及实践策略，旨在深入剖析其在数字时代对个体隐私保护与权力机构透明度提升所作出的重要贡献。密码朋克运动始于 20 世纪 90 年代，主张通过加密技术的广泛应用来维护个人隐私，对抗政府和企业日益增长的监控能力。其核心伦理观可概括为“弱者要隐私，强者要透明”，强调个人隐私权的同时，呼吁政府及大型机构提高透明度并接受公众监督。密码朋克认识论将加密技术作为数据行动主义的主要工具，通过主动与被动相结合的策略，在保护个人信息安全的同时促进社会公正。在实践层面，密码朋克运动通过创建维基解密等平台，实现了对大型权力机构的逆向监控，展现了加密技术在重塑权力关系和推动社会变革中的关键作用。本文通过对密码朋克理论与实践的分析，揭示了其在数字监控时代的重要意义，为理解与应对当前的全球数据监控环境提供了新的视角。引言密码朋克 (Cypherpunk) 运动始于 20 世纪 90 年代初，主张将普及强加密技术作为维护个人隐私及对抗数字时代权威政府的策略。加密技术被誉为“保护信息安全的艺术与科学”1，对密码朋克而言，加密技术是抵御强势实体干预个人生活的核心工具。Robert Manne 阐述了密码朋克哲学的核心观点：“互联网时代的重大政治议题在于，国家是否会利用电子监控能力剥夺个人自由与隐私，抑或是自主的个人能否凭借新获得的电子武器破坏甚至瓦解国家权力。”2密码朋克认为，审查与监控是计算机时代的双重威胁，Manne 进一步指出，“密码朋克最深层的体制对手即为国家安全局 (NSA)。”2在 20 世纪 90 年代的加密战争 (Crypto War) 期间，密码朋克与国家安全局及联邦调查局 (FBI) 进行了正面交锋，挑战美国政府垄断密码学技术的企图3。NSA 和 FBI 认为加密技术的普及会阻碍国家安全情报和执法证据的搜集。而密码朋克则坚信，缺乏加密技术所提供的隐私保障，社会将不可避免地滑向威权主义。最终，密码朋克在加密战争中取得胜利，远促成了强加密技术向全球公众的普及。尽管密码朋克运动在对抗监控的历史进程中发挥了重要作用，但其在监控研究领域的理论贡献，尤其是其对抗监控系统的策略，却未得到充分的认识，甚至常被误解。例如 David Brin 曾批评密码朋克虚伪，认为其对隐私的关注源自一种“自我正义的狭隘视角，这种视角可能会阻碍我们探索未来数十年内可能遭遇的某些风险的有效解决方案”4。Brin 认为，密码朋克的理论在两个自相矛盾的原则之间来回摇摆：“一方面，原则 A 认为增加信息或数据的流动性是危险的；另一方面，原则 B 则认为这种增加是有益的”。在 Brin 看来，密码朋克们是“以自我正义的态度要求”其对手提高透明度，同时却不愿自身受到相同的约束。此外，Mir Adnan Ali 与 Steve Mann 认为密码朋克提倡的基于加密技术的隐私保护与“逆向监控” (sousveillance) 相对立。Ali 和 Mann 提出，“两者都能对交易中的信息进行控制，加密技术通过提供保持隐私的选项来实现这一点，而监控则通过提供分享信息的选项来实现。”5Brin 和 Mann 等人的观点源自于对密码朋克哲学中一些核心要素的误解。Ali 和 Mann 认为密码朋克仅专注于隐私问题，而缺乏对监控，尤其是逆向监控的关注。然而，正如 Suelette Dreyfus 所述：“密码朋克坚信个人应享有隐私权，同时政府应当公开透明并对公众负责”6。密码朋克运动的核心理念可以浓缩为一句简洁而深刻的口号：“弱者要隐私，强者要透明”7，密码朋克们认为，加密技术不仅能够保护个人隐私，还能通过特定形式的逆向监控促进透明度提升。换句话说，Ali 和 Mann 没有充分理解密码朋克的观点，即密码学既是反监控的工具，也是实施逆向监控的手段。而 Brin 则未能认识到监控和透明度仅在权力关系的背景下才具有实质意义。正如 Adam Moore 所指出的：“权力的一个显著特征是能够在要求他人公开信息的同时，自己却能保持信息的私密。”8 当今密码朋克哲学的重要倡导者 Julian Assange 主张，隐私的价值不应仅基于其内在属性而被维护，而是应在“权力计算” (calculus of power) 的框架下得到保护，因为“隐私的丧失会加剧统治阶级与其他社会成员之间的权力失衡。”9本文的主要目标是系统化梳理密码朋克哲学的主要方面，以便更好地理解和学习该运动的思想和实践策略。文章分为四个部分。第一部分通过将密码朋克“弱者要隐私，强者要透明”的理念置于其历史和理论背景中，解释了密码朋克在 1990 年代加密战争中发挥的作用，并强调了权力在密码朋克对隐私、保密和透明度的理解中的重要性，概述了密码朋克伦理的基本规范性原则。与各种社会运动一样，密码朋克内部也存在意识形态和伦理上的分歧，同时存在明显的代际差异，本文所提供的视角主要反映了影响力最大的第三代密码朋克所表达的思想。文章第二部分阐述了密码朋克认识论的核心要素。密码朋克认识论是以加密技术为核心的数据行动主义，主张依靠主动（要透明）和被动（要隐私）相结合的策略积极应对监控数据化的趋势。第三部分通过维基解密的案例，探讨了“强者要透明”的原则如何通过“密码朋克逆向监控”这一社会运动模式在实践中得以体现。最后，本文将密码朋克的数据行动主义置于“信息通量”的语境下，展示其在现有监控架构中的潜在应用价值。密码朋克提出的“弱者要隐私，强者要透明”的理念，连同其主动与被动相结合的数据行动主义策略，形成了一种全新的监控抵抗范式，或许能为系统性调整信息通量提供新的思考方向。密码朋克伦理观中的隐私与透明度要深刻理解密码朋克“弱者要隐私，强者要透明”这一核心原则的源起，就必须追溯至密码朋克运动作为对“加密战争”之回应的兴起背景。这场“战争”是美国政府与计算机专家、隐私倡导者及黑客之间的一场激烈博弈3。自 20 世纪 90 年代至 21 世纪初，随着密码朋克运动的发展，新一代密码朋克不仅承袭了前辈对于隐私保护的重视，更进一步强调了对机构透明度的诉求。尽管在维护弱者隐私权益方面，密码朋克群体内部达成了广泛共识，但在要求权力机构透明度的问题上，成员间却显现出了明显的代际差异。直至后续世代，密码朋克伦理的核心理念才得到了全面的阐述与深化。密码朋克组织最初形成于美国政府企图垄断密码学技术，阻止其向公众开放之时。1991 年初，时任参议院司法委员会主席的 Joe Biden 与其他议员联合提出了立法提案，意在禁止公民利用不可破解的加密技术保障电子文档和通信的安全。该提案要求所有电信运营商与设备制造商有义务确保，在合法授权下，政府能够获取语音、数据及其他通信形式的明文内容。面对这一提案，旧金山湾区的一批密码学爱好者迅速集结，发起抗议行动，旨在反抗政府对数字加密技术公共使用的限制，进而催生了密码朋克运动。90 年代期间，密码朋克与众多活动家联手对抗政府试图垄断密码学技术的举措，通过揭露并传播政府内部有关加密技术的机密资料，促使国家采取应对措施。尽管政府屡次尝试对密码朋克及其支持者提起诉讼或实施打压，但当联邦地区法院裁定加密软件属于代码范畴，而代码等同于言论，且言论自由受到宪法第一修正案保护之时，所有指控均告无效。法官 Betty Fletcher 明确表示：“政府对加密技术的管控……不仅可能侵犯密码学家依据第一修正案享有的权利，同时也可能触及我们每一个人作为加密技术受益者的宪法权益”3。鉴于密码朋克在加密战争中的角色，其规范理念中强调“为技术或资源有限的个体提供隐私”这一点就不足为奇了。密码朋克们常将数字时代的隐私威胁与美国十八世纪末期的情况相比较，那时美国宪法的制定者们正活跃于政坛10。彼时，人们可以在远离城镇的地方进行私人对话，几乎不受任何形式的监控。然而，进入数字时代后，几乎所有的通讯和经济活动都发生在联网的计算机系统上，若不采用加密技术，个人隐私便难以维系。因此，《密码朋克宣言》的作者 Eric Hughes 视密码学为“电子时代”保护隐私的关键手段。Hughes 将隐私定义为“选择性地向外界展示自我的能力”11。但他发现在数字交易和通讯中，“一旦我的身份因交易的技术流程而被揭露，我就丧失了隐私”，也就意味着“我无法选择性地展现自我；我必须始终处于曝光状态”11。但当个人采用加密技术时，便能重获掌控，因为人们能够自主决定如何向外界呈现自我。Hughes 指出，“我们不应期待政府、企业或任何大型、无名的组织会出于善意向我们提供隐私保护”，因此，“如果想要保留隐私，我们必须自行捍卫”。密码朋克对隐私的关注已广为人知，但人们往往仅聚焦于其对隐私的倡导，而忽视了其对“强者要透明”的强烈呼吁，后者实际上也是同样重要的议题，但却鲜少被讨论。在透明度问题上，密码朋克内部的分歧比在隐私议题上的分歧要大得多。为了更好地理解“强者要透明”的规范含义，对比两种不同的透明度理论颇有裨益：一是 Tim May 提出的基于加密无政府主义的透明度理念，二是 Julian Assange 倡导的以正义为中心的透明度理念。这两种视角不仅揭示了密码朋克运动内部的多元性，也展示了其对社会变革的深远影响。Tim May 认为，密码朋克运动的核心在于推动所谓的“加密无政府主义” (crypto anarchy)，这是一种融合了自由意志主义或无政府资本主义的思想体系，主张依靠密码技术的广泛应用来终结国家政府的权威。May 指出12，数字加密技术有可能从三个方面对国家政府的稳定性构成挑战。首先，加密通信将极大地限制政府监控和审查信息的能力，从而削弱其对思想和言论的控制力。其次，加密货币的兴起将削减政府对货币流通的控制权以及征税的能力。第三，随着“信息流动市场”的兴起，秘密的保守将变得更加困难。在 May 的构想中，信息市场由经济利益所驱动，这会导致政府机密等敏感信息被高价出售。此外，一些市场将成为举报人和记者的聚集地，令其可借助加密技术匿名揭露文件。May 认为，即便是 CIA 也无力阻止其秘密被泄露至新闻组、网站或网页。但同时 May 也指出密码技术的应用并不等同于全方位的透明。他援引密码朋克的口号称：“加密无政府主义并不意味着一个没有秘密的社会；相反，它强调个人必须自行保护其秘密，不能依赖政府或公司。这也意味着像军队调动和生产计划这样的‘公共秘密’将难以长期保密”13。总体而言，May 将透明度提升视为密码技术广泛运用的一种必然结果。May 提出的密码朋克透明度理念主要聚焦于政府以及秘密文件交易。相比之下，Julian Assange 14则将透明度视为实现正义的手段，其对象涵盖了政府、企业、情报机构以及主要政党等一切不断寻求自身权力的扩大化的强势实体。Assange 反对将秘密文件当作商品在黑市上交易，他主张公开这些文件的目的在于促进正义、责任和知识共享，而非服务于个人或组织的经济利益15。因此，Assange 将透明度视为推动机构改革、增强其公正性的手段，而非单纯市场力量的表现。作为第三代密码朋克的关键人物，Assange 为透明度的概念增添了道德维度，这是 May 所代表的第一代密码朋克未能充分探讨的。尽管 Assange 与 May 在透明度的具体实践中持有不同观点，但 Assange 同样认可加密技术作为社会和政治实践基石的重要性。Assange 通过构建加密的提交与发布平台维基解密 (WikiLeaks)，确保了举报者可以在不暴露身份的前提下，强制权力机构透明化，进而实现信息与权力的再分配。在此背景下，可以看出某些针对密码朋克运动的批评其实具有误导性，甚至存在根本性的误解。例如，Brin 对密码朋克的批评4其实源自未能准确把握该运动的核心理念，对数据收集问题的看法过于简单化，未能充分考虑不同个体和组织在分析和利用数据时的能力差异。这种忽视权力动态的观点，导致 Brin 错误地将“隐私” (privacy) 与“保密” (secrecy) 混为一谈，进而曲解了密码朋克的基本立场。在密码朋克看来，隐私与保密是截然不同的两个概念：隐私是个体及相对弱势群体应享有的权利，应通过加密技术加以保护；而保密则是强势组织掩盖其不义、不公乃至反民主行为的工具。在此背景下，透明度这一概念与个人及弱势组织的隐私无关，而是直接针对那些构成密码朋克所描述的“跨国监控反乌托邦”的政府、企业、主要政党及监控机构的保密行为。简而言之，对于密码朋克而言，隐私与保密是由权力关系界定的，而透明度是专门针对保密行为的。其次，将密码朋克与主流加密宣传活动混为一谈，实际上是误判了密码朋克运动的视域。一些批评指出，部分加密倡导者过度关注西方中产阶级白人关切，而忽视了深受数据化和监控之害的被殖民及被种族压迫群体16。这也揭示了一个关键问题：许多西方主流加密运动只是在反对政府对自己公民的监控，而对于本国推行的帝国主义和殖民主义外交政策则持默许态度。此类批评在一定程度上是适用于以隐私为中心的第一代密码朋克的， 如 May 等人，其观点中确实含有西方优越论的成分。但第三代密码朋克在推广加密技术的过程中，则展现出了更为国际化的视野以及反帝国主义的立场。他们拒绝在全球政治讨论中采取“西方中心主义”的视角14，并公开批评无人机战争、政权更迭行动、全球大规模监控以及帝国主义扩张等行为717。Julian Assange 本人更是明确表示，加密技术是反殖民主义斗争中的关键手段。他指出，“加密不仅能够保护个人的公民自由和权利，还能维护国家主权与独立，促进拥有共同目标的群体间的团结，以及推动全球解放的进程。它不仅是对抗国家对个人压迫的手段，也是抵制帝国对小国影响和控制的有效武器。”18由此，我们能够更深刻地理解“弱者要隐私，强者要透明”这一密码朋克伦理核心原则在全球范围内的深远意义和实际应用。密码朋克的观点体系包含了两大基本原则：一是保护个人隐私，二是追求机构透明度。一方面，密码学技术能够强化弱势群体的隐私保护，这是第一代密码朋克运动的核心主张；另一方面，密码学也被视为促进权力机构透明化的手段，这是后期密码朋克所坚持强调的重点。这一理念体系源自三十年前密码学倡导者们的政治参与实践，直到今天仍在为密码朋克运动在全球范围内开展的数据行动主义活动提供坚实的理论支撑。数据行动主义视角下的密码朋克认识论密码朋克运动主要利用信息技术和软件挑战当代社会的权力分配模式，该运动及其参与者由此常被贴上“黑客”的标签。一方面，密码朋克确实某种程度上继承了原始黑客文化的趣味性和创造性7，但将密码朋克简单地归类为“黑客”可能会在多个层面上扭曲我们对该运动的理解。首先，“黑客”一词本身具有高度争议性，对不同群体来说含义各异，将其应用于密码朋克可能会引发更多误解。其次，密码朋克运动的灵感主要来源于密码学家，而非传统意义上的黑客，这两个群体在起源上存在本质区别3。第三，大多数将密码朋克视为黑客的研究倾向于将隐私与透明度视为互斥的概念，未能认识到在密码朋克的理念中，隐私与透明度实际上是相辅相成的两个方面。为避免将密码朋克简单等同于黑客的模糊分类方式，我们应从数据行动主义的技术视角重新审视密码朋克。Stefania Milan 和 Lonneke van der Velden 提出19，当代数据行动主义可以通过三个关键特征来定义：（1）是对监控和社会数据化趋势的回应；（2）倡导实践性的参与方式；（3）不仅参与数字监控和数据技术的应用，同时对其进行批判性地挪用和改造。这些数据行动主义的特征共同构建了一种“替代认识论”，为描绘我们“数据化的社会现实”提供了新的叙事框架。他们认为，这种概念框架为更动态地理解以技术为中心的社会运动提供了有力工具。就其起源、倾向和原则而言，密码朋克运动完美诠释了这一数据行动主义概念，形成了一套独特的密码朋克认识论，加密技术在其中扮演着协调数字化社会关系的核心角色。首先，Milan 和 van der Velden 指出，“数据化带来了根本性的范式转变”19。自 20 世纪 80 年代末以来，学者们记录了从 20 世纪中叶的三重监控概念（物理监控、心理监控和数据监控）20向数据化与数字化相结合的新模式转变，这种新模式将前三种监控方式统一于数据监控21和数字监控22的框架之下。20 世纪的数字数据监控取代并整合了几乎所有旧形式的音频/视频及电磁数据监控。元数据现已成为监控的主要手段，其背后是一种将数据视为待收割的原材料的意识形态23。在此背景下，Milan 和 van der Velden 认为，数据行动主义者的驱动力在很大程度上源自其对数字数据监控普遍性的深刻认知。密码朋克对数字数据监控的高度敏感性可以追溯到该运动的根源。被誉为“数字匿名性的先知和教父”和“终极密码朋克”3的 David Chaum 是一位数学家和密码学家，他的研究对密码朋克运动的形成产生了重要影响。早在 20 世纪 80 年代初，Chaum 就关注到国家和全球计算机系统中个人通信和交易的可追踪性问题，他警告说：“一个档案社会正在形成，在这个社会中，计算机可以通过收集日常消费交易中的数据来推断个人的生活方式、习惯、行踪和关系。” Chaum 对早期数据化趋势的观点受到 David Burnham 的《计算机国家的崛起》24一书的启发，后者极具前瞻性地揭示了大规模计算机化官僚体系与数据库可能带来的风险，指出这些体系和数据库使得跟踪个人行为成为可能。Burnham 指出：“隐私的丧失是我们这个时代一个基本社会问题的关键症候，反映了大型公共与私人机构相对普通公民权力的日益增长”，特别是“计算机、数据库和计算机化通信网络”的使用，以及这些技术如何被权力机构用来“影响我们认为重要的事务”。可见密码朋克对强加密的倡导，根植于数据化引发的范式转变和对数字数据监控现象日益加剧的认识。其次，Milan 和 van der Velden 指出，数据行动主义者采取实践导向的方法，不仅从事理论研究，更注重实际行动，这源于其对“信息获取、代码编写、合作以及通过技术创新改善世界”等理念的认同。这种实践精神在 Hughes 的密码朋克宣言中得到了充分展现。Hughes 曾言：“密码朋克编写代码。密码朋克积极投身于提升网络隐私安全。”11实际上密码朋克也确实利用加密技术开发了众多数字工具，包括匿名邮件系统、加密货币以及其他隐私保护和匿名性应用程序。鉴于“隐私的维护有赖于社会成员之间的合作”，Hughes 强调，“密码朋克的代码向所有人免费开放”。密码朋克不仅编写代码，还将之共享给广大用户，这一做法至今仍被视为密码朋克社区的核心准则7。此外，Milan 和 van der Velden 注意到，数据行动主义者在对抗的同时还巧妙地利用数字监控和数据技术。数据行动主义具有多重意义和整体性：多重意义体现在其采用了“多种互补的手段来实现政治目标”；整体性则在于其深刻认识到“‘大数据’现象既存在负面影响也具备正面价值”。因此，数据行动主义既包括被动数据行动主义，即采取“加密或匿名网络等措施抵御国家和企业的监控”；也包括主动数据行动主义，即利用数据来“构建社会现实的替代性叙述、质疑其他描述的真实性和权威性、揭露不公并推动社会变革”19。虽然在 Brin 等人的分析中，这两种策略看似冲突，但 Milan 和 van der Velden 认为二者实际是相辅相成的，因为“两者都视信息为塑造社会现实的关键社会力量”。密码朋克的口号“弱者要隐私，强者要透明”十分恰当地概括了数据行动主义对数据化现象的回应。用加密技术保障弱势群体隐私实际上就是在实践被动的行动主义策略。诸如端到端加密的即时通讯应用、加密邮件服务、虚拟私人网络 (VPN)、Tor 浏览器和比特币等工具都是该策略的具体体现。而利用加密技术提高权力机构的透明度则代表了主动的行动主义立场。自 20 世纪 90 年代起，密码朋克不仅论述了加密技术在提升透明度方面的作用，还实际构建了若干基于加密技术的透明度平台，如 Tim May 的 BlackNet 及 John Young 的 Cryptome.org13。Julian Assange 创办的维基解密，可能是这种主动数据行动主义模式最为成熟的实例。维基解密通过安全加密的匿名提交机制，为举报者提供了强大的保护，不仅减少了举报者被发现的风险，也由此降低了 Assange 所说的“勇气门槛”。鉴于大型权力机构的运行无法脱离大规模的数字官僚体系，Assange 认为，仅凭文件泄露的可能性，就足以迫使这些机构采取防御措施，从而削弱其执行秘密有害政策的能力。14Milan 和 van der Velden 将数据行动主义视为一个连续谱系，被动性和主动性数据行动主义分别位于谱系的两端。他们提到某些运动能够融合这两种极端，但未深入探讨密码朋克如何成为这种框架下数据行动主义的典范。以这种方式来理解作为数据行动主义者的密码朋克，不仅可以更深入地把握密码朋克伦理观的复杂性，也能更好地认识到将被动性和主动性数据行动主义相结合，形成一种连贯的社会与技术参与方式的重要性。此外，尽管一些其他的社会运动也可被归类为数据行动主义，但只有密码朋克认识论着重强调了加密技术的重要性：该运动明确指出，其根基在于技术7。很多社会运动都会利用加密来确保通信的安全性，但密码朋克的特别之处在于其将加密技术视为运动的核心。这种对加密技术的重视意味着密码朋克认识论深入到了电子通信的本质逻辑之中，这是其他认识论所难以企及的深度。在对最早的电子媒介——电报的研究中，James Carey 指出，“电报不仅是商业的新工具，也是一种新的思维方式，一种重塑思想的媒介”，它“重塑了意识的本质”25。电报为加密通信革命奠定了基础。正如密码史学家 David Kahn26所言，“电报造就了现代密码学。”结合 Carey 和 Kahn 的见解可知，加密技术已经成为电子时代人类意识的基本构成部分之一。如果将计算机视为电报的延续，那么密码朋克认识论将因其在技术和伦理层面对加密技术的重视而在各种形式的数据行动主义中独树一帜。作为数据行动主义的一种形式，密码朋克认识论不仅深入理解社会关系的数据化进程，还通过主动与被动、进攻与防御的策略来应对这一进程，一方面通过抵抗监控来挑战现有的权力结构，另一方面又积极参与到了促成当代监控体系的数字技术中，通过利用和改造将其转化为反抗的工具。密码朋克逆向监控：重新审视维基解密维基解密的创始人 Julian Assange 曾是密码朋克邮件列表的早期参与者之一，至今仍被视为该运动后继世代中最重要的代言人之一。在创立维基解密时，Assange 深受密码朋克“强者要透明”理念的启发。Assange 曾解释说：“密码朋克精神引导我思考如何最有效地对抗那些压迫性机构——无论是政府、企业还是监控组织……这些机构通常依赖于对数据的掌控，以此作为伤害、压迫或压制异议的手段。我认为，密码朋克精神能够保护人们免受此类伤害。”由此，维基解密成为了密码朋克逆向监控实践的典范。密码朋克逆向监控的理念与传统逆向监控理论一样，认为权力关系和权威是监控与逆向监控的核心议题27，但同时也引入了更加现代的逆向监控形式，突破了传统的音/视频逆向监控框架，探索了利用网络和数据库来实现“对监控者的监控”。在从音/视频逆向监控到数字数据逆向监控演变的过程中，理论家们往往更关注逆向监控的内容，而相对忽略了逆向监控的形式。密码朋克逆向监控则在当代数字数据逆向监控的语境下，重新强调了逆向监控形式的重要性。传统音/视频逆向监控理论明确指出，逆向监控的内容与形式均为实践的关键要素。在传统的逆向监控场景中，逆向监控者利用音/视频技术来对抗同类监控，形成了“摄像机对摄像机”的对峙局面。28这种做法常被用来对抗执法机构，被称为“警察监视” (cop watching)。在这种情境下，逆向监控者会记录被拍摄对象（如警察或其他实体）的信息，相关资料日后可在法律诉讼等过程中作为权力滥用的证据。除了为逆向监控者提供内容之外，音频/视频逆向监控的形式也为其对象创造了一个全新的媒体环境，迫使其调整自身行为。在“警察监视”的情境中，公民不必等到警察暴力事件发生后才开始记录警察的行为，很多情况下，这种逆向监控都是预防性的。Singh 在其所谓的“预设监控” (Prolepticon) 29警务环境中观察到，“携带手机的公民不断预见到可能发生的负面执法行为并拍摄执法过程，影像由公民控制，并可通过互联网全球传播，这营造出了一个警察不确定自己是否正被监控的环境，进而促使警察成为自我约束的主体。在“预设监控”中，关键并不在于视频所记录下的具体内容，而在于手机摄像头的无处不在。当警察无法确定何时、何地或被何人拍摄时，就必须始终假定自己可能处于被拍摄的状态，进而避免采取不希望被记录和公开的行为。维基解密作为深受密码朋克伦理影响的平台，实现了独特的密码朋克逆向监控模式。该平台利用数字技术收集文档与数据，并将其公之于众，利用网络实现信息的全球传播。与其它形式的数字逆向监控相仿，维基解密不仅促进了对监控内容的分析，还推动了对监控体系图景的描绘，进而揭露隐秘的权力运作模式。维基解密的部分公开资料，勾勒出了国家监控能力的轮廓，例如 Vault 7 系列资料中涵盖了“数千页详述中央情报局用于破解智能设备、电脑乃至互联网电视的高级软件工具与技术的文档”30，首次向世界展示了未为人知的数据与设备安全漏洞，也为个人的安全与隐私保护措施调整提供了依据。此外，维基解密的其他公开资料，如伊拉克战争日志、阿富汗战争日记及美国国务院电报等，也为公众揭开了权力机构和军事人员在封闭决策过程中的真实样貌，为民主监督创造了机会17。维基解密通过实践数据逆向监控的理念，向全球民众提供了关于其自身被监控状况的信息，并揭露了权力机构的行为模式。但维基解密作为密码朋克逆向监控的典型代表，其意义远不止于此。维基解密不仅从内容上揭露监控实况、报道精英活动，更以其形式本身在系统层面干预并重塑了权力结构。维基解密所推动的逆向监控将所有内部人士都转变为了潜在的信息源和举报人。与 Singh 所述的“预设监控”情境类似，维基解密力图营造一种令政经精英们无法确定自己是否处于监控之下的氛围，进而利用这种不确定性迫使其成为自我约束的主体。要深入理解维基解密所带来的影响，可以参考 Steve Mann 关于开放系统、闭环系统以及反馈循环的论述31。9·11 恐怖袭击事件后，美国政府出台了一系列旨在强化国家安全的措施，与此同时，政府的保密程度与监控力度也得到了显著提升。这些举措名义上是为了对抗恐怖主义，但 Mann 认为，政府在恐怖主义问题上的看法有误。政府倾向于认为，9·11 恐怖袭击得逞的原因在于情报机构的监控不足。而 Mann 则提出，恐怖主义实际上是对于政府不负责任行为的一种反应。在他看来，虽然政府认为隐私保护给予了恐怖分子可乘之机，但事实上正是过度的保密措施激发了恐怖主义情绪。因此，增强政府的保密性非但不能解决问题，反而可能使之恶化。缺乏有效的外部反馈，政府在不断加强保密的同时，也会无限制地扩张其权力。正如 Mann 所言：“秘密组织往往以开环模式运行，缺少能够提供关键制衡的正常反馈机制……问题的核心并不在于隐私。不应将责任归咎于那些没有被拍摄、没有留下指纹、没有被监控的公民，而是应当终止那套更大的高压锅式的机制。”只要实施有效的逆向监控，就能改变反馈机制进而影响系统的运行方式。Assange 以相似的逻辑指出，保密性是所有政府腐败、不当行为和权威主义的核心诱因。他认为，通过构建看似无所不在的逆向监控环境，可以降低机构保密水平，进而减轻因腐败引发的负面影响。在此背景下，“阴谋”被定义为秘密策划并协同执行有害行为的过程；据此定义，任何因保密而产生的有害政府或企业政策均可被视为阴谋。政府之所以对其不道德的计划保密，是因为担心一旦公众知晓并持反对态度，这些计划就可能被阻止。然而，Assange 指出，现代政府依赖庞大的官僚体系来维持日常运作，进入数字时代后，这些官僚体系将记录存储于计算机网络之中，这为内部人员披露大量文件资料提供了更加便捷的渠道。尽管 Assange 的黑客伦理原则禁止非法窃取文件6，但并不妨碍他建立一个加密的举报平台。Assange 在创立维基解密时写道：“我们认识到，发起一场全球范围内的大规模泄密运动，是我们所能采取的最为有效的政治干预手段”13。当权力机构文件泄露时，原本开放的反馈循环将以两种方式得以闭合。首先是内容层面：公众能够阅读这些文件，对曝光的计划表达异议，这就可能阻止计划的实施14。其次是形式层面：由于机构无法确定何时会遭受内部人员的监控，这会迫使其缩小内部通讯网络，从而削弱机构日常运作的效率，降低其实施不道德计划的能力32。密码朋克逆向监控对逆向监控的理论和实践做出了两项重要的贡献。一方面，在从音/视频逆向监控转向数字数据逆向监控的过程中，理论研究者逐渐将焦点更多地放在内容上。密码朋克逆向监控则重新将我们的关注点引导到数字数据逆向监控的形式上，这有助于我们理解 Singh 的“预设监控”概念如何突破传统音/视频模式的局限实现更广泛的扩展。另一方面，鉴于加密技术在密码朋克伦理观和认识论中的核心地位，密码朋克逆向监控进一步表明，加密技术的意义远不止于增强隐私保护，实践证明加密技术可以通过建立加密的举报和发布平台，有效促进逆向监控的发展。密码朋克与信息黑洞密码朋克伦理观为应对数字监控技术提供了规范性框架；密码朋克认识论则体现为结合了主动与被动策略的数据行动主义，旨在挑战现有的监控、信息流通和权力分配格局。作为这两者结合的产物，密码朋克逆向监控为实现密码朋克追求的权力透明化目标提供了一条实现路径。整体而言，密码朋克的思想体系为我们理解和应对当前的监控环境带来了全新的视角，其核心理念和实践与当今最强大的公共和私有监控机构——美国国家安全局 (NSA) 和谷歌——形成了鲜明对比。借由 Bossewitch 和 Sinnreich33提出的“信息通量” (information flux) 概念，我们可以更好地理解 NSA 和谷歌等机构的运作方式。这些机构力求成为所谓的“信息黑洞”，即尽可能多地收集外部数据，同时极力防止自身数据的外泄。在由这类监控机构主导的信息环境中，密码朋克提供了一种有效的应对策略。秉持着“弱者要隐私，强者要透明”的理念，密码朋克在倡导使用加密技术来保护个人数据的安全的同时，也致力于对大型权力组织数据的收集。从这个角度讲，密码朋克伦理为大数据时代的个体提供了必要的概念框架、实践方法和工具，为构建更加公平的信息交流环境奠定了基础。Bossewitch 和 Sinnreich 借鉴了物理学中的“通量”概念（即通过特定界面的物质流动速率）来描述现代信息流。他们指出，近年来网络空间中的信息通量呈现出指数级的增长趋势，但由于这些通量是多向的，在个人、组织和系统之间来回流动，因此理解信息网络中每个实体如何与网络交互变得尤为重要。研究整个信息网络，可以帮助我们了解信息流动的净方向，明确哪些实体在积极收发信息及其规模如何。Bossewitch 和 Sinnreich 进一步指出，分析信息通量有助于我们理解在当代监控体系和网络社会中“正在形成的知识/权力动态”。这种理解不仅能揭示信息控制的现状，也为探讨如何打破现有的不平衡、促进信息的公平流通提供了理论基础。信息通量存在的三种模式：中性、正向和负向。这些术语在语义上是中性的，仅用于描述不同类型的流动，在特定情境下，负信息通量可能带来正面效果，而正信息通量则可能产生负面影响。中性通量则指所有参与者对信息享有平等和开放的访问权限，即实现了“完全透明”的状态。在 Bossewitch 和 Sinnreich 看来，当前的监控环境下，最值得关注的是正向和负向两种通量。正向通量描述了一方比另一方更多地泄露或发送信息的情形，这表明前者在信息访问权方面处于劣势。相反，负向通量描述了一方比另一方收集更多信息的情况，这意味着前者在信息掌控上占据优势。基于“信息即权力”的原则，追求权力的个体或机构将努力保持最大的负向通量。某些行为体可能倾向于成为所谓的“贪婪收集者”，即采取积极的信息收集策略，但未必会严格防止自身数据的泄露。而追求更大权力的行为体，则会力求成为“信息黑洞”，即“试图从外部尽可能多地收集和分析信息，同时尽可能减少自身信息的泄露”33的行为体。从这一点上看，信息黑洞的概念与数据行动主义其实有一定的相似性，因为成为黑洞意味着既要积极获取信息（主动策略），同时又要保护自身数据不被泄露（被动策略）。在 Bossewitch 和 Sinnreich 的构架中，信息黑洞是一个既多元又统一的概念，会通过一系列独立而互补的策略，不断增强其权力并巩固其在信息体系中的地位。黑洞策略在全球两大监控巨头：NSA 和谷歌身上得到了完美体现。NSA 作为信息通量黑洞，其目标是全面掌握机构外部所有个人和组织的信息，同时极力保持自身的绝对机密性——即追求绝对的负向通量。Edward Snowden 泄露的文件揭示了 NSA 的一条监控原则：“嗅探一切，了解一切，收集一切，处理一切，利用一切”34。而同时，NSA 对外部公开的信息则极其有限，以至于常被戏称为“不存在的机构” (No Such Agency)。NSA 在一项秘密行政命令下成立，在美国政府公开承认其存在之前，已经秘密运作了十多年。位于马里兰州米德堡的 NSA 总部，以其著名的“三层围栏”著称，象征着其对组织机密性的极致追求。正如 Steven Levy 所述：“围绕 NSA 总部的三层电网和铁丝网不仅是物理障碍，更是 NSA 近乎偏执地隐藏其自身及活动信息的象征。”3尽管 Snowden 的爆料说明即使是 NSA 级别的保密措施也无法完全防止数据泄露，但 NSA 仍在努力成为信息黑洞，力求实现信息的单向流动，所有信息只进不出。谷歌同样追求成为信息黑洞。为了满足广告收入的需求，谷歌在数据收集上表现得异常激进，几乎无孔不入地收集用户的各类信息；这种监控资本主义模式最终促使谷歌的业务从搜索引擎扩展至电子邮件、浏览器、操作系统和智能手机等多个领域，从而能够捕获大量的用户活动数据。从某种程度上讲，谷歌堪称私营领域中“嗅探一切，了解一切，收集一切”的机构。与 NSA 一样，谷歌对其核心技术和算法保持着高度的保密，因为在用户不知情的情况下收集数据的能力正是其商业成功的关键。前谷歌高管 Douglas Edwards 曾提到，联合创始人 Larry Page 坚决反对任何可能泄露技术秘密或引发隐私争议、威胁到公司数据收集能力的行为35。谷歌并未给予用户对其数据收集过程的知情同意权，而是选择在“完全保密”的状态下运营。与 NSA 一样，谷歌虽然难以做到完全不泄露信息，但同样致力于通过维持最大的负信息通量成为信息黑洞。面对诸如 NSA 和谷歌等庞大监控机构所构建的信息黑洞，密码朋克伦理提出了一种简洁而有力的应对策略：个人也应努力成为信息黑洞。密码朋克倡导的“弱者要隐私，强者要透明”不仅号召公民和活动家保护个人隐私，还鼓励大家积极参与到增强隐私保护和推动透明度提升的项目中。密码朋克伦理的指导原则赋予了个人学习、掌握并应用技术工具的能力，以便为自己创造负向信息通量，既能在监控环境中保护个人或组织的数据（被动防御），又能积极收集有关其网络环境的信息（主动进攻）。隐私保护一直是密码朋克关注的焦点，当这一关注点与密码朋克逆向监控相结合时，便产生了更为深刻的意涵。例如，Julian Assange 曾将维基解密描述为“人民的情报机构”15，这表明维基解密实际上是从弱势群体的角度出发在模仿 NSA 的工作方式。不过 Assange 强调，维基解密与传统国家情报机构的关键区别在于，前者公开分享其调查结果，而后者则维持高度保密。这一点至关重要，因为如果维基解密真正成为了人民的情报机构，且确实践行了密码朋克逆向监控的理念，那么它就成为了吹哨人、记者和活动家用来挑战如 NSA、谷歌等信息黑洞、抗衡当代监控社会中权力动态并实现对强者问责的工具。作为主动数据行动主义的象征，维基解密致力于阻止强大的监控机构实现绝对负向信息通量，从而为构建更加公平、透明的信息生态系统贡献力量。当然，正如 NSA 和谷歌难以实现绝对负通量一样，个人也无法做到这一点。不过实现绝对负向通量并不是密码朋克伦理的最终目标。密码朋克强调的是保护隐私，而隐私的本质是个体自由且选择性地向外界展示自己的能力。因此，绝对负向通量不仅不现实，而且也不符合密码朋克的精神，因为这意味着人际交往的隔绝。尽管如此，但密码朋克伦理至少提供了一个实用的基本框架，来帮助个人理解如何通过积极参与技术和网络通信系统来控制和调整信息流，进而以此恢复权力平衡。关于信息通量和黑洞的讨论，使我们能更动态地理解监控机构与反抗社会运动之间的关系。Yasha Levine 曾将密码朋克运动描述为美国公私监控体系的对立面36，他写道：  密码朋克对未来的展望是五角大楼和硅谷所追求的军事网络梦想的反面：不是利用全球计算机系统使世界更加透明和可预测，密码朋克希望通过计算机和密码学使世界变得不透明和不可追踪。这是一种反作用力，一种旨在保护个人隐私和自由的网络武器，用以对抗政府监控和控制的网络武器。尽管该诠释揭示了密码朋克运动的核心理念，但此视角仍有一定的局限性。从信息通量的角度来看，五角大楼和硅谷与挑战其权威的社会运动之间的关系，并非简单的“反转”。实际上这是一个多方面竞争的过程，各方都在争夺对信息流的控制权。五角大楼和硅谷追求的不仅仅是透明度，而是希望他人透明同时自身保密。同样，密码朋克的关注点也不仅限于保护弱者的隐私，也同样追求强者的透明。因此，这场斗争并不是透明与隐私之间的零和博弈，而是一场黑洞间为了各自利益而争夺信息通量控制权的较量。总结密码朋克运动为对抗当前的全球数据监控体系提供了简单有效的参与模型。密码朋克伦理提出了“弱者要隐私，强者要透明”的规范性原则，旨在保护权力结构中处于弱势地位的实体，并对权力机构进行制约。密码朋克认识论将加密技术视为有力的数据行动主义手段，通过主动与被动相结合的策略，可以有效实现其规范性原则。密码朋克逆向监控是追求权力机构透明化的一种实践范式，强调数字数据逆向监控在形式和内容上的双重效益。总体而言，密码朋克的世界观呼吁个体尽力成为信息黑洞，以从系统层面控制并改变信息通量，进而使权利结构的重塑成为可能。实践者通过行动与反思构建其道德世界。理论与实践是相辅相成的，通过考察密码朋克等行动者的理念，我们得以更深入地反思和质疑主导自身认识环境的主流叙事与思维方式。密码朋克运动创造的新范式是对当代权力机构监控策略的有力回应，并在技术、道德、认知和实践层面为其愿景的实现提供了全面的支持。未来，密码朋克的核心理念和实践策略，以及对信息权力关系的深刻洞察，将继续为我们理解和应对复杂的数据监控社会提供帮助，为构建更加公平、透明的信息生态贡献力量。参考            Schneier, Bruce. 1996. Applied Cryptography: Protocols, Algorithms and Source Code in C. Second edition. &#8617;              Manne, Robert. 2011. The Cypherpunk Revolutionary. The Monthly, February 16. http://archive.fo/kwI60 . &#8617; &#8617;2              Levy, Steve. 2001. Crypto: How the Code Rebels Beat the Government—Saving Privacy in the Digital Age. &#8617; &#8617;2 &#8617;3 &#8617;4 &#8617;5 &#8617;6              Brin, David. 1998. The Transparent Society: Will Technology Force Us to Choose Between Privacy and Freedom? Reading, MA: Addison-Wesley. &#8617; &#8617;2              The Inevitability of the Transition from a Surveillance-Society to a Veillance-Society: Moral and Economic Grounding for Sousveillance. Presented at The IEEE International Symposium on Technology and Society, Ontario, CA, June 27–29, 243–254. Piscataway, NJ: IEEE. &#8617;              Dreyfus, Suelette, and Julian Assange. 2012. Underground. &#8617; &#8617;2              Assange, Julian, Jacob Appelbaum, Andy Müller-Maguhn, and Jérémie Zimmermann. 2012. Cypherpunks: Freedom and the Future of the Internet. &#8617; &#8617;2 &#8617;3 &#8617;4 &#8617;5              Moore, Adam D. 2011. Privacy, Security, and Government Surveillance: WikiLeaks and the New Accountability. Public Affairs Quarterly 25 (2): 141–156. &#8617;              Assange, Julian. 2014. Who Should Own the Internet? New York Times, December 4. https://archive.fo/vxLJd &#8617;              Castronovo, Russ. 2013. Ben Franklin and WikiLeaks. Critical Inquiry 39 (3): 425–450 &#8617;              Hughes, Eric. 2001. A Cypherpunk’s Manifesto. In Crypto Anarchy, Cyberstates, and Pirate Utopias, edited by Peter Ludlow, 81–84. &#8617; &#8617;2 &#8617;3              May, Timothy. 2001a. The Crypto Anarchist Manifesto. In Crypto Anarchy, Cyberstates, and Pirate Utopias, edited by Peter Ludlow, 61–64. &#8617;              Greenberg, Andy. 2012. This Machine Kills Secrets: How WikiLeakers, Cypherpunks, and Hacktivists Aim to Free the World’s Information. &#8617; &#8617;2 &#8617;3              Assange, Julian. 2016. When Google Met WikiLeaks. &#8617; &#8617;2 &#8617;3 &#8617;4              Assange, Julian. 2011. Of the People and For the People. &#8617; &#8617;2              Gürses, Seda, Arun Kundnani, and Joris Van Hoboken. 2016. Crypto and Empire: The Contradictions of Counter-Surveillance Advocacy. Media, Culture &amp; Society 38 (4): 576–590. &#8617;              Assange, Julian. 2015. Introduction: WikiLeaks and Empire. In The WikiLeaks Files: The World According to US Empire, by WikiLeaks, 1–19. &#8617; &#8617;2              Assange, Julian. 2013. How Cryptography is a Key Weapon in the Fight Against Empire States. The Guardian, July 9.https://archive.ph/Mbsx4 &#8617;              Milan, Stefania, and Lonneke van der Velden. 2016. The Alternative Epistemologies of Data Activism. Digital Culture and Society 2 (2): 57–74. &#8617; &#8617;2 &#8617;3              Westin, Alan. F. 1967. Privacy and Freedom. &#8617;              Clarke, Roger. A. 1988. Information Technology and Dataveillance. Communications of the AMC 31(5): 498–512. &#8617;              Graham, Stephen, and David Wood. 2003. Digitizing Surveillance: Categorization, Space, Inequality. Critical Social Policy 23 (2): 227–248. &#8617;              van Dijck, José. 2014. Datafication, Dataism and Dataveillance: Big Data Between Scientific Paradigm and Ideology. Surveillance &amp; Society 12 (2): 197–208. &#8617;              Burnham, David. 2014. The Rise of the Computer State: The Threat to Our Freedoms, Our Ethics and Our Democratic Process. &#8617;              Carey, James. 2009. Culture as Communication: Essays on Media and Society. Revised edition. &#8617;              Kahn, David. 1967. The Codebreakers: The Story of Secret Writing. &#8617;              Mann, Steve, and Joseph Ferenbok. 2013. New Media and the Power Politics of Sousveillance in a Surveillance-Dominated World. Surveillance &amp; Society 11 (1/2): 18–34. &#8617;              Mann, Steve, Jason Nolan, and Barry Wellmann. 2003. Sousveillance: Inventing and Using Wearable Computing Devices for Data Collection in Surveillance Environments. Surveillance &amp; Society 1 (3): 331–355. &#8617;              Singh, Ajay. 2017. Prolepticon: Anticipatory Citizen Surveillance of the Police. Surveillance &amp; Society 15 (5): 676–688. &#8617;              Shane, Scott, Matthew Rosenberg, and Andrew W. Lehren. 2017. WikiLeaks Releases Trove of Alleged C.I.A. Hacking Documents. New York Times, March 7. https://archive.fo/nY9Hs &#8617;              Mann, Steve. 2002. Sousveillance, Not Just Surveillance, in Response to Terrorism. Metal and Flesh 6 (1): 1–8. &#8617;              Assange, Julian. 2006. Conspiracy as Governance. Cryptome.org, December 3. http://archive.fo/kr8Pr &#8617;              Bossewitch, Jonah, and Aram Sinnreich. 2012. The End of Forgetting: Strategic Agency Beyond the Panopticon. New Media &amp; Society 15 (2): 224–242. &#8617; &#8617;2              Greenwald, Glenn. 2014. No Place to Hide: Edward Snowden, the NSA, and the US Surveillance State. &#8617;              Zuboff, Shoshana. 2019. The Age of Surveillance Capitalism: The Fight for a Human Future at the New Frontier of Power. &#8617;              Levine, Yasha. 2018. Surveillance Valley: The Secret Military History of the Internet. &#8617;      "
  },
  
  {
    "title": "隐私保护工具推荐",
    "url": "/posts/privacyprotection/",
    "categories": "Privacy",
    "tags": "privacy",
    "date": "2024-08-13 12:00:00 +0800",
    





    
    "snippet": "  “We must defend our own privacy if we expect to have any.”  — A Cypherpunk’s Manifesto本文是个人使用体验较好的各类隐私保护工具的推荐，以开源软件为主，不定期更新。网页浏览 Firefox完全开源的浏览器，支持大量隐私插件，内置反追踪功能，可以阻止第三方 Cookie，可以设置 DNS over HTTP...",
    "content": "  “We must defend our own privacy if we expect to have any.”  — A Cypherpunk’s Manifesto本文是个人使用体验较好的各类隐私保护工具的推荐，以开源软件为主，不定期更新。网页浏览 Firefox完全开源的浏览器，支持大量隐私插件，内置反追踪功能，可以阻止第三方 Cookie，可以设置 DNS over HTTPS 加密 DNS 查询。安装后需要进行一定的安全配置，参考 @arkenfox user.js 、restore privacy 或 12bytes。  官网：mozilla.org/firefox  隐私：tosdr.org/en/service/188  开源：✅ LibreWolfLibreWolf 是 Firefox 的一个独立分支，提供更严格的默认设置，以保障用户隐私、安全性和自由。禁用 Mozilla Telemetry，切断了与 Google（Safe Browsing）的关联，内置了内容拦截器 uBlock Origin，隐私默认设置参考了 Arkenfox project 等研究成果。  官网：librewolf.net  隐私：tosdr.org/en/service/6389  开源：✅ Tor BrowserTor Browser 基于 Firefox ESR 开发，由 Tor 项目维护。基于洋葱路由技术提供匿名层，所有流量通过 Tor 网络加密传输，隐藏用户真实 IP 地址，通过多层加密和多跳转发实现匿名。速度会比普通浏览器慢，某些网站可能无法访问。  官网：torproject.org  隐私：tosdr.org/en/service/2845  安卓：https://play.google.com/store/apps/details?id=org.torproject.torbrowser  开源：✅邮件客户端 Thunderbird由 Mozilla 开发和维护的免费开源邮件客户端，从 v78.2.1 起内置了 OpenPGP 的加密和验签功能，可使用 TorBirdy 扩展通过 Tor 网络路由所有流量。Betterbird 等分支添加了额外功能。  官网：thunderbird.net  隐私：tosdr.org/en/service/3365  开源：✅ Tuta免费的开源电子邮件服务，支持匿名注册，并提供加密日历，有免费和付费版本，支持桌面、Web和移动端。与其他加密邮件提供商不同，Tuta 不使用 OpenPGP，而是采用了由对称和非对称加密算法（包括 AES256、RSA 2048 或 ECC (x25519) 和 Kyber-1024）构成的混合方案，在与使用 PGP 的联系人沟通时会有兼容性问题。但 Tuta 会加密更多的标头数据（如正文、附件、主题行和发件人姓名等），这是 PGP 邮件提供商无法做到的。最近 Tuta 加密算法的升级使得通过其服务存储和发送的数据能够抵御量子计算机的攻击。  官网：tuta.com  隐私：tosdr.org/en/service/157  iOS：apps.apple.com/us/app/encrypted-email-tuta/id922429609  安卓：https://play.google.com/store/apps/details?id=de.tutao.tutanota  开源：✅密码管理 KeePass本地密码管理器，没有内置云同步功能（符合安全密码管理器的黄金标准）。支持高强度加密，可以生成随机密码。KeePass 客户端：Strongbox（Mac 和 iOS）、KeePassDX（Android）、KeeWeb（基于 Web/自托管）、KeePassXC（Windows、Mac 和 Linux）  官网：keepass.info  开源：✅ Bitwarden具有云同步功能的全功能开源密码管理器，所有数据端到端加密，支持双因素认证，有免费和付费版本，支持桌面、Web和移动端。Vaultwarden 是 Bitwarden 服务器的自托管 Rust 实现，与 Bitwarden 客户端 兼容。  官网：bitwarden.com  隐私：tosdr.org/en/service/1348  iOS：apps.apple.com/us/app/bitwarden-password-manager/id1137397744  安卓：https://play.google.com/store/apps/details?id=com.x8bit.bitwarden  开源：✅文件加密 VeraCrypt开源的跨平台磁盘加密软件，可加密特定文件或目录，或者整个磁盘或分区。 VeraCrypt 功能丰富，支持多种加密算法和哈希算法，提供易用的 GUI，同时也有 CLI 版本和便携版本。VeraCrypt 是在 TrueCrypt 的基础上开发的。  官网：veracrypt.fr  开源：✅ Cryptomator针对云文件的开源加密客户端，Cryptomator 加密时可保留单个文件结构，能透明集成各种云服务。与 VeraCrypt 相比，加密方式的可选项较少。Cryptomator 可跨平台运行，支持移动端。  官网：cryptomator.org  隐私：tosdr.org/en/service/4403  iOS： apps.apple.com/us/app/cryptomator/id1560822163  安卓： https://play.google.com/store/apps/details?id=org.cryptomator  开源：✅加密通讯 Signal端到端加密的跨平台即时通讯软件，所有通信内容都经过加密 (Signal Protocol)，不存储用户元数据，支持消息自动删除。支持已读回执、多媒体附件、音视频通话等常用功能。曾受到 Edward Snowden 推荐。  官网：signal.org  隐私：tosdr.org/en/service/528  iOS：apps.apple.com/us/app/signal-private-messenger/id874139669  安卓：https://play.google.com/store/apps/details?id=org.thoughtcrime.securesms  开源：✅ Matrix去中心化的加密通讯协议，采用 Olm 和 Megolm 进行端到端加密。可以自建服务器，有 Element 等多个客户端可选。  官网：matrix.org  隐私：tosdr.org/en/service/2455  开源：✅P2P 加密通讯 Briar基于 Tor 网络的点对点加密通讯工具，无中心服务器，内容存储在设备本地，支持离线消息，可直接与附近的联系人连接而无需访问互联网（使用蓝牙或 LAN）。支持安卓移动端及桌面端。  官网：briarproject.org  隐私：tosdr.org/en/service/2559  安卓：https://play.google.com/store/apps/details?id=org.briarproject.briar.android  开源：✅ Jami点对点加密聊天网络，支持音视频通话、屏幕共享、在线会议和即时消息。 支持移动端并提供跨平台 GNU 客户端。  官网：jami.net  IOS：apps.apple.com/ca/app/jami/id1306951055      安卓：https://play.google.com/store/apps/details?id=cx.ring    开源：✅两步验证 2FAS支持 iOS 和安卓的免费开源身份验证器。支持创建加密备份并在设备间同步，无需帐户。  官网：2fas.com  隐私：tosdr.org/en/service/8201  iOS：apps.apple.com/us/app/2fa-authenticator-2fas/id1217793794  安卓：https://play.google.com/store/apps/details?id=com.twofasapp  Discord：q4cP6qh2g5  开源：✅ Aegis安卓平台的免费开源身份验证器。具有备份/恢复功能，用户界面可定制，支持夜间模式。  官网：getaegis.app  隐私：tosdr.org/en/service/4076  安卓：https://play.google.com/store/apps/details?id=com.beemdevelopment.aegis  开源：✅匿名网络 I2PI2P 提供了优秀的通用传输方式，很适合访问隐藏服务，并且在技术上相比 Tor 有几项明显的优势：P2P 友好，采用单向短期隧道；使用 TCP 和 UDP 的分组交换（而非电路交换） ；能够持续分析以选择性能最佳的节点。I2P 比 Tor 更去中心化，是完全分布式和自组织的，体量较小使其尚未遭遇大量封锁或拒绝服务攻击。  官网：geti2p.net  开源：✅ Freenet分布式匿名存储网络，支持隐私文件共享，抗审查能力强，完全点对点架构  官网：freenetproject.org  开源：✅笔记工具 Standard Notes端到端加密笔记应用，支持多端同步，可通过扩展系统添加待办列表、电子表格、富文本、Markdown、数学编辑器、代码编辑器等功能，它具有内置的安全文件存储、标签/文件夹、快速搜索等功能，注重长期数据保存。  官网：standardnotes.com  隐私：tosdr.org/en/service/2116  iOS：apps.apple.com/us/app/standard-notes/id1285392450  安卓：https://play.google.com/store/apps/details?id=com.standardnotes  开源：✅ Joplin跨平台开源笔记应用，支持加密同步，提供 Markdown 编辑器，支持附件加密。  官网：joplinapp.org  隐私：tosdr.org/en/service/9477  iOS：apps.apple.com/gb/app/joplin/id1315599797  安卓：https://play.google.com/store/apps/details?id=net.cozic.joplin  开源：✅PGP 管理 KleopatraLinux 平台的证书管理器及通用加密 GUI，支持管理 GpgSM 密钥箱中的 X.509 和 OpenPGP 证书，可从 LDAP 服务器检索证书。Windows 版本：GPG4Win      官网：apps.kde.org/kleopatra        开源：✅   OpenKeychain用于管理密钥和加密消息的安卓应用，既可单独使用，也可集成到其他应用中，如 K9-Mail、Conversations 等。  官网：openkeychain.org  隐私：tosdr.org/en/service/7378  开源：✅元数据清理 ExifCleaner跨平台的高性能 EXIF 元数据清除工具，有较好的批处理支持。  官网：exifcleaner.com  开源：✅搜索引擎 DuckDuckGo不追踪用户的搜索引擎，不保存搜索记录，没有追踪器、cookie 或广告，提供 .onion 服务，有官方浏览器插件。  官网：duckduckgo.com  隐私：tosdr.org/en/service/222  iOS：apps.apple.com/us/app/duckduckgo-private-browser/id663592361  安卓：https://play.google.com/store/apps/details?id=com.duckduckgo.mobile.android  开源：✅ BraveSearch注重隐私的搜索引擎，不追踪用户，不使用秘密算法或用户分析，基于自主搜索索引。  官网：search.brave.com  隐私：tosdr.org/en/service/1487  iOS：apps.apple.com/us/app/brave-private-browser-adblock/id1052879175  安卓：https://play.google.com/store/apps/details?id=com.brave.browser  开源：✅浏览器插件 Privacy Badger智能追踪器检测，自动学习屏蔽，不依赖黑名单  官网：privacybadger.org  隐私：tosdr.org/en/service/682  开源：✅ uBlock Origin屏蔽广告、追踪器和恶意网站  官网：ublockorigin.com  隐私：tosdr.org/en/service/682  开源：✅匿名支付 Monero最注重隐私的加密货币，交易完全匿名，采用环签名、RingCT、Kovri 和隐秘地址等密码技术保护用户隐私。  官网：getmonero.org  隐私：tosdr.org/en/service/8279  开源：✅ ZCash使用零知识证明技术来保护隐私，允许用户在不泄露其真实身份或地址的情况下进行交易。 Zcash 区块链使用两种类型的地址和交易，Z 交易和地址是私有的，T 交易和地址是透明的。  官网：z.cash  隐私：tosdr.org/en/service/8258  开源：✅"
  },
  
  {
    "title": "L2：ETH 的价值抽取与重塑",
    "url": "/posts/ethl2/",
    "categories": "Blockchain, Ethereum",
    "tags": "rollup, layer2",
    "date": "2024-06-23 12:00:00 +0800",
    





    
    "snippet": "      以太坊 L2 扩容路线图因面临诸多挑战而屡遭诟病，但即便采用 L1 扩容方案，相同的挑战也仍将存在。    L2 扩容路线图对 ETH 需求的影响有限。常被提及的总交易费用下降和 ETH 通胀上升等现象，实则源于区块空间供过于求。    长远来看，ETH 在各类协议中的应用或将超越交易费用，成为其需求的主要驱动力，交易费用本身则将趋近于零。  以太坊的 Layer-2 (L2) ...",
    "content": "      以太坊 L2 扩容路线图因面临诸多挑战而屡遭诟病，但即便采用 L1 扩容方案，相同的挑战也仍将存在。    L2 扩容路线图对 ETH 需求的影响有限。常被提及的总交易费用下降和 ETH 通胀上升等现象，实则源于区块空间供过于求。    长远来看，ETH 在各类协议中的应用或将超越交易费用，成为其需求的主要驱动力，交易费用本身则将趋近于零。  以太坊的 Layer-2 (L2) 扩容策略引发了关于其对 ETH “价值抽取”的争议。Dencun 升级和 L2 兴起导致总手续费降至三年新低，似乎终结了ETH “Ultrasound Money” 的论调。但这类批评多属误解，相关问题实为区块链成功扩容的必然结果，无论是在 L1 还是 L2 层面扩容，都会产生类似的现象。1/9) Ethereum is dying while L2&#39;s dance on its graveETH cannot sustain high fee revenue because it lacks the capacityAt the same time, L2s are seeing record highs in usage &amp; fees while they lobby to keep ETH&#39;s capacity down!That is what makes it a parasitic relationship: 🧵&mdash; Justin Bons (@Justin_Bons) August 26, 2024Cyber Capital 创始人 Bons 称 L2 是对以太坊主网的吸血鬼攻击，正在窃取其用户和交易费分析表明，平均交易成本的下降速度已经超过了区块空间需求的增长，从而致使总手续费下滑。尽管以太坊交易量自 2023 年初至今增长超六倍，但总费用仍呈下降趋势。此外，链上交易活动历来是影响手续费的主要因素，但近来即便占用了更多区块空间，其对费用的影响仍有所减弱。这两项变化可能才是 ETH 销毁率下降的核心原因。当然，L2 确实也存在一些问题：L2 上的交易费用缺乏类似 EIP-1559 的销毁机制，定序器可从 L2 与 L1 费用差额中获利。此外，单独的 blob 定价机制的引入也可能导致短期内廉价区块空间的供给过剩。但尽管如此，L2 高吞吐量、低费用的环境十分利于催生新型应用，从而进一步提升 ETH 的使用价值。相较于质押等其他流动性来源，L2 带来的这些问题影响较小。扩容方案的成功当前以太坊活动已大规模迁移至 L2，有力地证明了其扩容路线图的技术成功。L2 交易量呈现显著增长：从 2023 年初的日均 100 万笔（占总量 35%），攀升至 2024 年初的 500万笔 (83%) 近期更是达到 1200 万笔 (92%)。Dencun 升级效果卓著，在 2024 年 3 月实施后的数周内，L2 交易量就翻倍至 1000 万笔。值得注意的是，L2 的增长并未影响以太坊主网吞吐量，主网交易量保持相对平稳（见图 1 中的灰色部分），自 2021 年 8 月以来，以太坊的 gas 限制始终保持不变，而 L2 则在持续扩展。图1 越来越多基于 ETH 的活动发生在 L2 上 | 图源：growthepie在交易量指数级增长的同时，交易费用总额却呈下降趋势。2024 年 8 月，以太坊日均交易费降至 Ξ700，创三年新低，仅为 2023 年平均水平 (Ξ3.4K) 的五分之一。相较于前低 2023 年 10 月的 Ξ2.0K 日均费用，当前水平显著降低。粗略估算，廉价的区块空间令每日交易费用至少减少 Ξ1.0K，考虑到链上拥堵对定价的指数效应，实际的节省可能更为显著。这一现象充分体现了以太坊扩容策略在降低用户成本方面的成效。图2 交易费用总额创多年来新低 | 图源：growthepie以太坊网络呈现出交易量激增而平均成本下降的特征，这是扩容策略成功的典型表现。然而，仍要充分认识到 ETH “收入”对该资产的重要性。当前数据显示出了显著的不对称性：尽管以太坊主网仅承载 11% 的总交易量，却仍然产生了 92% 的交易费用。这一现象凸显了 L2 解决方案在成本方面的巨大优势，其交易费用较 L1 低两个数量级以上。具体而言，截至 2024 年8 月 1 日的数据显示：  Arbitrum One 的中位交易费用仅为以太坊主网的 1/166  Base 的中位交易费用更是低至主网的 1/277图3 L2 交易费用占比极少 | 图源：growthepie扩容的副作用L2 活动激增及 Dencun 升级 (EIP-4844) 引入的低成本 blob 交易，显著重塑了 ETH 的费用需求结构。L2 容量扩张超出了需求增长，导致 ETH 总费用支出大幅下滑。从技术角度看，以太坊区块空间扩容无疑是成功的，但市场视角下，这种成功却反而可能对 ETH 不利，因为费用的减少也可能减少用户引入和购买 ETH 以支付费用的需求。然而，这恰恰印证了扩容路线图的预期效果，这点应与 L1 和 L2 扩容权衡的讨论区分开来。目前区块空间呈现供给过剩，这从 2021 年以来日 ETH 费用支出的持续下降（见图 4）中可见一斑。随着链上活动持续向低成本 L2 迁移，这一趋势料将延续。图4 以太坊每日交易费用 | 图源：Dune值得注意的是，总交易费用支出下降并不能直接归因于 L2 扩容路线图。任何扩容策略，只要区块空间增速超过需求，都会导致总费用支出净减。图 5 中的费用与交易量关系曲线（蓝色）清晰地展示了这一点：2021 年峰值时，平均交易费用*日交易量 = 9460（即平均日费用）。沿此曲线扩展交易量和费用支出，将使费用支出与历史高位保持一致。图5 交易数与交易成本然而自 2022 年以来，交易量增长显然未能抵消平均交易成本的下降趋势。这一现象并非 L2 架构特有，而是源于交易需求增长不足以匹配廉价区块空间供给的扩张。换言之，任何对 L1 进行扩展的路线图都可能导致总交易费用支出的减少。实际上，2021 年在 2020-2024 年的拟合线中是一个异常值。采用偏重近期数据的 x*y=k 模型，得出平均交易费用*日交易量 = 4186（图 5 绿色曲线）。这一模型预测日均费用为 Ξ4186，较年初至今实际日均费用 Ξ2897 高 40% 以上，说明可能仍有高估。也就是说，如果以太坊的区块空间继续扩展，而交易量未能相应跟上，交易费用可能会进一步下降。幂律拟合（图 5 红色曲线）或许能更精确地刻画了扩展费用结构。该模型假设交易成本与交易活动间存在非线性关系，其均方误差低于常量乘积函数拟合。随交易量增加，交易成本呈加速下降趋势，这与观察到的行为和未来L2扩展计划相符。基于此拟合，日均交易量达 2000 万笔时，预计将产生约 Ξ2443 的日均总费用支出。此现象是加速扩展的必然副产物，并非 L2 路线图所特有。费用的影响因素区块空间的扩张并非交易费用总支出下降的唯一因素。近年来，以太坊链上交易活动与交易成本的关联性也发生了显著变化。2020年夏季，以太坊费用首次大幅增长与去中心化交易所 (DEX) 交易的兴起密切相关，表明机会性交易活动的增加是费用上涨的主要推动力。图 6 中展示了包括 DEX 交易在内的各类交易占以太坊总交易量的比例。图6 各类交易的 gas 使用占比 | 图源：Dune对比图 4 中显示的网络总交易费用变化情况可以发现，DEX 交易比例的激增通常伴随着网络总费用的上升。然而，自 2022 年以来，这种关联性逐渐减弱，DEX 交易比例的上升仍然会对总费用产生影响，但其影响力已不如以往。当前，DEX 交易比例接近历史最高水平，甚至超越了 2020 年和 2021 年的峰值，形成了一个独特现象：基于 DEX 的活动比例处于历史高位，而交易费用却降至历史低点。实际上，Uniswap 仍是推动以太坊交易费用的最大单一因素，最近一年 Uniswap Universal Router 贡献了近 12% 的总交易费用。图7 以太坊合约 gas 消耗量 Top 100  | 图源：Dune交易活动对费用波动影响减弱的关键原因在于以太坊交易环境的成熟，这使得 MEV 竞价的机会空间缩小，而这类竞价会通过优先排序交易推高链上费用。此外，高波动性的 memecoin 活动迁移至 Solana 或 Base 等链上，也进一步减少了套利和 MEV 机会。DEX 中低默认滑点设置和对 MEV 防护（如CoW Swap）的关注也降低了优先费用的重要性（相较于以太坊早期）。人们在对比以太坊和 Solana 的总交易费用时，常常过度聚焦于扩展策略，而忽视了费用可持续性这一核心问题。Solana 网络上 95% 以上的非投票交易费用来自优先费用，主要与时间敏感的交易活动相关。此外，在所有非投票交易费用中，有 25% 至 45% 来自失败的交易。Solana 未来费用结构的演变将高度依赖其技术路线，而该路线在 MEV 和优先费用计算方面可能与以太坊大相径庭。也就是说，Solana 可能类似 2020-2021 年的以太坊，正处于费用支出的局部最大值，或者其本地费用市场的全面实施可能会维持甚至加剧当前高优先级但低基础费用的状态。图8 Solana 交易统计 | 图源：Dune总之，无论采用基于 L1 还是 L2 的设计，在成功的区块链扩容路线图中，交易费用总额的减少都是不可避免的结果。Ultrasound Money🦇🔊以太坊 “Ultrasound Money” 叙事因交易费用减少而受到重大影响。自 2022 年 9 月合并 (The Merge) 后，EIP-1559 引入的基础费用销毁机制使 ETH 成为净通缩资产。合并至 Dencun 升级期间，ETH 供应量借由销毁机制减少了超 44 万枚。然而，Dencun 升级后，这一趋势迅速逆转。升级后 ETH 供应量已增近 20 万枚。按当前通胀速度，合并后 ETH 的累计供应变化预计将在年底前转为净通胀。若 ETH 质押比例持续上升（进而推动净发行量增加），此趋势可能还会进一步加速。值得注意的是，这一现象并非主要由 L2 扩展引发。尽管 L2 交易费用缺乏 EIP-1559 的销毁机制，但 L2 上的总费用量相对较小，即使 L2 费用全部被销毁，ETH 仍会呈通胀态势。通胀率变化更多源于主网费用的普遍下降以及 ETH 质押比例上升导致的总体发行速度加快。关于所谓“超声波货币”可行性与重要性的讨论应与 L2 扩展区分开来，前者更多涉及的是质押比例增长和通用扩容经济学。定序器盈利L2 扩容路线图受到批评的另一个点是定序器的价值捕获。理论上，定序器可通过 L2 总交易费用减去支付给 L1 的“租金”来获取利润。这部分以太币计价的盈利不会被销毁，而且可能被（部分）出售以资助运营，为 ETH 带来额外的抛压。图9 L2 在以太坊上发布数据的成本已大幅下降 | 图源：growthepieDencun 升级将 L2 支付给以太坊主网的平均“租金”成本降低了 90% 以上，即每日平均减少约 Ξ200-300，进一步提高了定序器的盈利空间。然而，rollup 的链上盈利指标却反而呈现出好坏参半的状态，部分 rollup 的定序器在 Dencun 升级后中位日盈利反而有所下降，如下图所示。图10 rollup 每日中位盈利 | 图源：growthepie造成这一结果的主要原因在于，大部分因 Dencun 升级而减少的“租金”成本实际上已通过更低的手续费转让给了 L2 用户。Base、Blast、OP Mainnet 等主流 rollup 的 L2 中位交易费用可被压缩至每笔约 0.002 美元，在成本和速度上具备与 Polygon PoS 和 Solana 等高性能 L1 网络相当的竞争力。尽管如此，L1 “租金”减少 Ξ200-300 仍是 L2 扩展方法导致交易费用损失的最直接体现（相较于假设的 L1 扩展策略）。blob 交易类型目前由于供给过剩而维持在最低价格水平，导致每日总费用减少约 10%。不过与质押比例增加、净发行量和应用增长等其他重要驱动因素相比，这一影响相对较小。图11 平均 L2 交易成本随 L1 结算成本的降低而下降 | 图源：growthepie效用驱动增长长期来看，预计平均交易费用将持续趋近于零，这一趋势不受扩展架构影响。除时间敏感型交易（如交易所交易）可能保持较高优先费用外，通过扩展实现的绝对费用减少将有效推动用户上链——即便是以总费用支出下降为代价。这一趋势引发了对 L1 资产长期基本驱动因素的思考：  一方面，新用户的初始上链费用在短期内会继续存在（除非智能钱包实现大规模普及），原生 L1 代币的持续需求源于地址需要持有超额的代币来支付手续费（“储备余额”）。  一旦交易成本降至特定阈值（用户的心理定价点）以下，大多数用户在初次使用时为了确保更连续、流畅的使用体验，可能会往账户中充入超出实际需要的原生代币，这可以视为一种“便利成本”。 交易费用足够低时，初始钱包资金的金额将主要受其法币价值的影响。例如，不少人会认为“10美元”是一个可接受的小额支出，这种思维会不自觉地带到加密货币当中，导致用户倾向于保持一个“感觉比较合理”的余额。钱包不再活跃时，剩余的小额储备余额可视为被“软锁定”，不太可能大规模重新进入流通。这部分代币相对于具体的供应变化（如发行或销毁）的重要性在日益上升。      另一方面，基于应用的效用很可能成为未来最主要的需求驱动因素。高吞吐量、低费用的 L2 环境将催生此前在 L1 上不可行的新型应用和原生资产效用。已有迹象表明这一转变正在发生：                  超 180 亿美元的资产通过标准桥接转移至 L2，超过 2022 年 1 月至 2024 年 8 月间支付的总费用。                    这些桥接的 ETH 大部分已部署到各种协议中，DeFi 应用仍是最大的流动性汇聚点。例如，Arbitrum 上的 Aave 目前持有超 Ξ200K。            ETH 的价值驱动正从单纯的交易费用支付转向更广泛的生态系统参与和应用使用。L2 的高效环境不仅降低了交易成本，还为 ETH 创造了新的使用场景和价值捕获机会。这种转变可能会重塑 ETH 的长期价值主张，从交易媒介向生态系统效用代币演进。图12 超 180 亿美元的资产转移至 L2 | 图源：Dune与此同时，以太坊主网协议上 ETH 的效用依然保持强劲态势。得益于 Eigenlayer 和再质押的推动，目前质押 ETH 供应已增加 Ξ4.7M，从 Ξ28.8M 增至 Ξ33.5M。DeFi 协议也在持续吸引 ETH 流入，例如，2024 年以太坊上的 Aave V3 wrappedETH 金库新增 Ξ500K 存款，当前 Aave V3 中不同形式的 ETH 存款总额超 Ξ2.8M。2024 年上半年，锁定在以太坊 Aave V3 中的 ETH 净增长达 Ξ524K，与同期交易费用支出总额相当。图13 一年内已有超 500K WETH 存入 Aave V3 | 图源：Aavescan此外，ETH 还是多个链上交易池的报价货币，这也是 ETH 的重要需求来源。例如，以太坊的 Uniswap V3 池 (WBTC/ETH) 持有超过 Ξ80K。未来，随着区块链垂直领域的进一步发展，ETH 在生态系统中的角色可能会不断演变，但其作为“可信中立”的原生 L1 基础资产的关键作用仍将延续。结论相较于直接的 L1 扩展，L2 路线图在 ETH 需求方面确实存在一定的价值抽取，主要源于两个因素：      L2 交易缺乏类似 EIP-1559 的销毁机制        L2 定序器可通过 L2 交易和 L1 费用差额获取一定利润  然而，这种“抽取性”在分析长期 ETH 供应动态时的影响并不显著。2021 年创纪录的费用在区块空间扩展和 MEV 机会减少的背景下难以持续。因此，交易费用总额的下降和销毁量减少并非 L2 方案所特有的现象，而是扩容经济学的一般性问题。同样，“Ultrasound Money” 叙事的受挫，更多是源于整体区块空间供需失衡，而非 L2 策略本身。L2 实际上为 ETH 创造了新的使用场景和价值捕获机会。当前趋势表明，来自协议渠道（如货币市场抵押或 DEX 交易对）的 ETH 需求仍将保持强劲。而 L2 低费用、高吞吐量的环境还将为孕育更多以 ETH 为中心的应用提供沃土。长远来看，ETH 的需求动力将从单纯的交易费用支付，转向更广泛的生态系统参与。这种转变有望重塑 ETH 的长期价值主张，推动其从交易媒介向全面的效用型资产演进。"
  },
  
  {
    "title": "AI × 加密：去中心化愿景的现实考验",
    "url": "/posts/aicrypto/",
    "categories": "Blockchain",
    "tags": "ai",
    "date": "2024-05-22 12:00:00 +0800",
    





    
    "snippet": "🍕 Happy Bitcoin Pizza Day !            多元化的机遇与挑战：AI 与加密技术的交叉领域呈现出多样化的发展格局。从去中心化数据市场到分布式计算，从模型输出验证到区块链身份识别，每个细分赛道都面临独特的机遇与挑战，不宜一概而论。              技术可行性 ≠ 市场优势：基于加密的 AI 解决方案大多在技术上是可行的，但要获得真正的市场优势仍需克服...",
    "content": "🍕 Happy Bitcoin Pizza Day !            多元化的机遇与挑战：AI 与加密技术的交叉领域呈现出多样化的发展格局。从去中心化数据市场到分布式计算，从模型输出验证到区块链身份识别，每个细分赛道都面临独特的机遇与挑战，不宜一概而论。              技术可行性 ≠ 市场优势：基于加密的 AI 解决方案大多在技术上是可行的，但要获得真正的市场优势仍需克服诸多障碍。在面对中心化竞品的挑战时，“去中心化”本身并不足以成为决定性的竞争优势，这些项目还需要在性能、成本、易用性和监管合规等方面取得突破。              理性看待市场热度：AI 代币的强劲表现可能更多地反映了市场对 AI 行业整体的乐观预期，而非这些项目本身的内在价值。许多 AI 代币可能缺乏真正的需求驱动力，投资者应当谨慎评估其长期可持续性。      自 2022 年 11 月 30 日 OpenAI 正式发布 ChatGPT 以来，生成式人工智能的更新迭代日新月异，引起了公众和投资者对 AI 行业的高度关注，同时也为基于 AI 叙事的加密项目提供了发展机会。AI 赛道的项目融合人工智能和加密技术，推动了 DeComp、ZK、数据以及应用等方面的创新。但随着人工智能行业的不断发展完善，我们也应该正视 AI×Crypto 所面临的实际挑战。后文将对当前 AI 行业的发展趋势以及加密与 AI 的交集领域进行梳理，并分析其中存在的机遇与挑战。AI 行业主要发展趋势人工智能领域的发展变化速度空前。据 Demandsage 报告显示，ChatGPT 仅用短短两个月就吸引了一亿用户，相比之下，社交巨头 Twitter 花了五年时间才达到同等规模，而流媒体翘楚 Netflix 更是耗时十八年。除了增长速度外，AI 模型本身的技术进步也是飞跃式的，一年多前独领风骚的 GPT-3.5 如今性能早已被后来居上的各类大语言模型全面超越。基于扩散的生成式图像、音视频模型输出效果相较一年前也堪称天壤之别。图1 ChatGPT 上线两个月就吸引了一亿用户 | 数据源：DemandsageAI 行业的飞速发展，要求我们必须以更加审慎的视角重新审视加密平台在 AI 领域所扮演的角色。当前大多数 AI 代币，尤其是采用固定代币经济模型的项目，其长期的价值积累路径仍充满着不确定性。同时，加密技术在应用于新兴的 AI 领域时，还可能因暴露在更广阔的市场竞争和监管环境中而面临更大的挑战。当然，人工智能与加密技术的交叉领域广阔，仍蕴含着多样化的机遇。以下是与加密行业密切相关的 AI 行业主要发展趋势。开源文化繁荣在人工智能领域，开源模型文化的持续蓬勃发展与加密 AI 产品密切相关。Hugging Face 是最大的 AI 开源平台，目前该平台上已有超过 70 万个模型供研究者和用户自由使用和微调。用加密领域广泛应用的平台类比，Hugging Face 之于 AI，就相当于 GitHub 之于代码托管或 Discord 之于社区运营，其地位举足轻重。Hugging Face 平台上的模型种类丰富多样，涵盖了从大型语言模型到生成式图像和视频模型等各个类型。其中不仅有来自 OpenAI、Meta、Google、阿里等行业巨头的模型，也包括众多个人开发者的创新成果。值得注意的是，如图 2 所示，某些开源语言模型在拥有可比输出质量的同时，处理速度和价格甚至超越了最先进的闭源模型，这也确保了开源模型与商业闭源模型之间能保持一定程度的竞争关系。图2 大语言模型输出质量、输出速度、价格对比 | 数据源：Artificial Analysis围绕 Hugging Face 形成的活跃开源生态，加上巨头间激烈的商业竞争，共同构建了一个开放、协作同时又充满挑战的环境，缺乏竞争力的模型会被迅速淘汰，从而促进整个行业的快速进步、优胜劣汰。开源文化正在推动整个 AI 行业通过良性迭代向更高效、更创新的方向加速发展。小模型性能提升小型模型的质量与性价比日益凸显，也成为了备受关注的重要趋势。早在 2020 年，学界已开始对小型模型进行深入研究，近期微软的一篇论文更为这一发展趋势增添了有力佐证。这一进程与开源文化相辅相成，为本地高性能 AI 模型的应用开辟了广阔前景。尤为引人注目的是，一些经微调的开源模型已在特定基准测试中超越了部分领先的闭源模型。这一突破为去中心化人工智能的实现提供了现实可能：部分 AI 模型可在本地设备上运行，从而最大程度地实现去中心化。当然，目前科技巨头们仍将在云端持续训练和运行大规模模型，不过这种多元化的技术路径仍能为 AI 的设计提供了更为丰富的可能性。此外，考虑到 AI 模型基准测试任务的日益复杂化（数据污染及测试范围变化），模型输出质量的最终评判可能还是要回归至终端用户。目前市面上已经出现了多样化的比对工具和专业测试服务，助力用户全面评估模型性能。诸如 MMLU、HellaSwag、TriviaQA 和 BoolQ 等开源大语言模型基准测试，往往覆盖常识推理、学术主题及多元问题格式，AI 模型评估的复杂性也由此可见一斑。AI 集成收益巨大拥有强大用户粘性或解决特定业务问题的现有平台，能从 AI 技术的集成中获得显著的收益。以 GitHub Copilot 为例，其与代码编辑器的无缝融合，不仅极大地提升了开发环境的智能化水平，更在实质上重塑了开发者的工作范式，进一步巩固了 GitHub 在技术社群中的核心枢纽地位。此外，AI 接口已被广泛嵌入邮件客户端、办公软件、设计工具等日常化应用场景中，充分显示了智能技术在提质增效方面的巨大潜能。图3 Github Copilot 有效提升了开发的效率和代码质量 | 数据源：Github Research但要注意的是，此类场景下，AI 模型并没有开辟全新的平台，而是在增强现有平台的功能和价值。这种趋势表明，当前的人工智能技术更像是一种倍增器或增强工具，其本质更近于赋能，而非彻底颠覆。这类专注于改善传统业务流程的 AI 模型，如 Meta 的 Lattice，往往依赖高度专有的数据生态和封闭系统。由于这些模型深度嵌入核心产品，并依托独特的数据资源，很可能将继续保持闭源状态。推理为重在 AI 硬件和计算领域，计算资源的使用重心正在从模型训练转向模型推理。AI 模型的初始开发阶段需要对模型进行“训练 (Training)”，即使用大量计算资源来处理海量数据集，通过不断调整模型参数，逐步提炼出能够捕捉复杂模式和深层关联的智能系统。模型训练通常需要大量高性能硬件和漫长的调试周期。而目前，产业焦点已经逐步转向模型的“推理 (Inference)”，即将训练完备的模型应用于现实场景，生成具有实际价值的预测或决策。硬件巨头们的战略布局也印证了这一趋势转折：英伟达的执行副总裁兼首席财务官 Colette Kress 在 2024 年 2 月的财报电话会议上披露，其业务中约 40% 收益来自 AI 推理；微软 CEO Satya Nadella 也在一月的财报电话会议上表示，Azure AI 的使用“绝大多数”都集中在推理方面。随着这一发展态势的持续演进，各类期望借助AI模型创造价值的商业实体，可能会愈加审慎地遴选安全可靠、已达生产就绪状态的技术平台。硬件竞争加剧硬件架构的竞争格局也日趋激烈。英伟达 2024 年 3 月发布的 B200 处理器相较于 H100 实现了 5 倍的性能提升，而 GB200 解决方案将 2 个 Blackwell GPU 和 1 个 Grace CPU 结合在一起，能够为 LLM 推理工作负载提供 30 倍性能提升，同时能效层面也有所突破。此外，谷歌的张量处理单元 (TPU) 和 Groq 的新型语言处理单元 (LPU) 也有望在未来数年中的 AI 硬件市场中分得一块蛋糕。这种硬件多元化的发展趋势可能会重塑 AI 行业的成本结构，能够快速调整技术路线、大规模采购硬件、满足复杂基础设施需求，并提供卓越开发者工具的云服务提供商，将获得关键性的战略优势。图4 NVIDIA Blackwell、H100 和  H200 不同并行和分块配置的峰值吞吐量  | 数据源：NVIDIA Developer总而言之，人工智能领域正处于快速发展的阶段。自 ChatGPT 首次面市至今时间尚不足一年半，该技术迭代与生态演进之迅速令人瞩目。尽管某些生成式 AI 模型的偏见问题曾引发广泛讨论，但市场本身已展现出筛选机制，逐步将表现欠佳的模型边缘化，转而青睐更更优秀的技术方案。随着新解决方案不断涌现，加之即将落地的监管框架，该领域的问题空间可能会频繁变化。对于这样一个创新迅猛的领域，投资者尤其需要谨慎对待将“去中心化”作为万能药的简单化叙事。这种在加密行业很屡见不鲜的修辞范式，往往试图回应一个并不存在的“中心化问题”。实际上，人工智能行业通过众多企业和开源项目在技术和垂直业务领域的激烈竞争，已然在相当程度上实现了一种内生、有机的去中心化。此外，去中心化协议在技术架构和社会治理层面的决策与共识机制，本质上决定了其发展要比中心化系统更为缓慢。在当前人工智能的快速迭代阶段，执着于在去中心化与功能竞争力之间寻求过度平衡的产品，很可能会被市场无情地边缘化。不过，从长远角度观之，加密技术与 AI 之间潜在的协同效应，仍有望在未来产生重要影响。潜在机遇我们可以将人工智能与加密技术的交汇部分大致分为两个主要类别：加密+AI、AI+加密。      加密+AI，即利用 AI 技术对加密行业的优化和改进。这包括了一系列应用，从使交易更易于理解、提升区块链数据分析能力，到在去中心化协议中灵活运用模型输出。这类应用的商业价值相对明确，尽管链上推理模型等前沿场景在技术实现上仍面临诸多挑战，但其长期前景依然广阔。中心化 AI 模型能够像改善其他技术密集型行业一样，优化重塑加密领域的技术形态，包括但不限于：改进开发工具、加强代码审计、将自然语言转化为链上操作等。        AI+加密，即利用加密技术通过去中心化的方法论——诸如去中心化计算、去中心化验证和身份管理等——从根本上颠覆传统人工智能的运作范式。对于这一类别的价值主张，投资者需要保持更为审慎和理性的态度。其面临的挑战不仅仅局限于技术层面（长期来看技术问题通常可克服），更涉及到与更广阔的市场和监管力量的复杂博弈。尽管如此，近期加密行业对 AI 叙事的关注仍主要集中于此类应用，究其原因，不外乎此类用例更契合代币发行的商业逻辑。  鉴于目前行业内“加密+AI”相关的代币相对稀少，接下来的讨论将主要关注第二类应用。这一方面反映了当前市场的关注热点，另一方面也为我们提供了一个机会来深入探讨去中心化 AI 在面对现实挑战时的潜力和局限。AI+加密在探讨加密技术如何影响人工智能行业流程时，本文将其潜在作用范围大致划分为四个主要阶段：  数据的收集、存储和工程化  模型的训练和推理  模型输出的验证  模型输出的追踪这四个阶段涵盖了 AI 从原始数据到最终应用的整个生命周期。近期，有大量新兴的加密 AI 项目在这些领域出现，试图将区块链技术的优势与 AI 流程的各个环节相结合。然而，在中短期内，这些项目很可能将面临两大现实性考验。首先是需求端的问题，即如何在一个急剧演变、充满不确定性的市场生态中，准确捕捉并有效满足真实、迫切的用户诉求。其次是来自既有中心化巨头与开源项目的激烈竞争，这些参与者已经在 AI 领域的技术实力、市场份额及用户基础等方面构建起显著的竞争壁垒。在接下来的讨论中，我们将深入探讨每个阶段的具体挑战和机遇，权衡加密 AI 项目的创新潜力与现实困境。数据获取与存储数据是人工智能模型的根基，也是专业 AI 模型性能差异的关键所在。区块链技术的兴起为 AI 模型提供了一个全新且丰富的数据源。创新项目如 Grass 正尝试利用加密货币的激励机制，从开放互联网上采集和整理数据集。就此角度而言，加密货币不仅能提供行业特定的数据集，还有机会激励新数据集的创建，在为数据提供者提供价值的同时，也为 AI 模型的发展提供了新的可能性。Reddit 与谷歌达成的每年 6000 万美元的数据授权协议，就是数据集货币化潜力的一个鲜明例证。图5 作为 AI 数据层的 Grass | 图源：Grass早期的许多模型，如 GPT-3，混合使用了多个开放数据集，包括 CommonCrawl、WebText2、书籍和维基百科等，这类数据集可以在 Hugging Face 上免费获取。但近期的闭源模型开始保护其训练数据集的具体组成，以维护商业利益，数据授权的重要性由此逐步凸显出来。现有的中心化数据市场正在努力弥合数据提供者和消费者之间的鸿沟。在此背景下，新兴的去中心化数据市场解决方案面临着被开源数据目录和企业竞争对手夹击的局面，同时还要解决数据接口标准化、数据完整性验证、产品冷启动等一系列问题，并在市场参与者之间平衡代币激励。这些挑战在没有法律框架支持的情况下更显艰巨。去中心化存储解决方案在 AI 行业的应用前景也值得关注。然而现实障碍不容忽视：一方面，开源数据集的分发渠道已相当成熟；另一方面，专有数据集所有者对安全和合规有严格要求，而当前的监管环境尚不允许在 Filecoin 和 Arweave 等去中心化平台上托管敏感数据。许多企业仍处在从本地服务器向中心化云存储提供商过渡的阶段。在技术层面，这些项目的去中心化特性目前也与敏感数据存储的物理隔离要求不相符。尽管有如图 6 所示的研究表明去中心化存储相比云存储在单位成本上更具优势，但对其竞争力的评估需要从更宏观的视角全面考量：系统迁移的前期投入、工具集成的成熟度、运营成本的可预测性、合同保障以及技术人才储备等多元要素。值得注意的是，除了亚马逊、谷歌、微软等云计算巨头，市场上还涌现出众多低成本的云服务提供商，为追求性价比的用户提供了更多选择。图6 去中心化存储与云存储成本对比  | 数据源：CoinGecko尽管如此，Filecoin 的 Compute-over-data 和 Arweave 的 AO 计算环境等去中心化存储领域的技术创新，依然在特定应用场景中展现出独特价值。例如，对于使用非敏感数据集或极其重视成本效益的新兴项目而言，这些创新解决方案不失为一种可行的选择。纵观全局，加密技术在 AI 数据领域确实存在一定发展空间，但要实现真正的颠覆性突破，关键在于精准定位并深耕能够创造独特价值的细分市场。在与成熟的传统技术和开源方案直接对垒的赛道上，去中心化产品或许需要更长的周期来积蓄实质性突破的力量。模型训练与推理加密领域的去中心化计算 (DeComp) 正崛起为中心化云计算的潜在替代方案。为应对当前 GPU 供应紧缺的挑战，Akash 和 Render 等项目提出利用分布式网络中闲置的计算资源，以低于传统云服务商的价格提供服务。初步数据显示，这类项目正获得用户和供应商的青睐。以 Akash 为例，今年四月底其活跃租约数量较年初增长八倍，主要得益于存储和计算资源使用量的提升。图7 Akash 网络租借数年初快速增长后出现大幅下跌 | 数据源：AkashStats然而，网络收费在 2023 年 12 月、2024 年 5 月均出现下降趋势，跌幅超 60%，一方面是受币价波动影响，另一方面则是因为 GPU 供应增速超过了需求增长。对于定价随供需变化而调整的网络来说，如果供给增长持续超过需求增长，长期维持对原生代币的需求可能面临挑战。因此，这些项目的代币经济模型可能需要重新评估调整，以更好地适应市场变化。图8 Akash 网络收费出现超 60% 的跌幅 | 数据源：AkashStats技术层面上，去中心化计算还面临网络带宽的限制。对于需要多节点训练的大模型，物理网络基础设施至关重要。数据传输速度、同步开销以及对特定分布式训练算法的支持，都需要特殊的网络配置和定制通信方案。在大规模集群中以去中心化方式实现这点是极为困难的。综上所述，去中心化计算（及存储）要在长期取得成功，必须直面来自中央化云服务提供商的激烈竞争，其市场普及将是一个漫长而曲折的过程，时间跨度可能不亚于云计算的普及历程。考虑到去中心化网络开发所涉及的技术复杂性，以及开发及销售团队在可扩展性方面的不匹配，要全面实现去中心化计算的宏伟愿景是充满挑战的。输出验证与信任随着人工智能技术日益深入日常生活，公众对 AI 模型输出质量和潜在偏见的关注度日益提高。在这一背景下，一些加密项目开始探索基于去中心化市场机制的创新解决方案，力图构建一个全面评估AI输出的算法体系。然而，模型基准测试本身的内在复杂性，加之成本、性能与质量间错综复杂的平衡关系，使得跨模型的直接比较仍然充满挑战。BitTensor 作为 AI 导向型加密项目中的佼佼者，正积极突破这些技术瓶颈。其独特的子网系统致力于构建智能市场生态，通过 32 个子网激励不同类型的智能行为。这一精心设计的激励机制旨在从信息提供者那里提取最具价值的智能输出，从根本上应对传统基准测试中固有的局限性。以旗舰子网 1 为例，该子网围绕文本提示展开竞争，奖励能够针对验证者发送的提示生成最佳文本响应的矿工。响应质量由子网中的其他验证者进行评判，这一机制为智能经济的初步构建提供了基础，网络参与者会在各种市场中竞相创建和优化模型。然而，这一验证和奖励体系仍处于起步阶段，面临诸如对抗攻击风险、在主观领域（如语言和艺术）中建立客观评估标准等诸多挑战。具体到子网 1，其验证机制要求验证者生成一个或多个参考答案，并将所有矿工的响应与之进行比较，与参考答案最相似的响应将获得最多奖励。当前的相似性算法采用字符串字面匹配和语义匹配的组合作为奖励标准，但可能难以充分兼顾多样的风格偏好。图9 BitTensor 子网 1 说明 | 图源：BitTensorBitTensor 激励结构所产生的模型能否最终超越中心化模型，顶级模型是否会迁移至该平台，以及如何在模型规模与计算成本间实现平衡，这些问题仍悬而未决。从根本上讲，BitTensor 试图通过其激励机制优化 AI 模型的开发与使用，实现资源的高效分配。然而，值得思考的是，一个允许用户自主选择的自由市场同样可能依靠“看不见的手”自发实现资源优化，复杂的去中心化激励系统未必比简单的用户自由选择机制更为高效。不可否认，BitTensor 正在应对一个不断扩大的、极具挑战性的问题领域，但其在短期内的广泛应用仍面临重重障碍。与此同时，免信任模型推理技术——旨在证明模型输出确实源自特定模型——成为 AI 与加密技术交叉领域的另一个研究热点。然而，随着开源模型趋向小型化和轻量化，个人计算设备性能的持续提升，此类解决方案可能面临需求端的重大挑战。在当前技术环境下，用户已可通过下载模型在本地运行，并借助文件哈希或校验和方法验证内容完整性，这使得免信任推理技术的必要性和实际价值受到质疑。尽管许多大语言模型因其复杂性和资源需求尚未能在智能手机等轻量级设备上运行，但配置较高的台式计算机已具备运行多种高性能 AI 模型的能力。这一发展趋势不仅凸显了 AI 技术的飞速进步和普及，更引发了我们对未来 AI 应用场景和技术需求的深度思考。在 AI 能力持续下沉的时代，如何在技术创新、用户需求与安全性之间找到微妙的平衡，将成为推动 AI+加密领域持续发展的关键所在。数据来源与身份在人工智能技术迅猛发展的今天，生成式 AI 的输出已愈发逼近人类创作水准，在某些领域甚至已难以辨别真伪。这一技术进程不仅引发了社会各界对 AI 生成内容追踪和识别的广泛关注，更折射出人机交互边界正在日益模糊。数据印证了这一态势：GPT-4 通过图灵测试的概率已是 GPT-3.5 的三倍，预示着在线互动中“难辨人机”的时代即将到来。在此背景下，准确验证在线用户的真实身份以及为 AI 生成内容添加可识别标记，已然成为刻不容缓的关键技术议题。为应对这一挑战，去中心化身份识别系统应运而生，代表性项目如 Worldcoin 的“人性证明”机制，其核心旨在借助区块链技术精准识别真实用户。同时，将数据哈希值记录于区块链也为内容溯源提供了可能，有助于验证内容的创建时间和来源。然而，我们仍需保持理性，审慎权衡加密解决方案与中心化替代方案在可行性和效率层面的优劣。在身份管理领域，不同地区呈现出截然不同的治理模式。中国已构建起与政府管控数据库紧密关联的在线身份体系；而在全球其他地区，尽管中心化程度相对宽松，但由 KYC 服务提供商组成的联盟同样能够提供独立于区块链技术的“人性证明”解决方案，其模式与互联网安全领域的可信证书颁发机构体系颇为相似。与此同时，AI 水印技术研究也在不断推进。该技术旨在于文本和图像输出中嵌入隐藏信号，使算法能够检测内容是否由 AI 生成。多家领先的 AI 公司，包括微软、Anthropic 和亚马逊，已公开承诺为其生成的内容添加此类水印，显示了业界对内容真实性和可追溯性的重视。此外，许多现有的内容提供平台出于合规需求，已经建立了严格的内容元数据记录系统。尽管此类数据通常采用中心化方式存储，用户仍倾向于信任与社交媒体相关的元数据信息（如发布时间、作者身份、编辑记录等）。需要强调的是，任何基于加密技术的数据溯源和身份验证解决方案，要想真正发挥作用，还需要与用户日常使用的平台深度整合。因此，尽管基于加密技术的身份验证和数据溯源方案在技术层面上是可行的，但其广泛采用却并非必然。这一过程将受到商业考量、合规要求以及监管政策等多重因素的影响和制约。AI 代币观察自 2023 年第四季度以来，尽管面临前文所述的各种挑战，许多 AI 相关的加密货币却展现出了超越比特币、以太坊，甚至超过英伟达和微软等 AI 板块龙头股票的市场表现。AI 代币一方面受益于整体加密货币市场的强劲势头；另一方面，频繁涌现的 AI 相关新闻也带来了持续关注度和投资热情。这两大因素交互叠加，编织出AI代币市场的独特叙事。这种独特的市场地位使得 AI 代币呈现出与传统加密货币不同的价格走势。即便在比特币价格下跌的时期，AI 代币仍能保持上涨势头，在整体市场低迷时期展现出上行波动性。然而，理性投资者应对AI叙事交易的可持续性保持谨慎态度。目前，这一领域缺乏明确的应用预测和可靠的效用指标，这为各种 Meme 投机提供了土壤，但这类市场行为可能难以长期维系。代币的价格最终必将与其实际效用趋于一致——关键问题在于这一过程需要多长时间，以及究竟是效用提升以匹配当前价格，还是价格回落以反映实际价值。总的来说，AI 与加密货币的交叉领域正处于一个充满活力和不确定性的发展阶段。市场的短期波动可能会持续一段时间，但长远来看，那些能够展示实际应用价值、解决现实问题的项目才能在这个快速演进的领域中脱颖而出，赢得持续的市场认可和投资者信心。结论在技术进步开启的新时代里，AI 代表了先进的生产力，数据和算力是最核心的生产资料，而区块链和加密技术则带来了全新的生产组织方式。问题在于，在加密技术成熟度不足、相关法律和监管框架滞后的现阶段，AI 真的需要变更现有的生产组织方式，全面转向加密解决方案吗？加密技术在 AI 领域的应用不能只是空中楼阁，还需要面对现实的考验。去中心化平台的发展必须面对现有中心化解决方案的直接竞争，并在更广阔的商业环境和监管框架下接受全方位评估。仅仅为了追求所谓“去中心化”是不足以推动大规模应用的，更遑论颠覆和取代现有的中心化服务提供商。况且，在市场竞争和开源平台的推动下，AI 行业目前其实已经实现了一定程度的去中心化。加密解决方案虽然理论上可行，但要达到与成熟中心化平台相当的功能水平，仍需投入大量的资源和时间。而与此同时，中心化平台也不会停滞不前。去中心化系统赖以存在的共识机制决定了其开发推进速度通常会比中心化开发更慢，这一点在 AI 这样快速迭代的领域会显得尤为致命。总的来说，AI 和加密技术的交叉领域整体仍处于初期阶段，整个市场充满了变数，其发展轨迹可能会随着 AI 行业的演进而快速变化，而 AI 行业本身的未来走向也尚未明朗。因此，投资者和开发者需要更深入地研究基于加密技术的解决方案如何才能提供独特的价值，或者至少充分理解潜在的市场叙事及其背后的驱动逻辑。“AI × 加密”代表了一个充满机遇和挑战的新前沿，成功的关键在于在技术创新、市场需求和监管合规之间找到平衡点，同时保持对行业动态的敏锐洞察力。须知技术本身绝非目的，而是达成更高社会价值的手段。在 AI 与加密技术的融合进程中，我们不应被炒作和理想主义所迷惑，而要始终保持理性和务实的态度。只有那些能够切实解决实际问题、创造真实价值的项目，才能在这个快速演变的生态系统中站稳脚跟，并最终推动整个行业向前迈进。"
  },
  
  {
    "title": "拆分与重构：模块化架构下的区块链新生态",
    "url": "/posts/modular/",
    "categories": "Blockchain",
    "tags": "da, modular, rollup",
    "date": "2024-03-08 12:00:00 +0800",
    





    
    "snippet": "1 背景区块链可扩展性问题一直是业界难以逾越的一道鸿沟，众多项目努力创新却始终无法突破“不可能三角”的桎梏。随着去中心化应用生态的蓬勃兴起，对区块空间的需求激增，原有架构难以负荷，吞吐量瓶颈日益显现。如果可扩展性问题无解，那么应用层的繁荣也将无以为继。“模块化区块链 (Modular Blockchain) ”范式正是在此背景下提出的。模块化概念起源于两篇文章。第一篇是由 Mustafa A...",
    "content": "1 背景区块链可扩展性问题一直是业界难以逾越的一道鸿沟，众多项目努力创新却始终无法突破“不可能三角”的桎梏。随着去中心化应用生态的蓬勃兴起，对区块空间的需求激增，原有架构难以负荷，吞吐量瓶颈日益显现。如果可扩展性问题无解，那么应用层的繁荣也将无以为继。“模块化区块链 (Modular Blockchain) ”范式正是在此背景下提出的。模块化概念起源于两篇文章。第一篇是由 Mustafa Albasan 和 Vitalik Buterin 在 2018 年共同撰写的，题为《欺诈与数据可用性证明》。这篇文章阐述了如何在保持安全性和去中心化的前提下解决区块链的可扩展性问题。具体方法是让轻节点接收并验证来自全节点的欺诈证明，同时设计数据可用性证明系统，以减小链上容量与安全性之间的取舍。随后在 2019 年，Mustafa Albasan 撰写了关于 Lazy Ledger 的白皮书，详细介绍了一种旨在解决可扩展性问题的创新架构。在此架构中，区块链仅用于对交易数据进行排序和确保其可用性，而不负责执行和验证交易。后续 Rollup 的出现使这一概念更加明晰。归根到底，模块化区块链的核心思想非常简单：依照功能层级将 Layer 1 区块链的核心组件解耦，划分为相互独立的功能模块，由此即可对不同模块进行针对性的修改和扩展，从而实现更具可扩展性、可组合性的去中心化系统。本文将从传统区块链架构和实现方式所面临的问题出发，详细介绍模块化范式的概念、关键技术原理以及生态发展情况。1.1 区块链分层结构区块链本质上是一个由交易驱动、状态不断更新的分布式状态机，每个状态转换由一个区块表示，所有节点通过特定共识机制达成一致并共同维护该状态机。图1 区块链状态转换区块链系统的运行过程中涉及了状态转换的计算、交易数据的存储、共识达成等环节，由此也可将区块链划分为四个基本层级：  执行层 (Execution) ：负责从当前状态出发，执行交易的计算逻辑，根据交易输入和执行规则，计算出新的状态转换。  结算层 (Settlement) ：负责 Rollup 的证明验证和争议仲裁，为其提供最终性保障。结算层是模块化架构中的可选部分，单体链架构中不存在这一层。  共识层 (Consensus) ：负责确认状态转换的有效性，确保所有分布式节点达成共识，对状态机状态保持一致视图。可以为结算层和数据可用性层提供安全性保障  数据可用性层 (Data Availability) ：负责发布和存储验证状态转换所需的交易数据，保障其可及性和可用性，确保交易可被验证和审计。单体链架构和模块化架构的核心区别就在于：单体区块链在一条链上同时执行上述四层的功能，而模块化区块链则尝试将不同的功能分配到多个不同的链上完成。图2 单体架构与模块化架构一般而言，常见的 Layer 1 公链都可被视为单体区块链，其优点是可以从钱包、应用、中间件到基础设施，全方位独立地建立完整的生态系统，各方之间维持紧密的关系。但是随着生态发展壮大，单体链通常会面临交易阻塞、交易成本上升、网络参与门槛过高以及维护全网状态的成本增加等问题。本质上，单体链同时负责所有功能的架构决定了其必然要面对所谓的区块链“可扩展性三难”。1.2 可扩展性三难与分布式系统领域的“CAP定理”类似，区块链“可扩展性三难”（或称“区块链不可能三角”）要求区块链的架构必须在安全性、去中心化和可扩展性这三种属性之间做出权衡，最多取二舍一，三者不可兼得。图3 区块链可扩展性三难具体而言：  安全性 (Security) ：指网络在遭受攻击时仍能正常运行的能力。这一特性是区块链的核心准则，不应妥协。因此，实践当中取舍通常发生在可扩展性和去中心化之间。  去中心化 (Decentralization) ：指网络中控制权的分散程度。Web2 和 Web3 系统之间、区块链和传统分布式系统之间的核心区别是可验证性，用户可以通过亲自操作全节点来验证区块链的运行状况。因此可以用全节点数量来衡量区块链系统的去中心化程度。要使区块链更去中心化，运行节点的硬件要求就不应成为参与网络的限制因素，验证网络的资源要求也应当较低。  可扩展性 (Scalability) ：指区块链的吞吐量与其验证成本的比率，即区块链在保持验证资源要求较低的条件下处理更多交易的能力。提高吞吐量的方法主要有两种。一是增加区块大小，从而增加每个区块可以包含的交易数量。但大区块必然导致网络中心化，因为运行全节点的硬件要求会随着计算输出需求的提高而提高。第二种方法是将执行转移到链下进行，将计算负担从主网节点外包出去，利用证明来在链上验证计算结果。模块化架构旨在通过“关注点分离”原则解决区块链可扩展性三难。例如，通过分离执行层和数据可用性层，区块链可以在保持网络的去信任化和去中心化特性的同时扩展吞吐量，这本质上是通过打破计算和验证成本之间的相关性来实现的。2 模块化概览模块化区块链由相互连接的模块或组件构成，每个模块负责特定的功能。这种架构简化了系统的开发、测试和维护，同时增强了其灵活性和可扩展性。通过将组件解耦，并分别进行针对性优化，可以实现 1 + 1 &gt; 2 的效果。在模块化生态中，每条链只负责少数功能，其余部分都交由其他层来完成，不同的链成为了一个个“可插拔”的模块，开发者可以根据用例自由进行组合或替换。当前的模块化生态主要根据上述区块链四层模型来划分功能，为使读者对各层级的作用有更直观的理解，这里用 F1 比赛进行类比：  执行层根据交易输入和执行规则计算得出新的状态转换，类似于 F1 比赛按规则进行并计算出车队积分；  结算层负责证明验证和争议仲裁，类似于在 F1 比赛期间组委会回看录像、做出判罚并解决车队争端；  共识层负责确保分布式节点间形成共识，类似于各地的车迷观看同一场比赛，并就积分结果达成一致；  数据可用性层负责发布和存储验证状态转换所需的数据，类似于直播并录制 F1 比赛，所有人都能通过观看回放来验证积分是否正确。在对各层级的作用和功能有了清晰直观的认识后，接下来将简要介绍各层的背景及项目发展现状。2.1 执行层在执行层项目真正落地之前，我们经常听到一个词：以太坊杀手。不少新公链尝试从单体链本身的交易结构、区块设计、共识机制和网络广播机制等方面着手进行探索和研究，依靠有着海量交易吞吐量、极快交易速度和低廉交易成本的高性能公链与以太坊正面竞争，希望取而代之。可见用户对于交易吞吐量、交易速度和交易成本等方面的需求与以太坊当时的性能存在着严重的不匹配。与此同时，以太坊生态对于各种各样的技术和解决方案也在持续进行探索研发。自 2020 年 Vitalik Buterin 发表文章《以 Rollup 为中心的以太坊路线图》以来，以太坊经历了多次升级，逐渐向 Rollup 中心架构转型，该架构不再强调以太坊基础层本身的可扩展性，转而专注于共识、结算和数据可用性，模块化叙事也由此开始受到广泛关注。时至今日，以 Rollup 为主体方案的执行层 L2 路线已经占据主导地位，其中基于欺诈证明的 Optimistic Rollup（Optimism、Arbitrum）在项目构建、用户吸引和留存方面都陆续超越其他 EVM 兼容的新公链，此外，基于零知识证明的 ZK Rollup（Starknet、Hermez、zkSync、Scroll、Taiko 等）以及并行交易项目（Fuel，AltLayer、Smooth）也在各自的方案领域持续发展。共享定序器、互操作协议、Rollup 框架以及 Rollup-as-a-Service (RaaS) 等基础设施在快速涌现，这些解决方案能进一步优化执行层 L2 技术栈，更好地将单体链功能抽象到专门的层级中。图4 以太坊与 L2 单日交易量占比根据 Dune 的数据，自上线以来，以太坊 L2 的活跃度始终保持着强劲的上升趋势，L2 单日交易量相较一年前增长近三倍，当前占以太坊单日总交易量的 73.6%。图5 以太坊与 L2 单日活跃地址数L2 上的单日活跃地址数超过 64 万个，一年内增长近六倍，比以太坊多出近 23 万个。这些数据体现出了当前执行层 L2 的关键地位和重要作用，未来，随着大量用户和开发者被吸引而来，执行层 L2 还将继续为区块链扩容和模块化生态的发展提供动力。2.2 结算层在模块化架构中，结算层是可选层，可作为 Rollup 和其他应用的部署平台，也可为传统智能合约应用提供具有共享流动性的通用执行环境。Rollup 在结算层上以智能合约的形式部署。合约既负责 Rollup 交易证明的发布和验证，也可以充当到 Rollup 的跨链桥。结算层可以将 Rollup 桥接回结算层然后再桥接到另一个 Rollup，也可以用状态转换证明来验证所有部署在结算层的 Rollup 的状态，以实现低成本、低延迟的跨链桥。结算层依靠智能合约执行和 Rollup 数据发布赚取手续费，对用户来说手续费相较于 L1 会低很多，因为执行层的交易数据会被压缩发布到共识和数据可用性层，这些基础层的执行成本会由结算层用户分摊。总体而言，结算层作为模块化范式的独特组成部分，为 Rollup 提供了争议解决机制、高效的跨链桥方案和共享流动性的途径，开发者根据需求精准定制 Rollup。图6 各类结算层项目在实际应用中，结算层有多种实现方式：可以将高性能的单体公链用作结算层，也可以选择专门用于结算的模块化链。前者的实现形式类似于当前典型的 L2 Rollup，后者则可以根据需求做出更多针对性的优化：  结算层的主要服务于部署在其之上的 Rollup，因此专门化的结算链可以通过限制应用的部署，尽可能减少用户的直接交互，从而为 Rollup 提供更大的区块空间。面向用户的应用应部署在专门化的执行层，而执行层则部署在专门化的结算层，由此通过更清晰的职责划分实现降本提效。  结算层可以利用欺诈证明或有效性证明创建信任最小化轻节点。结算层轻节点接收并验证证明，独立判断区块的有效性并拒绝无效区块，无需再依靠共识盲信验证者，系统安全性进一步提升。想要验证结算层的执行层只需运行一个轻节点即可。2.3 共识层当前，PoS 共识机制在 Web3 世界已成为主流。PoS 共识的核心是利用权益保护网络，权益的价值将决定网络的总体价值，只有足够高价值的权益才能保障高价值的网络。著名的 PoS 网络包括 Cosmos、波卡和以太坊等。Cosmos 秉承最小化信任机制，作为 Hub 不会干预生态应用链的共识过程。虽然 Cosmos 生态的应用链可以利用完善的开发栈，但是，建立和维护应用链网络的验证者集需要极高的门槛和成本，这也是信任和安全的代价。许多应用链通常通过空投吸引 Cosmos 的验证者，以高额的通胀奖励鼓励他们进行质押以保护网络。为了降低共识机制的成本并提高应用链的安全性，Cosmos 2.0 提出了各种改进方案，例如借助 Cosmos 的安全共享 ICS 为应用链提供安全性，以及通过 Space Mesh 实现应用链的共识共享等。此外，Cosmos 生态的 Babylon 还尝试引入比特币网络的 PoW 共识，以增强应用链的安全性。波卡融合了强大的链上治理模型和先进的共识理念。通过平行链卡槽拍卖机制，波卡直接将共识机制的保障范围扩展到其他链的交易。尽管这些机制在共识复用方面具有前瞻性理念，但由于链上治理效率和共识需求之间存在不匹配，导致了波卡平行链逐渐淡出视野。以太坊作为共识层资源的优势非常明显，多年的 PoW 发展历程使其积累了大量的价值。经过多年的探索和迭代，以太坊的 PoS 机制已经日臻完善。结合其强大的智能合约平台和不断蓬勃发展的执行层产品，以太坊 PoS 成为共识层资源的前提条件已经成熟。在现有的以太坊质押逻辑基础上，通过设计合理有效的激励和惩罚机制，可以复用质押的以太坊，实现安全性的延伸和共享。EigenLayer 在这一领域进行了多年的研究，提出了再质押 (Restaking) 概念。再质押旨在解决区块链安全碎片化问题。在构建去中心化网络时，项目方必须确保其具备可靠的加密经济安全机制。但从零开始建立 PoS 共识需要巨额的资本投入，且要耗费较长时间。若项目发行代币来实现安全保障，则需说服生态参与者承担新币的价格风险和质押以太坊的机会成本。即便由此成功建立了共识，也难以与以太坊的安全性相媲美。再质押协议通过整合以太坊等大型链的安全性，并使其可被其他应用程序利用，来解决这一问题。质押者可以在获取主链质押奖励的同时，获得保护其他应用的额外收益。这一方案不仅提高了质押资产的资本效率，也为各种新兴应用引入了去中心化的安全机制。2.4 数据可用性层“Don’t trust, verify” 是区块链世界的一条核心准则，其中的 “verify” 就包含了对数据可用性的要求。具体而言，数据可用性指的是用户对于验证区块所需数据确实可供所有网络参与者访问的置信度。所有 L1 单体链其实都隐式地确保了 DA，如以太坊全节点会下载每个区块的所有数据副本，下载本身就要求数据必须可用，缺少数据的区块将被丢弃，不会被添加到区块链中。但对于模块化链、L2 Rollup 和轻节点而言，DA 问题显得尤为重要。与面向终端用户的应用层相比，DA 本质上是一个面向 Rollup 开发者的概念。在 Rollup 的运作模式下，DA 层应该是对用户透明的，应用用户通常不会直接与交易数据交互，只需享受 Rollup 带来的廉价、快速的交易体验即可。但尽管 DA 看似远离用户视线，其影响却不容小觑，DA 方案直接决定了 Rollup 的成本，而成本最终会传递给终端用户。可见 DA 在模块化生态中扮演着重要角色，在用户体验和开发者创新方面都起到了关键性作用。我们知道，目前的 L2 Rollup 是将执行层抽离出来，在 L1 外执行交易，然后将用于证明状态转换的数据发布到 L1 上。不同类型的 Rollup 方案需要发布的数据也不同，ZK Rollup 只需发布轻量的有效性证明，而 OP Rollup 由于欺诈证明的要求，需要将其所有交易数据发布到 L1 ，这实际就是将 L1 用作 DA 层。当前，两类 Rollup 都使用 Calldata 将数据发布到以太坊，Calldata 是与以太坊交易一同传递的数据，用于实现信息交换。由于 Calldata 本身的空间大小有限，且其中所有的数据都会被以太坊节点处理并永久存储于链上，这一方面会导致状态膨胀，给全节点带来更多存储负担，另一方面也带来了高昂的数据可用性成本。理论上而言，L2 的数据并不需要永久存储于以太坊 L1 上，仅需存储一定时间满足欺诈证明等验证需求即可。换言之，以太坊 L2 的交易数据在过去是缺乏合适的存储空间的。从数据上看，L2 80% 的交易成本以及 Gas 费用均来自于 Calldata 中昂贵的数据存储成本。以太坊吞吐量和区块空间有限，L2 性能受限的同时，还面临着与普通交易相同的手续费竞争。在需求高峰期，随着 Gas 费飙升，DA 成本甚至能占到 L2 手续费的 90% 以上。图7 L1 数据发布费用是 Rollup 的主要成本以太坊即将实施的 Dencun 升级正是为了解决上述问题，升级包括了 EIP-4844 ( “Proto-Danksharding” )，这一提案的应用将会在以太坊区块中引入一种新的数据存储结构：Blob。Blob 将专门用于存储 L2 向 L1 提交的交易数据。图8 以太坊 DankshardingProto-Danksharing 所引进的每个 Blob 大小为 128 KB，每个以太坊区块计划包含 3-6 个 Blob（0.375 MB - 0.75 MB），未来逐步拓展至 64 个。相比之下，目前以太坊每个区块可以容纳的数据大小不到 200KB，引入 Blob 后，以太坊区块可容纳的数据量将显著提高。引入 Blob 后，L2 所提交的交易数据将不再需要竞争 Calldata 的存储空间，而是直接提交至 Blob 中进行存储。并且，Blob 的数据会在大约一个月左右的时间后自动删除，进一步降低没必要的存储负担。Blob 的引进意味着 L2 的交易费用将极大地降低（约降低 90%），并且，由于 Blob 相当于为 L2 额外拓展了区块空间，L2 能同时提交的交易吞吐量也将显著提高。若 Dencun 升级后成功实现了一个区块外挂 3 个 Blob 的平均目标，L2 的吞吐量将有接近 2 倍的提升。若最终实现了一个区块外挂 64 个 Blob 的目标，L2 的吞吐量将有接近 40 倍的提升。另外，Blob 还拥有独立的费用市场。Proto-Danksharing 还引入了一种新型 Gas，称为 Blob Gas。EIP-4844 中的 Blob Gas 费用机制植根于之前引入的 EIP-1559 机制，其中 Blob 的存储空间将根据自己的费用市场进行拍卖。这意味着 Blob 的费⽤市场完全独立于区块空间的需求，以此提高网络资源分配的灵活性和效率。Blob 中的数据存储成本约为每字节 1 个数据 Gas (one data-gas per bype)，而 Calldata 定价为每字节 16 个数据 Gas (16 data-gas per byte)。相比之下，Blob 的数据存储成本显著低于 Calldata 的数据存储成本。目前，以太坊 Rollup 数据可用性市场仍由以太坊原生 DA 主导。但第三方独立的专用 DA 方案，如 Celestia、EigenDA、Avail 等已经陆续推出，后续以太坊原生 DA 的主导地位可能会受到挑战。专用 DA 层不仅对数据可用性采样、纠删码等技术方案进行了探索，也有各自对于数据可用性领域的研究突破，例如 Polygon Avail 的 Fast Sync 技术和 Celestia 的主权性和互操作性。未来值得关注的是，Celestia 等专用 DA 层究竟会成为主流还是被以太坊长期愿景中的完整 Danksharding 所取代。3 主要项目当前的模块化赛道项目众多，本节将选取具有代表性的项目进行详细介绍。主要包括：提出模块化区块链概念的数据可用性协议 Celestia，利用再质押原语扩展以太坊共识安全性的 EigenLayer，以及基于模块化方式构建的以太坊首个 SVM 通用 L2 Eclipse。3.1 Celestia作为首个提出模块化区块链概念的项目，Celestia 在该领域中有着极高的知名度和先驱地位。近来替代 DA 赛道蓬勃发展，Celestia 也功不可没。2023 年 11 月，Celestia 主网上线并获得了市场热捧，首次空投估值即达 20 亿美元，至今完全稀释估值 (FDV) 更是一路飙升至 180 亿美元以上。Celestia 也是首个专门针对数据可用性进行优化的网络，其数据可用性层由 PoS 链构成，基于 Cosmos SDK 构建，能为 L2 网络提供临时存储批量交易数据的专用空间。功能上，Celestia 仅负责数据可用性与共识部分，交易执行和结算负载都被转移至其他网络。Celestia 本身不执行智能合约，也不支持跨 Rollup 桥接和争议解决机制。其核心功能是：以信任最小化的方式存储和编码数据、达成数据顺序共识并允许用户检索数据。就本质而言，Celestia 更像是去中心化的数据平台，这种专门化的设计也使其上的 L2 网络能够有效避免以太坊上数据费用高昂和网络拥堵的问题。关键技术及原理Celestia 的可扩展性方案的核心是将执行与共识解耦，并引入数据可用性采样 (Data Availability Sampling, “DAS”) 和命名空间 Merkle 树 (Namedspace Merkle Trees, “NMTs”) 等关键技术。DAS 使轻节点无需下载整个区块数据即可验证数据可用性；NMTs 使得应用程序仅需处理与其相关的部分数据，从而显著降低了数据处理需求。图9 Celestia 的 2D Reed-Solomon 编码      数据可用性采样 (DAS) ：如图 10 所示，Celestia 使用 2D Reed-Solomon 编码处理区块数据，将其划分为 $k×k$ 个分片，排列成 $k×k$ 的矩阵，并通过多次 Reed-Solomon 编码扩展为 $2k×2k$ 的扩展矩阵。计算扩展矩阵行和列的 $4k$ 个独立 Merkle 根，再对这些根计算 Merkle 根，结果将被用作区块头中的区块数据承诺。要验证数据是否可用，Celestia 轻节点会对 $2k×2k$ 个数据分片进行采样。每个轻节点在扩展矩阵中随机选择一组坐标，向全节点查询所选坐标处的数据分片及对应的 Merkle 证明。如果轻节点每个采样查询都收到有效响应，则以高概率确认整个区块数据可用。具体而言，要使 2D Reed-Solomon 矩阵中的分片不可恢复，则在 $(2k)^2$ 个分片中至少需要 $(k+1)^2$ 个分片不可用。因此，轻节点应当从扩展矩阵中随机采样 $0&lt;s&lt;(2k)^2$ 个不同的分片，只有在收到所有分片时才接受该区块。给定如图 10 所示的 $2k × 2k$ 扩展矩阵 $E$，其中有 $(k + 1)^2$ 个分片不可用。如果节点从 E 中随机抽取 $0 &lt; s ≤ (2k)^2$ 个分片，那么至少抽到一个不可用分片的概率为：\\[p_1(X \\geq 1) = 1 - \\prod_{i=0}^{s-1} \\left(1 - \\frac{(k+1)^2}{4k^2 - i}\\right)\\]    图 11 显示了对于 $k = 32$ 和 $k = 256$，概率随着采样数 $s$ 的变化。在进行 15 次采样后，概率超过 99%，数据量仅相当于 $k = 32$ 时 0.4% 的区块大小，$k = 256$ 时 0.005% 的区块大小。    图10 概率 $p_1(X \\geq 1)$ 随采样数 $s$ 的变化    轻节点会将收到的附带正确 Merkle 证明的数据分片在网络中广播，因此只要 Celestia 网络中的轻节点共采样至少 $k×k$ 不同数据分片，诚实全节点就能够恢复完整的区块。    目前，DAS 技术使 Celestia 在区块链数据可用性扩展方面具备显著优势，尤其相较于仍在开发类似解决方案的以太坊。只要网络中有足够的节点对区块进行采样，Celestia 就能保持安全性。网络中轻节点越多，集体下载和存储的数据就越多，因而区块大小可以相应地增加，同时无需牺牲安全性或去中心化特性。Celestia 主网上线初期区块大小为 2 ~ 8 MB，可通过链上治理升级。        命名空间默克尔树 (NMTs) ：Celestia将区块数据划分为多个命名空间，每个应用（如 Rollup）对应一个命名空间，每个应用只需下载与其相关的数据，可以忽略其他应用的数据。为了实现这一点，节点要能证明所提供的数据是完整的，即返回了给定命名空间的所有数据。为此，Celestia 使用了命名空间默克尔树。NMT 是一种有序的 Merkle 树，使用修改过的哈希函数，树中的每个节点都包含了该节点所有后代节点中消息的命名空间范围。树的叶节点按照消息的命名空间标识符进行排序。    图11 命名空间默克尔树示例    在 NMT 中，每个非叶节点包含所有后代叶节点中最低和最高的命名空间标识符，以及子节点哈希值的拼接。由此可以创建 Merkle 包含证明，保证所返回数据的完整性。NMT 使应用可以仅下载与自身相关的交易数据，从而提高了网络效率。  发展潜力根据 Celenium 的数据，Celestia 已处理超过 1250 万笔交易，累计 Blob 大小为 4.8GB。Celestia 的优势在于其高性能、低成本的数据可用性服务。未来，Celestia 的成败取决于能否被执行层和结算层的协议广泛采用，进而吸引更多终端用户和资本。目前，L2 网络 Manta Pacific 以及 Aevo 和 Lyra 等面向高吞吐量场景的应用链已采用 Celestia 作为其数据可用性层。另一方面，将 Celestia 作为数据可用性层相较于使用以太坊原生 DA 会引入新的安全风险，依赖 Celestia 的 L2 网络和应用链需要信任其共识机制和加密经济保障。因此，对各类协议项目方而言，是否选择 Celestia 本质上是在安全性和可扩展性之间做权衡取舍。总体而言，与以太坊生态联系不紧密、性能要求高、成本敏感的新兴项目，如游戏、AI、社交等赛道的 DApp，未来与 Celestia 合作的可能性较大。而传统 DeFi 协议，由于对安全性要求较高且可扩展性带来的提升相对有限，可能更倾向于选择以太坊原生 DA。值得一提的是，Celestia 提供了以太坊回退 (Ethereum fallback) 机制，在 Celestia 主网出现中断时，L2 网络将回退到使用以太坊的 Calldata 来保障数据可用性，从而确保用户可以继续安全、无缝地进行交易，降低了安全风险。此功能目前已支持 OP Stack 和 Arbitrum Nitro。图12 Celestia 的以太坊回退机制此外，Celestia 还提供了桥接其模块化 DA 层与以太坊的 Blobstream。Blobstream 将 Celestia 的数据根承诺中继到部署在以太坊上的轻客户端智能合约中，确保数据的可用性和完整性。在以太坊上，L2 的智能合约可以通过与 Blobstream 轻客户端智能合约交互来验证特定数据块确实已经在 Celestia 上发布并可用。一旦验证了数据的可用性和完整性，L2 就可以安全地从 Celestia 网络读取数据，并在以太坊上重新执行或处理这些数据。由此即可实现用 Celestia 保障数据可用性的同时用以太坊进行结算，从而充分利用两个网络的优势，即 Celestia 的可扩展性和数据可用性，以及以太坊的安全性和去中心化特性。图13 Celestia 的 BlobstreamBlobstream 是实现 Celestia 可扩展、安全和去中心化区块链生态系统愿景的关键部分。这一技术促进了未来区块链技术所需的互操作性。Blobstream 当前的实现是基于零知识证明的 Blobstream X。在生态方面，Celestia 目前已经吸引了 Eclipse、Dymension、AltLayer、Saga 等一系列 L2 网络及提供商，并积极与跨链桥、结算层方案、游戏、排序器等赛道的项目合作。同时也在推进技术整合，如与 Arbitrum Orbit、Polygon CDK等集成，优化开发者体验。生态的不断扩张和多元化也反过来对 Celestia 的发展起到了正面作用，多个项目都宣布对 TIA 质押者进行空投，这也推动了其质押人数不断上升。3.2 EigenLayerEigenLayer 是基于以太坊构建的协议，引入了“再质押”这一加密经济安全的新原语。该机制允许在共识层上对 ETH 进行再次抵押。质押者可以通过 EigenLayer 重新利用自己质押的 ETH，为网络上的其他协议和应用提供安全服务，并从中赚取收益。本质上，EigenLayer 是通过再质押来聚合和扩展以太坊基础层的加密经济安全性，解决区块链安全碎片化的问题，并建立去中心化信任的市场。图14 EigenLayer 再质押流程如上图所示，传统意义上的以太坊原生质押，主要是以网络验证者为主，其最大的弊病在于门槛高 (32 ETH)、锁定期长。在此背景下，流动性质押平台应运而生。流动性质押协议基本业务就是将散户质押的 ETH 聚合成池，以满足以太坊验证节点 32 ETH 的要求。用户存入 ETH 的回报是协议 1:1 铸造的 ERC-20 衍生代币，也就是流动性质押代币 (Liquid Staking Token, “LST”)。若用户要取回 ETH，则 LST 退还(销毁)，同时获得质押的同等份额的 ETH 及奖励。目前规模最大的质押平台当属 Lido，占据了近 70 以上的市场份额。流动性质押协议主要解决了两个问题：  降低门槛：如 Lido 可以质押任意数量的 ETH 且无技术门槛；  释放流动性：如在 Lido 质押 ETH 可以获取 stETH，stETH 可以参与 Defi 或者近似等价地兑换 ETH。而 Eigenlayer 再质押则在此基础上更进一步，作为质押凭证的 LST 可以在 Eigenlayer 上进行再次质押。这一方面为以太坊生态的新兴应用提供了扩展的以太坊安全性，另一方面则为质押者带来更多收益。用户存入 LST 资产时，也会获得一个流动性再质押凭证 (Liquid Restaking Token, “LRT”)，用户可以使用 LRT 进行更多的金融操作，例如抵押或借贷等，从而进一步释放再质押中锁死的流动性。关键技术及原理EigenLayer 系统中的参与者有三类：      再质押者：质押其 ETH 或 LST 以保护网络上的其他协议，并从中赚取额外收益，同事承受额外的罚没条件。        节点运营商（验证者）：再质押者可选择由其资产委托给可信的节点运营商，无需自己运行 EigenLayer 节点。节点运营商从收益中抽成获利。        主动验证服务 (Actively Validated Services, “AVS”)：指构建在 EigenLayer 之上的服务，也称模块，旨在吸引再质押者为其提供安全性。AVS 可以是DA 层、虚拟机、预言机网络、跨链桥等。  基于该体系，EigenLayer 引入了两个核心概念：通过再质押实现的池化安全性、开放市场。      通过再质押实现的池化安全性：EigenLayer 使新模块无需发币，而是通过再质押的 ETH 来获得安全性。具体而言，在再质押者将 LST 或 ETH 锁定给验证者后，验证者即可选择要保护模块，并将提现凭证设置为 EigenLayer 智能合约，以接受在行为失当时的自动罚没。作为回报，被保护模块会为安全性和验证者服务付费，这些费用将发放给验证者和再质押者。由此，以太坊强大的加密经济安全性便被池化到了构建于其之上的其他协议之中。    图15 EigenLayer 的池化安全性机制        开放市场治理：EigenLayer 建立了一种开放的市场机制，允许验证者权衡风险回报比，自由选择为哪些模块提供池化安全性。模块则需要利用适当的激励措施吸引验证者。本质上，这一模式类似于风投公司和创业者间的关系，再质押者在支持创新并获取额外收益的同时也要承担相应（罚没）风险。EigenLayer 认为这一设计是对区块链治理能力的补充和完善，同时也使新模块能够有效利用验证者中的异构资源，从而更好地实现安全性和性能的权衡。  这两个因素共同建立起了竞争性的自由市场，新协议可以直接购买池化的以太坊安全性，因而消除了启动新共识网络所需的大量资金成本。理想情况下，这一设计还将具有飞轮效应：由 EigenLayer 保障的协议越有价值 =&gt; 对再质押者的回报就越高 =&gt; ETH 的价值提高 =&gt; 以太坊的安全性提高 =&gt; 为 EigenLayer 上的协议带来更高的安全性 =&gt; 进一步激励开发者在其上构建新协议。发展潜力图16 EigenLayer TVL截至目前，EigenLayer 的 TVL 已超过 300 万 ETH，折合约 110 亿美元，其增长速度惊人，现已成为以太坊上锁仓量仅次于 Lido （以太坊头部流动性质押协议）的项目。值得注意的是，这一数字是在 EigenLayer 设定了 LST 存款上限且还未推出任何 AVS 的情况下达到的。但其实我们很难将长期的质押需求与短期内用户对积分和空投挖矿的兴趣完全区分开来。随着协议的成熟，长期来看再质押的 ETH 数量或许仍会保持增长，但在积分挖矿结束或 AVS 奖励低于预期时，TVL 可能会在短期内出现下降。EigenLayer 协议本身及其所代表的再质押模式在推动创新的同时也存在着潜在风险：      验证者串谋攻击：由于验证者可以选择为多个不同的 AVS 进行再质押，理论上可能使攻击在经济上可行。EigenLayer 白皮书中对此提出了一个开源仪表板的解决方案，该仪表板可以监控验证者的再质押行为，使协议可激励仅参与少数 AVS 的验证者。    意外罚没：EigenLayer 上不成熟 AVS 的智能合约安全问题或程序 bug 可能导致意外罚没。针对这一问题，EigenLayer 提出了两种解决方案：          针对 AVS 代码库的安全审计；      可通过多签否决罚没决定的治理层 （此方案存在中心化问题，仅作为辅助手段存在，最终将被移除）。            罚没冲突：EigenLayer 允许将同一笔再质押的 ETH 质押给多个 AVS，这会导致不同服务间罚没和索赔条件的层级关系难以厘清。每个协议都会创建自己定制的罚没条件，因此可能出现：一个 AVS 因为不当行为要罚没再质押ETH，而另一个 AVS 则要求收回同一笔再质押 ETH 作为对受损参与者的补偿。这就导致了罚没冲突。随着越来越多的 AVS 启动，运营者的角色会变得更加复杂，罚没规则也更难以遵循。而在此之上，LRT 的引入进一步抽象了再质押的复杂性，可能令持币者难以识别潜在的风险。        协议可持续性：代币是协议经济激励和收入的主要来源，如果所有价值都经由 EigenLayer 流向 ETH，那么某些项目可能难以长期发展壮大。        LRT 估值风险：LRT 也存在不可忽视的估值风险，例如在质押退出队列延长时（如以太坊 Dencun 硬分叉后验证者波动限制从14 降至 8），LRT 可能暂时偏离其内在价值。而如果 LRT 像 LST 一样在 DeFi 中被广为接受且被用作抵押品，则可能加剧清算风险，在流动性较低的市场中尤甚。此外，LRT 本质上代表着一系列多元化的投资组合持仓，这些持仓的风险状况可能会随着时间的推移而发生变化。可以添加或移除成分，或者 AVS 本身也可能因其收益或偿付能力风险而发生变化。市场低迷可能会同时影响多个 AVS，进而危及 LRT 的稳定性，加剧被迫清算和市场波动的风险。递归借贷只会放大这些损失。        共识过载及协议类型受限：基于 EigenLayer 构建的协议在出现重大损失时如果依赖以太坊的底层共识，会将冲击传导至 L1 网络，从而引发以太坊网络自体的规范冲突，也就是所谓“共识过载”问题，用金融世界的话语就是：对共识的过度加杠杆可能引发共识本身的坍塌。如 Vitalik Buterin 的文章所言，以太坊不能对任何应用级别的失误负责，再质押平台上复杂金融系统出错造成的损失是不应依靠以太坊 L1 硬分叉来修正或弥补的。EigenLayer 的白皮书和其 CEO 的推文也显示出对这一观点的认同。这可能会限制能在 EigenLayer 上构建的协议类型。    We welcome this excellent analysis of the different kinds of risks using restaking for different use cases a la @eigenlayer by @VitalikButerin. It is consistent with what we have been advocating with Eigenlayer. A brief summary here: https://t.co/8ppF9jUp6t&mdash; Sreeram Kannan (@sreeramkannan) May 21, 2023      3.3 EclipseEclipse 是全面拥抱模块化叙事的典型项目，其主要目标可以概括为：通过合理利用模块化技术栈来构建以太坊上性能最优的通用 L2 方案。具体而言，在当前的架构中，Eclipse 使用了并行处理的 Solana 虚拟机 SVM 作为其执行环境，使用 Celestia 作为数据可用性层，并在以太坊主网上进行结算。由此 Eclipse 将 SVM 的速度和可扩展性、Celestia 的吞吐量和可验证性以及以太坊的安全性和流动性结合了起来，最大限度地发挥出了模块化架构的优势。图17 Eclipse 的模块化架构关键技术及原理      执行：SVM    SVM 及其 Sealevel 运行时亮点在于支持交易的并行处理，不会导致数据竞争或状态冲突的交易将被分配到不同的处理器核心中并行执行。得益于此，SVM 可以随计算机处理器核心数量的增长和成本下降而直接获得硬件级的扩展性。 而单线程运行时（例如现有的 EVM）则很难从单核心成本的下降中受益。由下图不难看出，过去十多年里，单线程性能的提升速度在不断降低，性能提升主要都来自处理器核心数量的增加，因此充分利用并行处理，顺应这一趋势至关重要：     图18 微处理器趋势数据    当前也有一些针对 EVM 并行化的初步尝试，但这些方案都存在一些限制：在保持 EVM 兼容性的前提下加入并行化功能，往往需要进行权衡，导致性能不佳。并且即便实现了并行化，也无法解决 EVM 面临的其他瓶颈问题，例如状态增长问题等。相比之下，SVM 采用了不同的技术路线，通过智能合约事先声明状态依赖关系的方式，实现了更优化的并行处理。          Solana 上的交易需要显示声明将要读写的内存位置（即账户）。以下这段代码定义了相关信息的格式，运行时会使用这些信息来决定哪些交易可以并行执行。      pub struct AccountInfo&lt;'a&gt; {   /// Public key of the account   pub key: &amp;'a Pubkey,   /// The lamports in the account.  Modifiable by programs.   pub lamports: Rc&lt;RefCell&lt;&amp;'a mut u64&gt;&gt;,   /// The data held in this account.  Modifiable by programs.   pub data: Rc&lt;RefCell&lt;&amp;'a mut [u8]&gt;&gt;,   /// Program that owns this account   pub owner: &amp;'a Pubkey,   /// The epoch at which this account will next owe rent   pub rent_epoch: Epoch,   /// Was the transaction signed by this account's public key?   pub is_signer: bool,   /// Is the account writable?   pub is_writable: bool,   /// This account's data contains a loaded program (and is now read-only)   pub executable: bool,}              SVM 的另一显著优势在于局部手续费市场。现有区块链的手续费市场大多采用全局模式，导致热门应用会抬高整个网络的手续费，影响其他用户。例如，NFT 铸造热潮可能令整个网络交易手续费飙升，普通用户的转账也无法以可接受的成本进行。Solana 通过局部手续费市场巧妙地解决了这一问题，有效避免了跨应用程序状态争用的问题。该方案的核心在于 Solana 独具特色的并行化运行时，令调度器能够高效识别并优先处理无冲突交易，让其以更低的手续费完成。而未来 Solana 将在协议层面实现局部手续费市场，更加彻底地解决跨应用程序状态争用问题，确保单个应用的手续费高峰不会影响网络的其他部分。而 EVM 由于缺乏天然的并行化能力，难以像 Solana 一样高效实现局部手续费市场，如果采用启发式方法，不仅效率低下，还可能带来新的安全风险。     图19 局部手续费市场能高效处理状态热点    兼容性方面，开发者可以利用 Neon EVM 和 Solang 在 Eclipse 主网上部署以太坊智能合约。Neon EVM 是一款以智能合约形式运行在 SVM 链上的 EVM，为 Eclipse 主网带来了完整的 EVM 兼容性，包括 EVM 字节码支持和 Ethereum JSON-RPC，同时吞吐量远超单线程 EVM。得益于 Neon EVM 的局部手续费市场机制，每个 Neon EVM 实例都拥有独立的手续费管理体系。应用只需部署自己的合约，即可享受应用链的优势，无需分散用户体验、安全性或流动性。但值得注意的是，基于 Neon EVM 的应用和基于 SVM 的 Eclipse 应用将无法互操作，使用 Neon EVM 部署的 DApp 类似于部署到 Eclipse 的应用 Rollup，两者在安全性方面也存在差异。此外，Solang 编译器能够将 Solidity 智能合约代码编译为 SVM 字节码，使开发者可以轻松将现有 Solidity 合约迁移到 Neon EVM 平台。        数据可用性：Celestia    以太坊当前的带宽不足以支持 Eclipse 主网的目标吞吐量和费用，EIP-4844 升级也无法解决这一问题。如前文所述，EIP-4844 平均每个区块提供约 0.375 MB 的数据块空间（上限为 0.75 MB），对于经过基本压缩（每笔交易约 154 字节）的 ERC-20 转账交易，所有 Rollup 总计约 213 TPS，而对于经过压缩 (每笔交易约 400 字节) 的 Swap，所有 Rollup 总计约 82 TPS。          以太坊每个 Slot 时间为 12s对转账交易：0.375 × 1024 × 1024 ÷ 154 ÷ 12 = 212.77 TPS对 Swap：0.375 × 1024 × 1024 ÷ 400 ÷ 12 = 81.92 TPS        相较之下，Celestia 的 Blob 空间达 8 MB，且 DAS 轻节点使用户能够自行验证 Eclipse 区块数据是否可用，随着更多 DAS 轻节点上线，DA 层可以安全地提高吞吐量，从而为整个网络的安全扩展做出贡献。Celestia 高度可扩展的数据可用性吞吐量、对轻节点的原生支持以及密码经济安全特性使其成为了 Eclipse 主网在数据可用性方面的最佳选择。如下图所示，Blobstream 将转发由 Celestia 验证者集签名的证明，向以太坊证明一批 Eclipse slot 的数据已正确发布。Eclipse 的验证桥可以利用来自 Celestia 的数据根来验证供欺诈证明使用的数据。     图20 Eclipse 各模块的交互    Eclipse 团队表示也将继续监测以太坊在 EIP-4844 实施后 DA 方面的进展，如果以太坊能为 Eclipse 提供更好的扩展能力从而惠及用户，Eclipse 也可能会考虑将 DA 迁移到以太坊。        结算：以太坊    Eclipse 与其他主要 L2 方案一样，也在以太坊上进行结算。Eclipse 内置了一个与以太坊连接的验证桥，Eclipse 节点需要同时运行以太坊全节点，并通过验证桥来确定自身的主链和交易顺序。由此， Eclipse 能从以太坊中继承部分安全性。该验证桥会验证所有 Eclipse 上的交易，防止提交无效状态。此外，在某些故障情况下，该验证桥还能保证最终的活性 (liveness) 和抗审查性 (censorship resistance)。即使定序器 (sequencer) 出现宕机或试图审查 L2 上的交易，用户也可以通过验证桥强制包含其交易。    以太坊结算也决定了 ETH 将在 Eclipse 生态中将扮演重要角色。目前 Eclipse 没有发币计划，使用 ETH 作为 gas 代币。        证明：RISC Zero    Eclipse 使用 RISC Zero 生成零知识欺诈证明，避免中间状态序列化带来的性能损耗。    总体而言，欺诈证明需要包含三类内容：          交易输入的承诺（一般通过提供 Rollup 状态树的 Merkle 根实现）      交易本身      重新执行该交易会产生不同于链上指定输出的证明        Eclipse 的执行器会为每个交易发布一个包含输入和输出的列表（包括账户哈希和相关全局状态），以及产生每个输入的交易索引。交易会发布到 Celestia 链上，任何全节点都可以按部就班地从自身状态中拉取输入账户、计算输出账户，并确认以太坊上的承诺是正确的。    可能出现的两种主要错误类型：          输出错误：在此情况下，验证器会在链上提供正确输出的零知识证明。Eclipse 使用 RISC Zero 为 SVM 的执行生成零知识证明，这使得结算合约无需在链上运行交易本身即可确保正确性。      输入错误：在此情况下，验证者会在链上发布指向历史数据的引用，证明输入状态与声明的不符。利用 Celestia 的 Blobstream，Eclipse 的结算合约可以确保该历史数据确实证明了欺诈行为。      发展潜力目前，Eclipse 仍处于开发和测试阶段，测试网已上线，计划在 2024 年第二季度上线主网。以太坊仍将 Rollup 视为其发展路线图的核心部分，抛开关于正统性的争论不谈，这也意味着以太坊将 L2 的广义定义留给了市场，在赋予开放性的同时，也巧妙地引入了各种形式的竞争。Eclipse 利用这一点，通过模块化开发将以太坊的安全性、Solana 的高性能以及 Celestia 的 DA 可扩展性相结合，构建出了强有力的市场叙事。早期的 Rollup 方案大多优先考虑 EVM 兼容性，或者专注于优化零知识证明的效率。但近来技术的巨大进步使这些权衡取舍不再必要，反而让现有方案处于劣势：  高性能并行虚拟机（例如 SVM）  支持 DAS 轻节点的 DA 扩容方案（例如 Celestia）  零知识证明基础设施（例如 RISC Zero）  跨生态的代码（例如 Neon 和 Solang）和用户 (例如 MetaMask Snaps) 可移植性作为以太坊上首个基于 SVM 的通用 L2，Eclipse 拥有巨大的后发优势。虽然 Optimism 的 Superchain、zkSync 的 Hyperchains、Arbitrum 的 Orbit chains 等都拥有共享基础设施的多链愿景，但都局限在优化同一生态内的跨链体验，并未解决跨生态的互操作性问题。模块化建设并不等于孤岛式发展。维护多个链上的账户会给用户带来更多复杂性，而依赖基础设施提供商来运营和维护多链也会更加昂贵。Eclipse 利用 SVM 的并行虚拟机和局部手续费市场，很好地解决了现有 L2 在复杂性、成本、用户体验、流动性等方面的权衡问题，将 Solana 的性能与以太坊的安全性、可验证性和网络效应结合了起来。Eclipse 的出现为以太坊和 Solana 生态带来了新的机遇和协同效应。对于以太坊生态而言，Eclipse 带来了 Solana 优越的性能和并行处理能力，为开发者提供了构建新型 DApp 的强大工具。而对于 Solana 生态而言，Eclipse 为开发者提供了从以太坊上获取大规模流动性和庞大用户群的机会，同时也将进一步推动围绕 SVM 的开发者和工具生态的繁荣发展。Eclipse 有望打破两大 L1 网络以往相对隔离的局面，为 Solana 和以太坊生态的融合带来新的可能性。4 总结面向服务的微服务架构已经成为现代软件开发的主流模式。其核心思想是将应用程序拆分成多个功能独立的服务，每个服务可以自主开发、部署和运行，服务之间通过通信和数据共享进行协作，从而实现更高的扩展性、灵活性和可维护性。尽管在分布式事务、服务治理、安全性等方面仍存在挑战，但随着技术的进步和经验的积累，这些问题正逐步得到解决。模块化区块链与微服务架构有许多相似之处。通过将区块链的各个层级解耦，开发者得以单独维护和优化特定功能模块，不同层级可灵活组合来适应更复杂的应用场景。模块化架构是对区块链价值主张的进一步延伸，也是对打破不可能三角的一次重要尝试。但与此同时，其设计的多样性也导致了系统复杂性的增加，如何构建稳定的模块化系统将成为重要挑战。此外，流动性的碎片化问题、跨链通信以及安全问题也同样值得关注。目前，模块化已经成为了区块链技术的重要探索方向，整个生态都在迅速发展，从执行层、共识层到数据可用性层，从底层的基础设施到上层的应用，都有新的技术和项目在不断涌现。未来，相信模块化区块链将会带来更多创新和机遇，为推动区块链技术的应用和发展做出重要贡献。"
  },
  
  {
    "title": "Ordinals & BRC-20 技术原理详解",
    "url": "/posts/ordinals/",
    "categories": "Blockchain",
    "tags": "bitcoin, inscription, ordinals",
    "date": "2024-01-11 13:00:00 +0800",
    





    
    "snippet": "本文从技术的角度出发，深入底层，详细介绍了比特币 Ordinals 协议以及 BRC-20 代币标准的原理。主要内容包括 Colored Coins 项目介绍、SegWit 与 Taproot 升级详解、序数铭文及 BRC-20 代币标准说明。1 背景知识1.1 比特币交易模型在比特币网络中，比特币的形式是“未花费交易输出” (Unspent Transaction Output, UTXO...",
    "content": "本文从技术的角度出发，深入底层，详细介绍了比特币 Ordinals 协议以及 BRC-20 代币标准的原理。主要内容包括 Colored Coins 项目介绍、SegWit 与 Taproot 升级详解、序数铭文及 BRC-20 代币标准说明。1 背景知识1.1 比特币交易模型在比特币网络中，比特币的形式是“未花费交易输出” (Unspent Transaction Output, UTXO)，即比特币交易执行后形成的面额不定的单元，其原理可简单类比现实中的现金支付，与传统的基于账户余额的模型有所不同。举例而言：在银行的账户记账模型流程中，当 A 向 B 转账 100 元时，银行会记录三个步骤，这三个步骤构成了一个交易过程。第一步是从 A 的账户中扣除 100 元，这个步骤的记录 ID 为 tid1。第二步是将 100 元存入 B 的账户中，这个步骤的记录 ID 为 tid2。第三步是记录一笔转账记录，该记录将 tid1 和 tid2 关联起来，表示 A 账户减少 100 元，B 账户增加 100 元。这样，A 和 B 之间的转账关系就被记录了下来，并且可以在未来查询与追踪。而在比特币区块链中，所有的余额都是存储在一个名为 UTXO 的列表中。每个 UTXO 都包含一定数量的比特币，以及这些比特币的所有者信息，并标明是否可用。可以将其想象成一张署有持有人姓名的现金支票，只要持有人在上面签名，就可以将使用权转让给他人。对于特定的地址，其所有的 UTXO 金额加起来即为该地址钱包的余额。通过遍历所有的 UTXO，我们可以获取每个地址的当前余额。将所有的 UTXO 金额加总，则为当前全部流通的比特币。在比特币的交易结构中，每笔交易都包括若干个输入和输出，其中每个输入是对一个已有的 UTXO 的引用，而每个输出则指定了新的资金接收地址及相应的金额。一旦一笔交易被发起，其输入部分所引用的 UTXO 便会被暂时锁定，以防止在交易完成前被重复使用。只有当这笔交易成功地被矿工打包到一个区块并获得网络确认后，相关的 UTXO 状态才会发生变化。具体来说，用于交易输入的 UTXO 将从 UTXO 列表中移除，表示它们已经被消费，而交易的输出则会生成新的 UTXO，并添加到 UTXO 列表中。可以理解为，旧现金支票被使用后失效，产生了新现金支票，其所有权属于新的持有人。值得强调的是，每个 UTXO 只能在一笔交易中被使用一次。一旦它作为输入被消费，它就会永久地从 UTXO 列表中移除。同时，新生成的输出作为新的 UTXO 加入到列表中。UTXO 列表是不断变化的，随着每个新区块的创建，它会相应地进行更新。并且，通过分析区块链中的交易历史，我们能够重建在任何给定时间点的 UTXO 列表状态。此外，一笔交易的总输入金额通常会略微超过其总输出金额。这个差额，称为交易费用 (Transaction fee)，是作为激励给予负责将交易打包到区块的矿工的。网络费的大小与交易的复杂性成正比，因此，一笔包含更多输入和输出的交易通常需要支付更高的网络费。图1 是通过区块链浏览器展示交易信息的例子（假设场景为 Alice 使用了 0.1 BTC，其中的 0.015 BTC 转账给了 Bob 用于购买咖啡，而 0.0845 BTC 退给了自己，0.0005 BTC 是转账手续费，由打包者获得）。这个交易在主网上真实存在，参考： 0627052b6f28912f2703066a912ea577f2ce4da4caa5a5fbd8a57286c345c2f2 。图1 交易信息上图中是比较高层次的交易信息。使用 bitcoin-cli 子命令 getrawtransaction 和 decoderawtransaction ，可以得到上面 Tx 的底层结构如下：{    \"version\": 1,    \"locktime\": 0,    \"vin\": [        {            \"txid\": \"7957a35fe64f80d234d76d83a2a8f1a0d8149a41d81de548f0a65a8a999f6f18\",            \"vout\": 0,            \"scriptSig\" : \"3045022100884d142d86652a3f47ba4746ec719bbfbd040a570b1deccbb6498c75c4ae24cb02204b9f039ff08df09cbe9f6addac960298cad530a863ea8f53982c09db8f6e3813[ALL] 0484ecc0d46f1918b30928fa0e4ed99f16a0fb4fde0735e7ade8416ab9fe423cc5412336376789d172787ec3457eee41c04f4938de5cc17b4a10fa336a8d752adf\",            \"sequence\": 4294967295        }    ],    \"vout\": [        {            \"value\": 0.01500000,            \"scriptPubKey\": \"OP_DUP OP_HASH160 ab68025513c3dbd2f7b92a94e0581f5d50f654e7 OP_EQUALVERIFY OP_CHECKSIG\"        },        {            \"value\": 0.08450000,            \"scriptPubKey\": \"OP_DUP OP_HASH160 7f9b1a7fb68d60c536c2fd8aeaa53a8f3cc025a8 OP_EQUALVERIFY OP_CHECKSIG\",        }    ]}可以看到交易中有 Input 和 Output（分别是上面的 vin 和 vout）。Input 标识哪个 UTXO 将被消费，并通过解锁脚本提供所有权证明。Output 则创建新的 UTXO 比特币块并用锁定脚本锁定，供所有者在未来交易中使用。比特币网络中交易输出有两个重要信息：地址（公钥哈希）和 value（比特币）。如果交易的输出没有出现在其它交易的输入中，则这个 Tx 的输出就称为 UTXO（未花费交易输出）。谁拥有 UTXO 中公钥对应的私钥，谁就可以使用（即花费）这个 UTXO。上例中交易的输入为：     \"vin\": [        {            \"txid\": \"7957a35fe64f80d234d76d83a2a8f1a0d8149a41d81de548f0a65a8a999f6f18\",            \"vout\": 0,            \"scriptSig\" : \"3045022100884d142d86652a3f47ba4746ec719bbfbd040a570b1deccbb6498c75c4ae24cb02204b9f039ff08df09cbe9f6addac960298cad530a863ea8f53982c09db8f6e3813[ALL] 0484ecc0d46f1918b30928fa0e4ed99f16a0fb4fde0735e7ade8416ab9fe423cc5412336376789d172787ec3457eee41c04f4938de5cc17b4a10fa336a8d752adf\",            \"sequence\": 4294967295        }     ]表示这个 Tx 所花费的 UTXO 来自于另外一个 Tx（其 id 为 7957a35fe64f80d234d76d83a2a8f1a0d8149a41d81de548f0a65a8a999f6f18）的第 0 个输出（一个 Tx 的输出可以有多个，索引从 0 开始编号），我们可以从历史 Tx 中查找出这个 UTXO 的 value（比如为 0.1），所以这个 Tx 中 Alice 花费了 0.1 BTC，数值 0.1 不需要显式地写在 Tx 中，而是通过查找 UTXO 信息来得到的。一旦这个 Tx 被提交，那么 Tx（7957a35fe64f80d234d76d83a2a8f1a0d8149a41d81de548f0a65a8a999f6f18）的第 0 个输出就不再是 UTXO 了。这个 Tx 中，输出有两个条目，如下所示：    \"vout\": [        {            \"value\": 0.01500000,            \"scriptPubKey\": \"OP_DUP OP_HASH160 ab68025513c3dbd2f7b92a94e0581f5d50f654e7 OP_EQUALVERIFY OP_CHECKSIG\"        },        {            \"value\": 0.08450000,            \"scriptPubKey\": \"OP_DUP OP_HASH160 7f9b1a7fb68d60c536c2fd8aeaa53a8f3cc025a8 OP_EQUALVERIFY OP_CHECKSIG\",        }    ]这两个条目刚开始都是 UTXO，直到有另外的 Tx 把它们作为输入花费掉为止。发起⼀笔交易时，发送方在交易中放入 scriptPubKey，即上文提到的锁定脚本。接收方在花费时生成⼀个 scriptSig，即上文提到的解锁脚本，包含了满足 scriptPubKey 脚本的数据参数的集合。针对解锁的条件，可以将比特币交易分为多类，其中经典的两类是 P2PKH 和 P2SH。P2PKHPay-to-PubKeyHash (P2PKH) 是⼀种传统的比特币交易，其地址以数字 1 开头。只有P2PKH 地址的所有者才能通过提供公钥哈希值和私钥签名来解锁 scriptPubKey 并花费其锁定的 UTXO。其中，私钥是用来证明公钥哈希值的所有权的。如下图所示，发送交易时，发送方需要在交易中包含 scriptPubKey。当接收方想要解锁这笔 UTXO 时，需要执行下图中的比特币脚本，如果 PubKHash 相等，且签名验证通过，就可以成功解锁交易。P2PKH 的脚本会增加交易的体积，产⽣的交易费比普通交易⾼出 5 倍左右，且该成本由发送方承担。P2SH 可以帮助发送方免去这⼀额外成本。图2 P2PKHP2SHPay-to-Script-Hash (P2SH) 可以帮助发送方免去额外成本，并将这⼀成本转移到真正需要使用锁定脚本中规定条件的接收方身上。P2SH 的比特币地址以数字 3 开头。发送交易时，发送方不再需要将 scriptPubKey 放在交易中，而是将赎回脚本 (Redeem Script) 哈希值放在交易中。赎回脚本哈希值由赎回脚本计算得到，赎回脚本与 scriptPubKey 类似，包含接收方在花费 UTXO 之前必须满足的条件。如下表所示，发送方只需在交易中注明赎回脚本的哈希值即可。而接收方想要解锁 UTXO 时，需要⽣成具有相同哈希值的赎回脚本并将其包含到交易内。因此，接收方用来解锁 UTXO 的交易大小会增加，执行交易的成本也会增加。表1 P2SH复杂脚本            Item      Value                  Redeem Script      2 PubKey1 PubKey2 PubKey3 PubKey4 PubKey5 5 CHECKMULTISIG              Locking Script      HASH160 &lt;20-byte hash of redeem script&gt; EQUAL              Unlocking Script      Sig1 Sig2 &lt; redeem script &gt;      1.2 比特币脚本比特币中的脚本 (Script) 是一种简单的基于栈的、非图灵完备的语言。这里以 Pay-to-Public-Key-Hash (P2PKH) 类型的脚本为例进行说明。scriptPubKey: OP_DUP OP_HASH160 &lt;pubKeyHash&gt; OP_EQUALVERIFY OP_CHECKSIGscriptSig: &lt;sig&gt; &lt;pubKey&gt;为了解释得更清楚，原始的 scriptPubKey如下：  76       A9             14OP_DUP OP_HASH160    Bytes to push89 AB CD EF AB BA AB BA AB BA AB BA AB BA AB BA AB BA AB BA   88         AC                      Data to push                     OP_EQUALVERIFY OP_CHECKSIG注意：scriptSig 在花费交易的输入中，主要包含转账的目标地址（公钥的 Hash）。而 scriptPubKey 在先前未花费交易（即“可用”交易）的输出中，主要包含签名和公钥（由此，我们可以利用公钥验证签名，从而确定用户身份）。脚本处理过程如下：表2 P2PKH流程说明            栈      脚本      说明                  空      &lt;sig&gt; &lt;pubKey&gt; OP_DUP OP_HASH160 &lt;pubKeyHash&gt; OP_EQUALVERIFY OP_CHECKSIG      组合 scriptSig 和 scriptPubKey              &lt;sig&gt; &lt;pubKey&gt;      OP_DUP OP_HASH160 &lt;pubKeyHash&gt; OP_EQUALVERIFY OP_CHECKSIG      常量入栈              &lt;sig&gt; &lt;pubKey&gt; &lt;pubKey&gt;      OP_HASH160 &lt;pubKeyHash&gt; OP_EQUALVERIFY OP_CHECKSIG      栈顶元素被复制              &lt;sig&gt; &lt;pubKey&gt; &lt;pubHashA&gt;      &lt;pubKeyHash&gt; OP_EQUALVERIFY OP_CHECKSIG      计算栈顶元素的哈希              &lt;sig&gt; &lt;pubKey&gt; &lt;pubHashA&gt; &lt;pubKeyHash&gt;      OP_EQUALVERIFY OP_CHECKSIG      常量入栈              &lt;sig&gt; &lt;pubKey&gt;      OP_CHECKSIG      检查确认栈顶两元素相等              TRUE      空      使用栈顶两元素验证签名合法      交易验证时，在栈上执行脚本，先执行交易输入中的 解锁脚本 (scriptSig)，再执行这个输入所引用的 UTXO 中的锁定脚本（scriptPubKey），如果最后栈中内容为 TRUE，则认为这个输入是合法的，如果所有的输入合法则整个交易通过验证。2 从历史出发：Colored Coins概述2012 年 12 ⽉ 4 ⽇，为了在比特币区块链上体现股票、房地产等实物资产，密码学家、数学家、以色列比特币协会主席 Meni Rosenfeld 发布了彩色币白皮书 Colored Coins whitepaper。比特币⾃设计之初就是同质的、可替代的，是⼀种中立的交换媒介。如果仔细追踪特定比特币的来源，可以将⼀组比特币 “上色” 成为彩色币，使其与其他比特币区分开来。这样，这些比特币就具备了非同质化特性，可以具有由发行机构或公共协议支持的特殊属性，并包含了独立于基础比特币面值的价值。这种彩色比特币可用于替代货币、商品证书、智能财产以及股票和债券等其他金融⼯具。例如，假设有⼀家汽车租赁公司，通过发布⼀枚彩色币来代表⼀辆汽车，然后将汽车配置为只有在收到用当前拥有该彩色币的私钥签署的信息时才会解锁。然后，该公司可以发布⼀个智能⼿机应用程序，任何人都可以用它来广播⼀条用⾃己的私人密钥签名的信息，并将彩色币放到⼀个交易平台上。基于此，任何人都可以购买彩色币，使用智能⼿机应用程序作为“汽车钥匙”，在任何时间内使用汽车，并在闲暇时再次出售彩色币。实现从技术角度来说，彩色币是在创世交易（不同于创世区块中的创世交易）里面被转移过的比特币。要发行⼀种新颜色的彩色币，必须创建 “彩色地址” 并将其存储在由颜色感知客户端（例如 Coinprism、Coloredcoins、Colu 或CoinSpark）控制的 “彩色钱包” 中。彩色币创世交易的输入和输出必须遵循特定的规则。输出包括⼀组将彩色币发送给原始所有者的输出、⼀个 OP_RETURN 数据输出、⼀个或多个“找零”输出（即将多余的未着色比特币发送回发行者）。发行者可以选择发行 Non-reissuable colors 或者 Reissuable colors。OP_RETURN 输出允许有 40 字节（后来改为 80 字节）作为标准交易的⼀部分。所有标识为彩色比特币交易的数据都将包含在该数据字段中。彩色币本身就是比特币，存储和转移的逻辑与比特币⼀致。EPOBC 是彩色币的第⼀个实现。# 彩色币的 OP_RETURN 数据定义[0...4]: [0,67,67,80,0] (that’s “CCP” padded with a zero-byte on both sides)[5...6]: protocol version number (currently 1)[7...8]: reissuance policy (0 for non-reissuable, 65535 for infinitely reissuable from the genesis address)[9..39]: optional (data about the color)优缺点彩色币第⼀次为比特币引入了创新概念并扩大了比特币的用例，使比特币可被用于表示资产的数字化形式、开展去中心化交易以及作为资产管理和点对点交易的创新解决方案，并且仍然保持着透明度、可追踪性、去中心化等多个特性。然而彩色币相对来说具有⼀定程度的复杂性，并且功能有限，无法很好地为现实世界的很多金融活动提供便利。例如，每笔交易都是可见的，可能并不适合所有类型的资产转移。另外，彩色币也面临着发行人不遵守相关义务的风险。对 Colored Coins 的更多思考虽然 OP_RETURN 是⼀个非常直接的用以存储信息至比特币区块链的⼿段，也是⼀个潜在的铭文方式。但是 OP_RETURN 的限制使得其在处理元数据存储时面临⼀些挑战。  OP_RETURN 只能存储非常有限的数据，对于需要存储更大量数据的情况来说，这种限制显然是无法满足的；  OP_RETURN 数据被存储在交易输出部分，虽然这种数据不存储在 UTXO 集中，但是它们占用了区块链的存储空间，导致区块链规模的增加。  使用 OP_RETURN 会导致交易费用的提⾼，因为它需要支付更多的费用来发布这些交易。Colored Coins 的出现说明人们对比特币有着更丰富的需求，但是受限于当时的比特币基础设施不够完善，Colored Coins 并没有取得成功。为了让比特币具有更多的用法，需要提升比特币的效率、灵活性，并且赋予比特币更⾼级的脚本能力。3 比特币的两次升级3.1 SegWit“隔离见证”（Segregated Witness，SegWit）是比特币的一个重要协议升级，由比特币核心开发者 Pieter Wuille 在 2015 年提出，最终在 2017 年的 0.16.0 版本中被正式采纳。在密码学中，术语“见证” (Witness) 用于描述解决密码难题的方案。在比特币语境中，见证是指能够满足对 UTXO 施加的条件并解锁该 UTXO 以供花费的解决方案。“见证”是 “Unlocking Script” 或 “scriptSig” 更一般化的术语。隔离见证，指的是把见证数据（即 scriptSig 中的内容）从交易信息里抽离出来，单独存放。 如图 3 所示。图3 SegWit 区块说明不使用隔离见证时，交易中包含了见证数据 (scriptSig)，也就是说见证数据 (scriptSig) 会影响 txid（txid 是 交易信息进行哈希运算后得到的一个唯一 id）；使用隔离见证后，交易中不再包含见证数据（scriptSig 总是为空），见证数据在另外的字段 (witness) 中（该字段不参与 txid 的计算）SegWit 升级消除了比特币网络中的“交易延展性” (Transaction Malleability) 问题，同时优化了网络和存储，增加了比特币区块中能容纳的交易数量。      交易延展性问题：是指交易在被比特币网络确认之前，txid 可能被攻击者修改导致的问题，其根本原因是比特币交易中的签名数据是与交易的其余部分相关联的，⼀起进行哈希以产⽣交易的整体签名。这使得即使交易中的签名数据被更改，交易的有效性仍然可能保持不变。举例来说，Alice 提交了一个交易，转移一个 BTC 给 Bob，记为 tx1，这个交易的输入中 scriptSig 字段中包含了 Alice 的签名。在交易打包确认前，攻击者稍微调整 scriptSig，（比如在脚本末尾添加 OP_PUSH 和 OP_DROP，或者把签名换个 S 值）这些调整不会影响交易信息的签名验证，但会得到一个新 txid，记为 tx2。现在 tx1 和 tx2 都在等待打包确认状态。如果恰好 tx2 被先打包（这很少出现，但可以伪造更多的 tx3，tx4 来增加被先打包的概率），那么 tx1 在验证时就会失败，因为交易输入中引用的 UXTO 已经被 tx2 花费了。这样，Alice 成功转移了一个 BTC 给 Bob，但 txid 却不是 tx1 了。    那么交易延展性带来的问题为什么不可接受呢？例如，假设 Alice 是交易所，而 Bob 是该交易所的用户。Bob 以前存入了 100 个 BTC，现在向 Alice 提出提币申请，Alice 向 Bob 转移 100 个 BTC，记下 txid 为 tx1，B 利用交易延展性，伪造了另外一个交易 tx2。这时恰好 tx2 被先打包了，tx1 失败。B 对 A 说“我没有收到 100 个 BTC”，A 去查询当时提交的 txid（即 tx1），发现确实 tx1 没有打包上链，然后再发起一个新交易，向 B 转移 100 BTC。这里，其实 A 已经向 B 转移了 200 BTC。        优化网络和存储：见证数据通常在交易的总大小中占了很大比重。更复杂的脚本，如用于多签或支付通道的脚本体积会非常大。在某些情况下，这些脚本会占到交易数据的大多数（超过 75%）。通过将见证数据从交易中移出，隔离见证提高了比特币的可扩展性。节点可以在验证签名后删减见证数据。见证数据不再需要发送到所有节点，也不需要被所有节点存储在磁盘上。同时，SetWit 升级还改变了区块大小的衡量方式，引入了“区块权重” (weight) 的概念，规定了见证数据的权重仅为交易数据权重的 25% 。这实际上意味着比特币区块大小增加了，并且在交易的 Witness 部分中存储数据变得更加便宜。例如，假设有⼀笔传统类型的交易，数据量大小为 200 Bytes。SegWit 升级前，1MB 的区块里面可以放进 5000 笔这样的交易。而⼀笔等效的 SegWit 交易有 120 Bytes 是放在 见证区域的，因此其加权大小为 80 + 0.25 *120 = 110 字节，所以区块可以放入 9090 笔这样的交易，容量几乎翻倍。  此外，SegWit 升级还有引入了两类支付脚本类型：P2WPKH 和 P2WSH 。      P2WPKH (Pay to Witness Public Key Hash)：    P2WPKH 的交易输出脚本是⼀个包含公钥哈希的锁定脚本，公钥哈希是见证程序的哈希值。P2WPKH 的交易输出以 bc1 开头。    前文 Alice 买咖啡的例子中，价值 0.015BTC 的 P2PKH 输出的锁定脚本如下：    OP_DUP OP_HASH160 ab68025513c3dbd2f7b92a94e0581f5d50f654e7 OP_EQUALVERIFY OP_CHECKSIG        而通过隔离见证，Alice 会创建一个“支付给见证公钥哈希” (P2WPKH) 脚本，如下：    0 bc1qw508d6qejxtdg4y5r3zarvary0c5xw7kv8f3t4        显然，隔离见证输出的锁定脚本比 P2PKH 的要简单得多。它包含两个值，第一个数字（0）是一个版本号，第二部分（20 字节）相当于一个锁定脚本，被称为见证程序（witness program）。这 20 字节的见证程序就像是 P2PKH 脚本中的公钥哈希值一样。    主网上 P2WPKH 交易的实例，可参考：ec9f03d79de1b408a2880e77b7be67c149ddb5e89c5b8c5a648fe29f4524d959        P2WSH（Pay to Witness Script Hash）：    P2WSH 对应 P2SH 脚本。P2WSH 的交易输出脚本是⼀个包含脚本哈希的锁定脚本，其中脚本哈希是一个由多个操作码组成的脚本的哈希值。    一个这样的锁定脚本：    OP_HASH160 54c557e07dde5bb6cb791c7a540e0a4796f5e97e EQUAL        使用隔离见证，可以创建一个如下的 P2WSH 输出进行付款：    0 9592d601848d04b172905e0ddb0adde59f1590f1e553ffc81ddc4b0ed927dd73        显然，隔离见证等效脚本要简单得多，省略了 P2SH 脚本中的各种脚本操作符。而且，隔离见证程序仅包含两个推送到堆栈的值：一个见证版本（0），另一个为 32 字节的兑换脚本（Redeem Script）的哈希值。  缺点：因为 SegWit 是⼀个软分叉，许多客户端可能不会升级，因此两种类型的 UTXO 会在网络中同时存在。SegWit 会降低网络的安全性，执行完全验证的节点会大幅减少，因为只有那些升级了的节点才有能力验证交易的 witness 部分。总结来说，SegWit 升级把脚本签名（scriptSig）信息从区块中提取出来，放在⼀个新的数据结构当中。虽然见证数据不是为了数据存储而设计的，但实际上却给了用户⼀个存储其他类型数据内容的机会。3.2 Taproot2021 年 11 月，比特币进行了 Taproot 软分叉升级，在区块 709632 处（2021 年 11 月 12 日）被激活。Taproot 升级由三个不同的比特币改进提案（BIP）组成：BIP 340、BIP 341 和 BIP 342 ，提升了比特币网络的可扩展性、隐私性和灵活性。而 Taproot 升级对后续铭文的影响主要体现在两个方面：更低的费用、更大的空间。      BIP 340 主要引入了 Schnorr 签名方案。相较于比特币使用的椭圆曲线数字签名算法（ECDSA），Schnorr 签名的主要优势在于，多重签名交易在链上呈现为普通的单一签名交易。使用Schnorr签名，多个签名者可以生成一个联合公钥，然后用一个签名共同签名，无需分别在区块链上发布每个公钥和每个签名。这就意味着 Schnorr 签名可实现显著的空间节省和验证时间节省，随着传统多重签名交易上签名者数量的增加，其比较优势变得更大。        BIP 341 包括两个技术升级：MAST（Merklized Abstract Syntax Tree，默克尔抽象语法树）和 P2TR（Pay-to-Taproot）。                  MAST 是比特币协议开发者 Dr. Johnson Lau 在2016年提出的一个概念。MAST 的理念是，交易可以包含多个花费条件，例如一个 2 对 2 的多重签名条件，以及一个时间锁条件。为了避免将所有这些条件和脚本都放入区块链，花费脚本可以被组织在一个 Merkle 树内，由此只需在使用时和必要的Merkle分支哈希一起被揭示即可。例如下图所示，要花费输出，签名者需要做的就是提供一个 Schnorr 多签和 Merkle 树右侧顶部的 Hash(1 &amp; 2)。因此，尽管存在Merkle树，在大多数情况下，只需要一个签名和 32 字节的哈希。        图4 MAST树        然而，这种结构的缺点是，即使在正常的最佳情况下，当Merkle树左上方提供单个密钥和脚本时，仍然需要将另一个哈希发布到区块链（在上图中为Hash(1 &amp; 2)，使用了32字节的数据）。这同时也降低了隐私性，因为第三方可以据此确定是否存在更复杂的花费条件。        图5 Taproot        Taproot 在结构上与 MAST 相似，只是在 Merkle 树的顶部有所不同。在 Taproot 中，用户通过发布公钥签名或满足默克尔树中包含的脚本之⼀来花费，前者称为密钥路径花费（Key Path），后者则是 脚本路径花费（Script Path）。用户可以选择只公布单个公钥和单个签名，而无需公布 Merkle 树的存在。如图 5 所示，左侧调整后的公钥可以从原始公钥和 Merkel 根哈希计算得出。在正常情况下，赎回时不需要在链上保留原始公钥，也不需要揭示 Merkle 树的存在，只需要发布一个单一的签名。与原始 MAST 结构相比，Taproot无需在区块链或脚本本身中包含额外的 32 字节哈希，提高了效率。除此之外，Taproot 使交易看起来很正常，只是一个带有公钥和签名的支付，其他花费条件的存在不会被公布，因此能更好地保护隐私。                    P2TR，即 Taproot Output ，是版本为 1 的隔离见证输出。P2TR 统一了 P2WPKH 和 P2WSH 这两类版本为 0 的隔离见证输出，也就是说其输出 的 scriptPubKey 字段是一样的，更有利于隐私保护。下表 是 P2WPKH/P2WSH/P2TR 的 scriptPubKey 字段及花费它们时的 Witness 字段的总结。        表3 P2WPKH/P2WSH/P2TR对比                                            Type              scriptPubKey（锁定时使用）              Witness（花费时使用）                                                          P2WPKH              0x0014{20-byte-key-hash}              &lt;signature&gt; &lt;pubkey&gt;                                      P2WSH              0x0020{32-byte-hash}              ……                                      P2TR (Key Path)              0x5120{32-byte-tweaked-public-key}              &lt;schnorr-signature&gt;                                      P2TR (Script Path)              0x5120{32-byte-tweaked-public-key}              …… &lt;script&gt; &lt;control-blok&gt;                                      从表中，可以看到创建一个 P2TR (Key Path) 输出时，要比创建 P2WPKH 输出要多占用更大的空间，因为 P2TR (Key Path) 的 scriptPubKey 直接含有 tweaked public key（32 字节），而 P2WPKH 则是公钥哈希（20 字节）。也就是说， 往 P2TR 转账比往 P2WPKH 地址转账要略微贵一点。        不过， 花费 P2TR (Key Path) 比花费 P2WPKH 要省更多的费用， 原因有：                  花费 P2TR (Key Path) 的 Witness 中不再包含公钥；          P2TR (Key Path) 采用的 Schnorr 签名比 P2WPKH 采用的 DER 格式的 ECDSA 签名要更小。                综合考虑创建输出和花费输出两方面，P2TR (Key Path) 比 P2WPKH 更省费用。直观对比 P2WPKH 和 P2TR（Key Path）的 Witness：                              P2WPKH 的 Witness 由“signature”和“pubkey”两个元素组成。            Tx 9d86b83297aaf232446e5ab41b603027fb37ad85f5259b78d6ff6fefe7cada9d 是花费 P2WPKH 的例子，它的 Witness 为：            0070: 02 .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. vin0 Witness Count: 20070: .. 48 .. .. .. .. .. .. .. .. .. .. .. .. .. .. DER vin0 Witness 0 Length:720070: .. .. 30 .. .. .. .. .. .. .. .. .. .. .. .. .. DER start0070: .. .. .. 45 .. .. .. .. .. .. .. .. .. .. .. .. DER length: 690070: .. .. .. .. 02 .. .. .. .. .. .. .. .. .. .. .. DER int0070: .. .. .. .. .. 21 .. .. .. .. .. .. .. .. .. .. DER R length: 330070: .. .. .. .. .. .. 00 f1 62 2d 41 47 f1 85 6c ad0080: c3 12 df 06 6e 5a 19 b3 82 b2 0a 45 d6 a1 6e fb0090: f9 12 b9 4e ac 5e bb .. .. .. .. .. .. .. .. .. DER R: 00f1622d4147f1856cadc312df066e5a19b382b20a45d6a16efbf912b94eac5ebb0090: .. .. .. .. .. .. .. 02 .. .. .. .. .. .. .. .. DER int0090: .. .. .. .. .. .. .. .. 20 .. .. .. .. .. .. .. DER S length: 320090: .. .. .. .. .. .. .. .. .. 36 cf e4 f3 a1 e2 3400a0: 2c 18 5a a0 16 8d be 0e 38 8c 31 99 d6 d0 d7 4d00b0: ed 41 4b c3 78 7e 41 66 9f .. .. .. .. .. .. .. DER S: 36cfe4f3a1e2342c185aa0168dbe0e388c3199d6d0d74ded414bc3787e41669f00b0: .. .. .. .. .. .. .. .. .. 01 .. .. .. .. .. .. DER sighash type byte: 100b0: .. .. .. .. .. .. .. .. .. .. 21 02 cd cf e7 b200c0: fa cd 6a 06 8f a0 fb 0a c5 aa 02 fe e4 09 95 ef00d0: 6a 7b d0 6a 54 d6 c7 58 98 8a d8 fa                                            P2TR（Key Path）的 Witness 只包含一个元素            Tx dbef583962e13e365a2069d451937a6de3c2a86149dc6a4ac0d84ab450509c91 是花费 P2TR (Key Path) 的例子，它的 Witness 为：            0050: .. 01 .. .. .. .. .. .. .. .. .. .. .. .. .. .. vin0 Witness Count: 10050: .. .. 41 .. .. .. .. .. .. .. .. .. .. .. .. .. vin0 Witness 0 Length:65, schnorr_sig (64 bytes) + sig_hash (1 bytes)0050: .. .. .. e6 e1 fe 41 52 4e 65 e3 04 0b c3 d0 80 schnorr_sig0060: a1 36 34 5c 2c 80 6e b7 f3 36 dd 6a 7a 79 e9 050070: 4b 0d 1f c6 a8 d8 36 66 7e f6 e9 f2 18 8c d1 270080: 0a b2 8e 5e 0e b6 42 ea c8 9f 2e c5 0a 32 ca 540090: aa f9 d60090: .. .. .. 01                                     sig_hash: SIGHASH_ALL (0x01)                        *上面例子中 signature 占 65 字节。当 sig_hash 为 SIGHASH_DEFAULT（0x00）时，sig_hash 可以省略，这时 signature 只占 64 字节，比如37777defed8717c581b4c0509329550e344bdc14ac38f71fc050096887e535c8 首个输入就是 signature 只占 64 字节的例子。                                如果在花费 P2TR UTXO 时，Witness 至少包含两个元素，则是 P2TR (Script Path)，即通过 Witness 中元素的个数来决定使用 Key Path（Witness 元素个数为 1）还是 Script Path（Witness 元素个数大于等于 2）。            Tx 905ecdf95a84804b192f4dc221cfed4d77959b81ed66013a7e41a6e61e7ed530 是花费 P2TR (Script Path) 的例子（它是一个 2-of-2 多签脚本），它的 Witness 为：            0070: .. .. .. .. .. .. 04 .. .. .. .. .. .. .. .. .. vin0 Witness Count: 40070: .. .. .. .. .. .. .. 41 23 b1 d4 ff 27 b1 6a f4 vin0 Witness 0 Length:65 (0x41)0080: b0 fc b9 67 2d f6 71 70 1a 1a 7f 5a 6b b7 35 2b0090: 05 1f 46 1e db c6 14 aa 60 68 b3 e5 31 3a 17 4f00a0: 90 f3 d9 5d c4 e0 6f 69 be bd 9c f5 a3 09 8f de00b0: 03 4b 01 e6 9e 8e 78 89 01 .. .. .. .. .. .. ..00b0: .. .. .. .. .. .. .. .. .. 40 0f d4 a0 d3 f3 6a vin0 Witness 1 Length:64 (0x40)00c0: 1f 10 74 cb 15 83 8a 48 f5 72 dc 18 d4 12 d0 f000d0: f0 fc 1e ed a9 fa 48 20 c9 42 ab b7 7e 4d 1a 3c00e0: 2b 99 cc f4 ad 29 d9 18 9e 6e 04 a0 17 fe 61 1700f0: 48 46 44 49 f6 81 bc 38 cf 39 .. .. .. .. .. ..00f0: .. .. .. .. .. .. .. .. .. .. 44 20 fe be 58 3f vin0 Witness 2 Length:68 (0x44)0100: a7 7e 49 08 9f 89 b7 8f a8 c1 16 71 07 15 d6 e40110: 0c c5 f5 a0 75 ef 16 81 55 0d d3 c4 ad 20 d0 fa0120: 46 cb 88 3e 94 0a c3 dc 54 21 f0 5b 03 85 99 720130: 63 9f 51 ed 2e cc bf 3d c5 a6 2e 2e 1b 15 ac ..0130: .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 41 vin0 Witness 3 Length:65 (0x41)0140: c0 2e 44 c9 e4 7e ae b4 bb 31 3a de cd 11 01 2d0150: fa d4 35 cd 72 ce 71 f5 25 32 9f 24 d7 5c 5b 940160: 32 77 4e 14 8e 92 09 ba f3 f1 65 6a 46 98 6d 5f0170: 38 dd f4 e2 09 12 c6 ac 28 f4 8d 6b f7 47 46 9f0180: b1                        这个 Witness 中一共有 4 个元素。Witness 最后一个元素是 control block：            c0                                                               # leaf version and parity bit2e44c9e47eaeb4bb313adecd11012dfad435cd72ce71f525329f24d75c5b9432 # internal key P774e148e9209baf3f1656a46986d5f38ddf4e20912c6ac28f48d6bf747469fb1 # hash e                        Witness 倒数第二个元素是 Script：            20febe583fa77e49089f89b78fa8c116710715d6e40cc5f5a075ef1681550dd3c4ad20d0fa46cb883e940ac3dc5421f05b03859972639f51ed2eccbf3dc5a62e2e1b15ac      # 对应 Script：febe583fa77e49089f89b78fa8c116710715d6e40cc5f5a075ef1681550dd3c4 OP_CHECKSIGVERIFY d0fa46cb883e940ac3dc5421f05b03859972639f51ed2eccbf3dc5a62e2e1b15 OP_CHECKSIG                        Witness 倒数第二个元素之前的所有元素都是“Script 的参数”，在这个 2-of-2 多签的例子中，是两个 Schnorr 签名。            如何校验这个 Witness 是合法的呢？具体规则在 BIP341 中，简单地总结有两点：                          检查 Script 确实在 MAST 上；              脚本 Script 执行完成后，检查栈上留下 True。                        这个 2-of-2 多签例子中，脚本为 &lt;P1&gt; OP_CHECKSIGVERIFY &lt;P2&gt; OP_CHECKSIG，输入参数是 [sig(P2), sig(P1)]，只要签名是正确的，则执行完成后，栈上留下的就是 True。                 Control Block    下面再举例说明一下 Control Block。对于下图所示的 MAST，其中一共有 5 个叶子节点（即 5 个脚本），如果花费这个 P2TR 时，想使用脚本 D，除公开脚本 D 源码外，只用提供 C/E/AB 三个哈希值就可以证明脚本 D 确实在 MAST 上了。我们并不需要公开其它脚本（即 A/B/C/E）的源码。        完整的 Control Block 如下：         &lt;control byte with leaf version and parity bit&gt; &lt;internal key P&gt; &lt;C&gt; &lt;E&gt; &lt;AB&gt;                铭文数据实际上存储在 P2TR（Script Path）的 Witness 中，后文会详细讲解。                                    BIP 342 对比特币的脚本语言进行了修改，使比特币的脚本系统与 BIP 340 和 BIP 341 中的所有更新兼容，能够读取 Schnorr 签名。此外，在升级前，比特币脚本的大小被限制为 10000 字节，可以在比特币代码中看到：参考1 参考2    // Maximum script length in bytesstatic const int MAX_SCRIPT_SIZE = 10000;...    if (script.size() &gt; MAX_SCRIPT_SIZE)        return set_error(serror, SCRIPT_ERR_SCRIPT_SIZE);        BIP 342 中将这一限制移除了，脚本长度仅隐式地受区块权重大小限制，相当于约 4 MB 的大小。  SegWit 与 Taproot 两次升级，让比特币具有了更好的可扩展性和隐私性，同时，升级带来的限制较少、价格低廉的链上数据存储空间也使铭文的诞生成为可能。4 Ordinal Theory2022 年 12 ⽉，Casey Rodarmor 提出了序数理论（Ordinal Theory），包括序数和铭文两个部分。4.1 序数比特币以“聪”（satoshi）为最小计价单位，1 比特币等于 $10^8$ 聪。比特币是同质、可互换的，也就是说比特币之间是没有任何区别的。而序数理论，就是通过一套额外定义的编号方案，对从创世区块开始，网络上所有被创建和转移的“聪”进行编号，赋予每个“聪”一个编号，使其被唯一标识出来，该编号由聪的挖掘顺序确定，在转账过程中，该编号遵循“先进先出”的顺序。由于编号和转移方案都依赖于顺序，因此得名 Ordinal “序数”。以上描述使用了序数理论中的说法，但值得注意的是，实际的比特币实现中，并不存在“聪”这一实体，“聪”仅是比特币最小计价单位的名称，不应当认为是聪“构成”了比特币，或比特币“包含”聪。因此，序数理论及其拥护者使用的“一个聪”、“每个聪”等说法是不准确的，正如我们不会对长度单位用“每个米”、或者对质量单位用“一个克”这样的说法一样。另外，每一笔比特币交易本质上都只是对 UTXO 的销毁与创建，这一过程中并不存在一个不变的“聪”的转移，序数理论仅仅是在其独立于比特币协议的方案层面“规定”了销毁和新建的 UTXO 存在序号上的联系，比特币协议并没有正式承认这种编号方式。序数理论不需要单独的代币、另一个区块链或对比特币的任何更改。它现在就可以工作。比特币总量是 2100 万个，每个比特币包含 $10^8$ 个聪。因此，比特币网络上⼀共有 2100 万亿个聪。序数将这些聪区分出来，为每个聪进行唯⼀编号，编号根据它们被开采的顺序而定。表示  整数符号：例如 2099994106992659 。表示该聪按照挖掘顺序所分配的序号。  ⼗进制符号：例如 3891094.16797 。第⼀个数字表示挖掘该聪的区块⾼度，第⼆个数字表示聪在区块中的偏移量。  度数符号：例如 3°111094′214″16797‴ 。第⼀个数字是周期，从0开始编号；第⼆个数字是减半纪元的区块索引；第三个数字是难度调整期间的区块索引；最后⼀个数字是聪在区块中的偏移量。  百分比符号：例如 99.99971949060254% 。表示该聪在比特币供应量中的位置，以百分比表示。  名称：例如 Satoshi 。使用字符 a 到 z 对序号进行编码的名称。转移假设用户 A 通过挖矿获得了第 100-109 个聪，这 10 个聪存放在同⼀个 id 为 abc123 的 UTXO 中。当 A 要支付给用户 B 5 个聪时，使用 abc123 作为交易输入，其中 5 个聪发送给 B，5 个聪作为找零返回给 A。这两份 “5个聪”都是⼀个整体，分别存放在 id 为 abc456 和 abc789 的 UTXO 中。在上述交易中，聪的流转路径为：      挖矿产⽣ 10 个聪，编号是 [100, 110)，存放在 id 为 abc123 的 UTXO 中。    A 进行转账，10 个聪分成两份，每份 5 个聪。根据序数“先进先出”的原则，聪的编号排序是按照它们在交易输出中的索引决定的。假设输出的顺序先是 A，然后是 B，那么：          A 剩余5个聪的序号是 [100, 105)，存放在 id为 abc456 的 UTXO 中；      B 的 5 个聪的序号是 [105, 110)，存放在 id 为 abc789 的 UTXO 中。            如果后续 A 要再进行转账，比如转账给 C，流程也和上述过程类似。    图6 聪的转账过程    稀有度  聪的稀有度可以根据挖掘顺序来定义。以下是不同聪的稀有程度：表4 聪的稀有度说明            稀有度      含义                  common      除区块第⼀个聪外的任何聪（总供应量为 2100 万亿）              uncommon      每个区块的第⼀个聪（总供应量为 6929999）              rare      每个难度调整期的第⼀个聪（总供应量为3437）              epic      每次减半后的第⼀个聪（总供应量为 32）              legendary      每个周期的第⼀个聪（总供应量为 5）              mythic      创世区块的第⼀个聪（总供应量为 1）      不同稀有度的聪在市场上具有不同的价值，能吸引潜在的收藏者和投资者。4.2 铭文单个聪可以刻有任意内容，创建独特的比特币原⽣的数字文物（Digital Artifact），也称为铭文（Inscription）。铭文可以保存在比特币钱包中并使用比特币交易进行传输，与比特币本身⼀样持久、安全。铭文内容模型类似于网络内容模型。一条铭文由内容类型（也称为 MIME 类型）和内容本身组成，后者是一个字节串。这使得铭文内容可以从Web服务器返回，并用于创建 HTML 铭文，这些铭文可以使用其他铭文的内容。铭文内容完全存储在链上，在 Taproot 脚本路径花费脚本中存储。Taproot 脚本对其内容的限制非常少，而且还可以获得见证折扣，使得铭文内容存储相对经济。创建Taproot 升级中 MAST 的⼀个重要特性是脚本在它被花费之前不会在链上显示。因此，为了让铭文显示在链上，通常需要创建两笔交易：⼀笔 commit 交易用来创建铭文；另⼀笔 reveal 交易花费第⼀笔交易的输出，从而在链上显示脚本信息。具体而言：  ⾸先，在 commit 交易中创建⼀个 commit 到包含铭文内容的脚本的 Taproot 输出；存储的格式是Taproot，即前⼀笔交易的输出是 P2TR。  在 reveal 交易中消费 commit 创建的输出，即通过将那笔铭文对应的 UTXO 作为输入，发起交易。随后，其对应的铭文内容被公开至网络。铭文内容存储在被称为“信封”（Envelope）的结构中，即用OP_FALSE、OP_IF…OP_ENDIF等操作包裹任意数量的铭文数据。由于信封实际上是无操作的，因此不会改变包含脚本的语义，也不会影响正常的交易，且可以与任何其他锁定脚本结合使用。例如，包含字符串“ Hello, world! ”的文本铭文的序列化如下：OP_FALSEOP_IF  OP_PUSH \"ord\"  OP_PUSH 1  OP_PUSH \"text/plain;charset=utf-8\"  OP_PUSH 0  OP_PUSH \"Hello, world!\"OP_ENDIF首先，将字符串 “ord” 入栈，以避免铭文与信封的其他用途的混淆。OP_PUSH 1 表示下一个压栈操作包含内容类型，而 OP_PUSH 0 表示随后的压栈操作包含内容本身。前文提到 Taproot 升级修改了部分脚本资源限制，但升级中保留了脚本元素大小的限制条件，即初始栈和压栈操作符的元素最大为 520 字节：（参考 比特币源码，BIP 342）// Maximum number of bytes pushable to the stackstatic const unsigned int MAX_SCRIPT_ELEMENT_SIZE = 520;因此，对于大型铭文，必须进行多次压栈操作。铭文底层解析            为更加清楚地阐释铭文原理，这里以一个实际的铭文交易为例进行说明：[1c5435cd882e98ec5532a5006a8afe313207f18dd54042219ac57587080a80b8]([Transaction: 1c5435cd882e98ec5532a5006a8afe313207f18dd54042219ac57587080a80b8      Blockchain.com](https://www.blockchain.com/explorer/transactions/btc/1c5435cd882e98ec5532a5006a8afe313207f18dd54042219ac57587080a80b8))      交易的十六进制数据可由此获得：mempool.space (hex)原始数据如下：0100000000010163 a2877c846b01bacb b529de2d2fa45243 2620857e76df70b2 b52dc3b2b2f06300 00000000fdffffff 0110270000000000 00160014311875fda80db86117dc560d 43d3dfc63275e4c8 0340d25a9c030e3e f58f42e0de170681 9ae0c70967508c5a 0106d369553fd4f8 12a8565d644cbee3 ed357a4ab146aadfe02285971e8b4fab 27743f4f85c62a44 8095cc20aa8bd547 3ac0576dfc551b8c 92832b312ad87422 235e82f614c7e66a 752b5627ac006303 6f7264010109696d6167652f706e6700 4c9489504e470d0a 1a0a0000000d4948 4452000000180000 00180806000000e0 773df80000005b49 44415478da636018 05a30008fee3c1941bbeae3b1127a6d4 12b841dbe637d1c4 12828653c5024278 705b400226cf7034 57627339f9166033 8caa160c5824e308 be919a4c7105cff0 0922aa95a658d23d55ea03aaba966616 0000ab897efb3d3f f6f5000000004945 4e44ae4260826821 c1aa8bd5473ac057 6dfc551b8c92832b 312ad87422235e82 f614c7e66a752b562700000000原始数据的解析如下：表5 铭文交易原始数据解析            十六进制数据      类型      含义                  01000000      uint32      版本 1              0001      2  octets      见证标识              01      varint      交易输入数量              输入 1                            63a2877c846b01ba cbb529de2d2fa452 432620857e76df70 b2b52dc3b2b2f063      32 octets      引用交易的哈希              00000000      uint32      先前输出的索引              00      varint      脚本签名长度 (Segwit 长度为0)              fdffffff      4 octets      序列号              01      varint      交易输出数              输出 1                            1027000000000000      int64      以聪为单位的数量（0.0001 BTC）              16      varint      脚本长度 (0x16 = 22)              0014311875fda80d b86117dc560d43d3 ` dfc63275e4c8`      22 octets      锁定脚本              见证数据                            03      varint      见证数              见证部分 1                            40      varint      见证部分长度 (0x40 = 64)              d25a9c030e3ef58f 42e0de1706819ae0 c70967508c5a0106 d369553fd4f812a8565d644cbee3ed35 7a4ab146aadfe022 85971e8b4fab2774 3f4f85c62a448095      64 octets      见证数据，从长度可以看出是签名              见证部分 2                            cc      varint      见证部分长度 (0xcc = 204)              20aa8bd5473ac057 6dfc551b8c92832b 312ad87422235e82  f614c7e66a752b56 27ac0063036f7264  010109696d616765  2f706e67004c9489  504e470d0a1a0a00   00000d4948445200  0000180000001808  06000000e0773df8  0000005b49444154 78da63601805a300   08fee3c1941bbeae  3b1127a6d412b841   dbe637d1c4128286    53c5024278705b40   0226cf7034576273  39f91660338caa16   0c5824e308be919a  4c7105cff00922aa    95a658d23d55ea03   aaba9666160000ab    897efb3d3ff6f500 00000049454e44ae    42608268       204 octets      见证数据，包含铭文实际数据              见证部分 3                            21      varint      见证部分长度 (0x21 = 33)              c1aa8bd5473ac057 6dfc551b8c92832b 312ad87422235e82 f614c7e66a752b56 27      33 octets      见证数据，长度与公钥一致，P2WPKH              00000000      unit32      Lock time 设为 0 表示交易立即传播和执行      该交易中，见证的第二部分包含了铭文的实际数据，铭文相关的操作码如下：OP_IF          63OP_FALSE       00N/A            01-4bOP_PUSHDATA1   4cOP_PUSHDATA2   4dOP_PUSHDATA4   4eOP_ENDIF       68原始数据块长度标识规则：  如果第一个字节小于 4c 直接用其作为长度，  如果第一个字节是 4c 后一字节为长度，  如果第一个字节是 4d 后两个字节为长度，  如果第一个字节是 4e 后四个字节为长度。铭文头/签名 (OP_IF + len + \"ord\" + 0101，可以直接搜索这一序列)：63 03 6f 72 64 01 01Mime 字符串长度：09Mime 文本 (ASCII)：69 6d 61 67 65 2f 70 6e 67OP_FALSE (将 Mime 和数据分开):00数据块长度 (0x94 = 148)：4c 94数据块内容：89 50 4e 47 0d 0a 1a 0a 00 00 00 0d 49 48 44 52 00 00 00 1800 00 00 18 08 06 00 00 00 e0 77 3d f8 00 00 00 5b 49 44 4154 78 da 63 60 18 05 a3 00 08 fe e3 c1 94 1b be ae 3b 11 27a6 d4 12 b8 41 db e6 37 d1 c4 12 82 86 53 c5 02 42 78 70 5b40 02 26 cf 70 34 57 62 73 39 f9 16 60 33 8c aa 16 0c 58 24e3 08 be 91 9a 4c 71 05 cf f0 09 22 aa 95 a6 58 d2 3d 55 ea03 aa ba 96 66 16 00 00 ab 89 7e fb 3d 3f f6 f5 00 00 00 0049 45 4e 44 ae 42 60 82(铭文中可能有多个数据块需要拼接，此例中只有一个)OP_ENDIF (标识铭文结束):68接下来在浏览器控制台中用几行 JS 代码即可还原铭文数据首先创建一个用于将十六进制字符串转换为字节数组的辅助函数：  const toBytes = (hex) =&gt;    Uint8Array.from(hex.match(/.{1,2}/g).map((byte) =&gt; parseInt(byte, 16)));解码 Mime 类型，得到 ` image/png` ：  const mime = new TextDecoder()    .decode(toBytes('696d6167652f706e67'))  console.log(mime) 获取文件字节流数据：  const data = toBytes('89504e470d0a1a0a0000000d4948445200000018000000180806000000e0773df80000005b4944415478da63601805a30008fee3c1941bbeae3b1127a6d412b841dbe637d1c412828653c5024278705b400226cf703457627339f91660338caa160c5824e308be919a4c7105cff00922aa95a658d23d55ea03aaba9666160000ab897efb3d3ff6f50000000049454e44ae426082')最后重建文件并在浏览器中打开：  const file = new File([data], 'myFile', {type: mime})  const url = URL.createObjectURL(file)  open(url)下图就是铭文中铭刻的内容了，是一张 24 x 24，大小 148 字节 的图片：图7 铭文中的图片5 BRC-20BRC-20 代币标准是由匿名开发者 domo（@domodata）于 2023 年 3 月 8 日创建的。该标准借助铭文功能实验性地在比特币上创建同质化代币。“BRC-20”名字借鉴于以太坊 ERC-20 代币标准。BRC-20 代币通过在比特币铭文里存储 JSON 代码数据进行代币记账，目前仅有部署、铸造和转帐三种功能，不具备以太坊上图灵完备的智能合约功能。BRC-20 代币每⼀次部署、铸造和转移都产⽣⼀个新的铭文，因此每次操作都需要 commit 和 reveal 两次交易。{  \"p\": \"brc-20\",     // Protocol: 帮助线下的记账系统识别和处理BRC-20事件  \"op\": \"deploy\",    // Operation: 事件类型 (Deploy, Mint, Transfer)  \"tick\": \"ordi\",    // Ticker: brc-20代币的标识符，⻓度为4个字⺟（可以是emoji）  \"max\": \"21000000\", // Max supply: brc-20代币的最大供应量  \"lim\": \"1000\"      // Mint limit: 每次brc-20代币铸造量的限制}尽管根据其提出者的说法，这仅是一个对可互换性的实验性标准，但 BRC-20 如今已在比特币社区内引起了极大的关注，人们纷纷开始创建自己的 BRC-20 代币。同时，像钱包服务和市场等序数基础设施提供商，已经开始集成 BRC-20，以使其用户能够铸造和交换 BRC-20 代币。目前比较火热的 BRC-20 代币项目有：      $ORDI：比特币上的⾸个 BRC-20 代币，利用 Ordinals 协议在聪上刻录信息。ORDI 拥有 2100 万枚硬币的限量供应，为 NFT 和代币引入了创新的可能性，根据挖掘和转移顺序分配序数。        $SATS：将比特币区块链上的最小单位聪转化为⼀个可交易和可收藏的代币，总量与比特币相同，每个 SATS都有⼀个唯⼀的序号。SATS 是 BRC-20 目前拥有持币地址最多的项目。        $MUBI：MultiBit 是首个为 BRC-20 与 ERC-20 代币间的跨网络转移的双向跨链桥。通过促进这些代币更便捷的流动性，MultiBit 加强了跨链互操作性，目的是以安全和用户友好的方式，促进 BRC 和 ERC 代币的流动性和可访问性。  局限性虽然 BRC-20 代币被称作同质化代币，但由于其基于铭文实现，只能以设定的增量进行交易，因此实质上更接近半同质化代币。用户需要创建铸币 JSON 铭文来定义要铸造数量，然后通过交易手续费拍卖与其他人竞争优先权，以最终完成铸币。为了在比特币网络上以原生方式交易 BRC-20 代币，卖方需要创建转账铭文，将原始铸币铭文分割成更小的块，以便能出售预定义批次的代币。如果想购买特定数量的 BRC-20 代币，买方必须找到愿意出售该数量代币的卖方。另外，要确定钱包中 BRC-20 代币的余额，仅依靠比特币全节点是不够的，用户还需要自己运行或信任第三方的链下索引器，由索引器通过规则集解析铭文。需要注意的是，这种中心化的索引方式具有极高的共识风险，若不同交易平台或交易双方使用不同的索引器，那么 BRC-20 代币的所有权可能产生分歧。相较于 ERC-20其实 BRC-20 与 ERC-20 之间的可比性基本仅限于名称。ERC-20 是以太坊上基于智能合约的同质化代币标准，而 BRC-20 是在比特币上基于序数铭文创建半同质化代币的一种方式。由于比特币有意限制了可编程性，目前 BRC-20 代币基本没有实际功能，比 ERC-20 简单、局限性更多。同时 BRC-20 代币的流动性也更差。影响序数、铭文、BRC-20 等新叙事一方面为比特币吸引了注意力，使比特币网络活跃程度明显上升，另一方面则加重了网络拥堵，交易手续费大幅提升。以下是几个关键指标：      内存池    2023 年 2 月 至 2024 年 1 月间，比特币内存池容量显著增长，目前已接近 600 MvB。    图8 2023.2-2024.1 比特币内存池容量变化    目前，交易的阈值费率已经来到了 22.3 sat/vB，也就是说手续费低于该费率标准的交易将被内存池满的比特币节点直接拒绝。当前内存占用达 1.62GB，远高于 Bitcoin Core 默认分配的 300 MB 内存，未确认的交易多达 27 万笔。    图9 交易阈值费率/内存占用/未确认交易数        平均手续费    由下图可以看出，比特币交易平均手续费明显上升，峰值超过 30 美金，较 2023 年初提升超 30 倍。        图10 比特币交易平均手续费变化                  搜索量        2023 年全年中“Bitcoin”的谷歌搜索量显著高于“Ethereum”、“Coinbase”、“NFT”等词条，且呈现明显的上升趋势。侧面反映了铭文和 BRC-20 为比特币生态吸引了更多关注。                      图11 \"Bitcoin\"词条的谷歌搜索量变化  值得一提的是，比特币社区中对序数铭文、BRC-20 等新叙事看法仍不统一，这也反映在了对上述指标的解读上：      铭文、BRC-20的拥护者认为，序数和铭文能够拓展比特币的应用场景，吸引更多的开发者和用户，同时提升比特币网络的活跃度。他们倾向于正面解读交易手续费的大幅上涨，认为随着交易量增加和交易费用上升，比特币矿⼯的收益更能得到保障，从而吸引更多算力加入比特币网络，使得比特币网络更加安全、健壮和去中心化。这方认为上涨的手续费是比特币网络不可或缺的“安全预算”。另外，他们提出可以通过建设闪电网络等二层扩展方案来解决交易手续费高企的问题。        而比特币原教旨主义者、铭文和 BRC-20 的反对者认为这类应用是对 Taproot 升级的滥用，随着类似的应用的增多，比特币网络可能被阻塞，影响比特币的转账甚至出块，带来“劣币驱逐良币”的效应，违背比特币最初成为“点对点电子现金”、数字硬通货的初衷，即便二层扩展能够一定程度缓解手续费的问题，也不能成为毫无价值的 BRC-20 挤占算力资源的理由。比特币常被认为是“数字黄金”，而这方认为“金子就该是纯粹的，不应掺入其他杂质”。比特币核心开发者 Luke Dash Jr.（@LukeDashjr）称铭文是在利用 Bitcoin Core 的漏洞，并提到“这个漏洞最近在 Bitcoin Knots v25.1 中得到了修复， Bitcoin Core 在即将发布的 v26 版本中仍然存在漏洞。我只能希望它将在明年的 v27 版本之前最终得到修复。”    PSA: “Inscriptions” are exploiting a vulnerability in #Bitcoin Core to spam the blockchain. Bitcoin Core has, since 2013, allowed users to set a limit on the size of extra data in transactions they relay or mine (`-datacarriersize`). By obfuscating their data as program code,…&mdash; Luke Dashjr (@LukeDashjr) December 6, 2023      6 总结本文深入最底层，详细介绍了比特币序数理论、铭文和 BRC-20 代币相关的技术知识。以 Colored Coin 为先驱，经过 SegWit 和 Taproot 两次升级，比特币交易拥有了更大的存储空间、能支持更复杂的脚本。随之而来的序数理论，提出了对“聪”进行排序标记的方法，而铭文则基于此定义了一套编码规则将网络媒体内容存储在链上。BRC-20 标准基于序数理论和铭文，通过将账本铭刻在聪上实现了在比特币上发行同质化代币。当前，比特币社区中对于铭文、BRC-20 价值的讨论仍莫衷一是，这一系列新叙事究竟是颠覆比特币生态的革新者，还是昙花一现的投机客，我们仍需拭目以待。不过这样的讨论本身，倒是很好地彰显了去中心化的 Web3 世界所独有的魅力。参考[1] BIP 340[2] BIP 341[3] BIP 342[4] Ordinal Theory[5] BRC-20 docs"
  },
  
  {
    "title": "账户抽象架构设计详解",
    "url": "/posts/aa/",
    "categories": "Blockchain, Ethereum",
    "tags": "account abstraction",
    "date": "2023-12-17 12:00:00 +0800",
    





    
    "snippet": "本文详细介绍了以太坊 ERC-4337 中账户抽象的设计思路，主要面向具备智能合约基础知识，但对账户抽象不甚了解的读者。文中描述的 API 和行为可能与最终版的 ERC-4337 存在部分差异。自定义钱包我们从一个资产管理的具体场景出发：用户希望对普通交易使用单一私钥签名，但对于价格昂贵的 NFT 资产，则要求额外使用存放在银行保险库中的另一把私钥才能完成转移。在以太坊中，账户分为两类：合约...",
    "content": "本文详细介绍了以太坊 ERC-4337 中账户抽象的设计思路，主要面向具备智能合约基础知识，但对账户抽象不甚了解的读者。文中描述的 API 和行为可能与最终版的 ERC-4337 存在部分差异。自定义钱包我们从一个资产管理的具体场景出发：用户希望对普通交易使用单一私钥签名，但对于价格昂贵的 NFT 资产，则要求额外使用存放在银行保险库中的另一把私钥才能完成转移。在以太坊中，账户分为两类：合约账户 (CA) 和外部账户 (EOA)。EOA 由公私钥对控制，属于“主动”类型，能够发起交易并支付 EVM 执行所需的 gas 费，但仅限执行基础操作，如转账或与合约交互。相比之下，CA 由智能合约的代码逻辑控制，是“被动”类型，只能响应 EOA 发起的交易，无法支付 gas 费，但具有可编程性，可根据存储的代码执行复杂逻辑。那么在我们假设的场景中，资产持有账户应该选择合约账户还是外部账户？答案是必须选择合约账户。若使用外部账户，所有资产都可能被私钥签名的交易任意转移，这显然无法满足我们既定的安全需求。因此，我们的链上身份将由一个智能合约代表，这类合约被称为“智能合约钱包”。我们需要设计一种机制，能够向该钱包发出指令，使其像 EOA 一样执行我们期望的转账或合约调用操作。  这种资产管理方式要求每个用户都必须拥有独立的智能合约。不能使用单一大型合约来集中管理多个用户的资产，因为以太坊生态的基本假设是一个地址对应一个独立实体。共享合约钱包将导致用户无法被有效区分，有违区块链追求的身份透明性和独立性原则。用户操作我们部署一个用来持有资产的智能合约，并提供一个方法，通过该方法可以向合约传递用户希望执行的操作的信息，我们称其为用户操作 (User Operation)。钱包合约如下：contract Wallet {  function executeOp(UserOperation op);}用户操作中具体包含哪些信息呢？首先是传给 eth_sendTransaction 的参数：struct UserOperation {  address to;  bytes data;  uint256 value;  uint256 gas;  // ...}此外，还需要提供用来授权请求的数据，钱包会根据这段数据来决定是否执行操作。在我们的场景中，对于大部分用户操作，只需要传递主密钥对其余操作数据的签名即可。但如果用户操作是转移 NFT，那么钱包将要求用户提供两个私钥分别对操作数据的签名。最后，还要加入一个随机数 (nonce) 以防止重放攻击：struct UserOperation {  // ...  bytes signature;  uint256 nonce;}至此，钱包合约达成了我们的目标：用户的 NFT 由该合约持有时，没有两个私钥签名就无法被转移。合约钱包调用还有一个问题是如何调用 executeOp(op)。由于没有用户的私钥签名合约不会执行任何操作，因此所有人都可以尝试进行调用，不会有安全风险。但要想执行操作，就一定得有人实际调用该方法。在以太坊上，只有 EOA 能发起交易，进行调用的 EOA 必须使用自己的 ETH 支付 gas 费。我们可以单独设置一个 EOA，仅用于调用钱包合约。虽然这个 EOA 没有钱包合约那样的双重签名保护，但它只需要持有足够支付调用 gas 费的 ETH 即可，大部分资产仍由更安全的钱包合约持有。用户使用单独的 EOA 调用智能合约钱包由此，我们只用一个相当简单的合约就实现了大部分账户抽象的功能。无需 EOA上述方案的缺点是用户需要运行一个单独的 EOA 来调用钱包合约，增加了流程的复杂性。对于希望自行支付 gas 费但不想维护两个账户的用户，该如何解决？前面提到钱包合约的 executeOp 方法可以被任何人调用，因此用户可以请求其他拥有 EOA 的人代为调用。我们暂时称这个 EOA 及其所有者为“执行器” (executor)。由于执行器需要承担调用产生的 gas 费用，我们可以设计一种机制：让钱包合约预留部分 ETH，并在调用过程中向执行器转账，作为 gas 费的补偿。  “执行器”并不是 ERC-4337 中的术语，但很好地描述了这个参与者的作用。后文会将其替换为 ERC-4337 中实际使用的术语“打包器” (bundler)，在尚未涉及打包操作的当前阶段，“执行器”这一表述更为恰当。值得注意的是，其他协议中也常用“中继器” (relayer) 来指代类似角色。当前钱包的接口是：contract Wallet {  function executeOp(UserOperation op);}我们的目标是调整 executeOp 的行为，使其能在完成操作后，根据实际消耗的 gas 量向执行器支付相应的 ETH 补偿。由执行器而非用户的 EOA 调用智能合约钱包这一方案面临的核心挑战是确保执行器能获得 gas 费补偿。如果执行器调用 executeOp 后钱包未退还费用，执行器将不得不自行承担开支。为避免这种风险，执行器可以先通过 debug_traceCall 等方式在本地模拟 executeOp 操作，验证是否能获得 gas 费补偿。只有确认补偿可期，执行器才会提交实际交易。然而，模拟无法保证完美预测。实际执行中完全可能出现模拟时钱包似乎支付了 gas 费，但在区块打包时却未能成功的情况。一些恶意钱包甚至可能故意为之，利用这一漏洞免费执行操作，同时将巨额 gas 费用转嫁给执行器。模拟与真实执行不一致的根本原因主要有：  操作可能涉及存储读取，而存储状态在模拟与实际执行间可能发生变化。  操作可能调用依赖环境的操作码，如 TIMESTAMP、BLOCKHASH、BASEFEE 等，这些信息在不同区块间本质上是不可预测的。执行器可以尝试通过限制操作范围来应对，比如拒绝使用“环境”操作码的操作。但这种做法过于武断。我们的初衷是使钱包具备与外部账户相当的操作能力，过度限制将阻碍诸如与 Uniswap 等 DApp 的正常交互，这显然背离了原始设计意图。由于 executeOp 本质上是一个可以包含任意代码的黑箱，因此我们无法合理地防止模拟欺骗，这个问题在现有接口下是无解的。入口点合约前述问题的本质是在于执行不可信合约代码的安全性：执行器希望在一个可控、有保障的环境中处理这些具有潜在风险的操作——这不正是智能合约的目的吗？为此，我们引入一个（经过严格审计和源代码验证的）可信合约，称为入口点 (EntryPoint)，并为其设计了一个由执行器调用的方法：contract EntryPoint {    function handleOp(UserOperation op); // ...}handleOp 方法将执行以下流程：  验证钱包是否具备足够资金支付潜在的最大 gas 费（基于用户操作中的 gas 字段）。若资金不足，直接拒绝执行。  以适当的 gas 限额调用钱包的 executeOp 方法，并精确记录实际消耗的 gas 量。  从钱包中提取一定量的 ETH 作为执行器的 gas 费补偿。为确保第三点能够顺利实施，入口点需要直接持有用于支付 gas 费的 ETH。考虑到我们无法保证能够从钱包中可靠地提取资金，我们还需要额外的方法，使钱包能向入口点存入 gas 费资金，并在需要时将资金取出：contract EntryPoint {    // ...    function deposit(address wallet) payable;    function withdrawTo(address payable destination); }引入经审计和源代码验证的入口点合约，可确保执行器获得补偿通过这种设计，我们成功解决了执行器获取 gas 费补偿的问题。然而，随之而来的是钱包需要面对的新挑战。拆分验证与执行我们之前将钱包的接口定义为：contract Wallet {     function executeOp(UserOperation op); }这个单一方法实际上同时承担了两个关键功能：验证用户操作的授权性，以及执行操作中指定的具体调用。在钱包所有者自行支付 gas 费时，这种设计没有问题；但随着引入执行器支付 gas 费的模式，区分验证和执行变得尤为重要。当前实现下，钱包无条件地向执行器退还 gas 费。然而，在验证失败的情况下，钱包本不应承担任何费用。验证失败意味着未经授权的参与者试图强制钱包执行操作。在这种场景中，尽管 executeOp 会正确阻止操作，但根据现有实现，钱包仍需支付 gas 费。攻击者可以利用这一缺陷，发起大量未经授权的操作，逐步耗尽钱包的 gas 资金。相反，如果验证成功但操作执行失败，钱包则应支付 gas 费。这类似于从外部账户发送一个最终回滚的交易：由于操作是经过所有者授权的，因此产生的 gas 费应由钱包承担。鉴于当前 executeOp 接口无法有效区分验证失败和执行失败，我们需要对接口进行重构。新钱包接口设计如下：contract Wallet {    function validateOp(UserOperation op);    function executeOp(UserOperation op);}相应地，入口点的 handleOp 方法将实现更复杂的逻辑：  首先调用 validateOp。如果验证未通过，立即终止执行。  从钱包的存款中预留 ETH，用于支付可能的最大 gas 费（基于操作的 gas 字段）。若钱包存款不足，拒绝执行。  调用 executeOp 并精确跟踪实际消耗的 gas 量。无论操作执行成功与否，都从预留资金中向执行器退还 gas 费，并将剩余资金退还至钱包存款。拆分验证与执行以区别验证失败和执行失败这一设计对钱包而言颇为合理，确保仅对授权操作收取 gas 费。但对执行器来说，风险反而有所增加。  为防止未经授权的用户直接调用 executeOp，钱包可通过限制该方法仅允许入口点合约调用来维护安全性。  一个潜在的风险是作恶钱包可能试图在 validateOp 中执行全部操作，以避免在执行失败时被收取 gas 费。后文将阐明 validateOp 会受到严格限制，使其不适合执行实质性操作。模拟机制在当前机制下，当未授权用户提交操作时，validateOp 将失败，钱包无需支付任何费用。然而，执行器仍需为 validateOp 的链上执行支付 gas 费，且无法获得补偿。尽管作恶钱包无法再免费执行操作，但仍可令执行器为失败操作支付 gas 费并遭受损失。先前，执行器通过本地模拟预测操作成功性，仅在模拟成功时提交链上交易调用 handleOp。但执行器难以合理限制执行，防止模拟成功而实际交易失败的情况。现在，通过解耦验证和执行，执行器只需模拟 validateOp 即可判断是否能获得补偿。与 executeOp 不同，对 validateOp 施加严格的限制不会阻碍钱包与区块链的交互。具体而言，执行器仅在 validateOp 满足以下条件时才提交用户操作：  不使用受限操作码，如 TIMESTAMP、BLOCKHASH 等。  仅访问钱包的关联存储，包括：          钱包自身存储      另一个合约中以钱包地址为键的映射存储位置 mapping(address =&gt; value)      另一个合约中与钱包地址对应的特定存储位置      这些规则旨在最大程度地减少模拟与实际执行不一致的风险。禁用特定操作码是直接的限制，而存储访问限制的核心思路是：任何存储访问都可能导致模拟失真。通过将存储访问限制在与钱包相关的位置，恶意者若要使模拟失真，需要更新特定的关联存储，其成本足以遏制恶意行为。这种模拟机制下，钱包和执行器都是安全的。  对 validateOp 存储访问的限制还有一项额外好处：由于 validateOp 仅能访问特定钱包的关联存储，不同钱包的存储位置相互独立，这意味着针对不同钱包的 validateOp 调用不会相互干扰。这种互不影响的特性为后续讨论的“打包”操作提供了重要的并行处理基础。钱包直接支付目前，钱包通过将 ETH 存入入口点合约来支付 gas 费，但普通的外部账户是直接使用其 ETH 余额支付的。我们的合约钱包同样应该支持这种方式。在拆分验证和执行后，入口点可以在验证阶段要求钱包向其发送 ETH，否则拒绝执行操作，从而实现钱包直接支付 gas 费。我们需要更新钱包的 validateOp 方法，使入口点能够向其索要资金。如果 validateOp 未向入口点支付所需金额，入口点将拒绝执行操作：contract Wallet {    function validateOp(UserOperation op, uint256 requiredPayment);     function executeOp(UserOperation op);}由于验证阶段无法精确预知执行期间的确切 gas 消耗，入口点将根据用户操作的 gas 字段，要求钱包支付可能使用的最大 gas 费。执行结束后，未使用的 gas 费将退还给钱包。在智能合约开发中，直接向任意合约发送 ETH 存在风险，这会调用该合约的代码，可能触发不可预测的代码执行、gas 消耗，甚至引发重入攻击。因此，我们采用“拉取支付”模式：多余的 gas 费将保留在入口点，由钱包主动通过提款方法取出。多余的 gas 费将通过 deposit 方法接收，钱包可使用 withdrawTo 方法提取。这意味着钱包可以从两个来源支付 gas 费：入口点合约中为其所持有的 ETH，以及钱包自身持有的 ETH，类似于支付宝余额与所绑定银行账户余额间的关系。入口点将优先使用已存入的 ETH 支付 gas 费，若存款不足，则在调用钱包的 validateOp 时要求支付剩余部分。执行器激励迄今为止，执行器需要进行大量模拟，却没有任何收益，且在模拟失真时还要自行承担 gas 费。为此，我们允许钱包所有者在用户操作中附加小费，作为执行器的补偿。在用户操作结构中新增字段：struct UserOperation {    // ...    uint256 maxPriorityFeePerGas; }maxPriorityFeePerGas 代表发送方为获得操作处理优先权愿意支付的最高 gas 价格。执行器在提交调用入口点 handleOp 方法的交易时，可以使用较低的 maxPriorityFeePerGas，将差价作为自身收益。单例入口点入口点的功能与特定钱包或执行器无关，因此可作为整个生态系统的单例存在。所有钱包和执行器都将与同一个入口点合约进行交互。为此，我们需要调整用户操作的结构，使其能指定操作针对的钱包地址，便于入口点的 handleOp 方法确定需要验证和执行的钱包：struct UserOperation {  // ...  address sender;}小结我们的目标是创建一个能够自主支付 gas 费、无需钱包所有者管理独立外部账户的链上钱包。这一目标现已实现：钱包接口设计如下：contract Wallet {  function validateOp(UserOperation op, uint256 requiredPayment);  function executeOp(UserOperation op);}区块链通用的单例入口点接口如下：contract EntryPoint {  function handleOp(UserOperation op);  function deposit(address wallet) payable;  function withdrawTo(address destination);}用户使用流程简述：  钱包所有者生成用户操作，并在链下寻求执行器处理。  执行器模拟钱包的 validateOp 方法，判断是否接受该操作。  若接受，执行器向入口点合约发送交易，调用 handleOp。  入口点在链上处理验证和执行，并从钱包存款中向执行器退款。打包操作当前模式下，执行器每笔交易仅处理单一用户操作，效率低下。得益于入口点的通用性，我们可以聚合来自不同用户的多个操作，在单笔交易中一次性执行，从而节省 gas 费——这就是打包操作 (bundling)。通过打包用户操作，可以避免重复支付固定的 21000 gas 交易发送费用，并降低冷存储访问成本（同一交易中多次访问相同存储空间的费用比首次访问便宜）。实现改动极为简单，只需将：contract EntryPoint {  function handleOp(UserOperation op);    // ...}替换为：contract EntryPoint {  function handleOps(UserOperation[] ops);  // ...}用户操作的打包、验证和执行handleOps 方法将执行以下逻辑：  对每个操作，在其发送者钱包上调用 validateOp。未通过验证的操作将被丢弃。  对每个操作，调用发送者钱包的 executeOp，跟踪 gas 消耗，并向执行器转账以支付 gas 费。关键是入口点将先验证所有操作，随后再执行所有操作，而非逐一验证和执行。这一设计对模拟至关重要：若在验证下一个操作前已执行前一个操作，则前一操作的执行可能干扰后续操作验证所依赖的存储状态。同样，也需要避免一个操作的验证干扰包中后续操作的验证。所幸，只要包中不包含同一钱包的多个操作，那么依靠前述存储限制，这一点很容易实现：两个操作的验证不涉及相同存储，就不会相互干扰。因此，执行器将确保每个钱包在包中最多只有一个操作。在此机制下，执行器可以通过调整包中用户操作的顺序（可能插入自身操作）来获得最大可提取价值 (MEV)。引入打包操作后，我们将使用 ERC-4337 中的术语“打包器” (bundler) 来指代控制外部账户的参与者，不再称其为“执行器”。网络参与在当前机制下，钱包所有者向打包器提交用户操作，并期望该操作能被纳入一个数据包中。这一机制与区块链上常规交易极其相似：账户所有者向区块构建者提交交易，并希望其被收录在特定区块中。我们可以从两者近似的网络结构中获得启发。类比于节点将普通交易存储在内存池并广播给其他节点，打包器同样可以将经过验证的用户操作储存在内存池中并传播至其他打包器。在共享用户操作前进行验证，不仅可以减少重复工作，还能提高整体网络效率。若打包器兼任区块构建者，则能精准选择操作被包含的区块，大幅降低甚至消除模拟成功后执行失败的可能性。更有趣的是，区块构建者和打包器能以相似方式通过 MEV 获利，随着技术演进，这两个角色很可能最终融为一体。代付合约目前，我们的钱包已全面实现 EOA 功能，并支持用户自定义验证逻辑。不过钱包仍需支付 gas 费，这意味着用户在进行链上操作前必须先准备一定数量的 ETH。然而在特定场景中，我们可能希望绕开钱包所有者，由他人代为支付 gas 费，例如：  钱包所有者可能是区块链新用户，在进行链上操作之前获取 ETH 门槛较高。  Dapp 可能愿意为其服务分担 gas 费用，降低用户进入的心理障碍。  赞助商或许会允许用户使用 USDC 等替代代币支付 gas 费。  在追求隐私的场景下，用户可能希望从混币器提取资产至新地址，并由无关账户代为支付 gas 费。尽管 Dapp 可能有意为用户分摊成本，但显然无法为所有用户的每一笔操作买单。因此，我们需要在链上部署具有自定义逻辑的合约，用以审核用户操作并决定是否代为支付费用。我们将这类合约称为代付者（Paymaster）。该合约提供了一个方法用于审查用户操作并判断是否代为承担费用：contract Paymaster {  function validatePaymasterOp(UserOperation op);}当钱包提交操作时，需要明确指出期望由哪个代付者（如有）支付 gas 费。为此，我们在 UserOperation 中新增字段，标识代付者地址，并允许钱包向代付合约传递相关的数据，以说服其买单。struct UserOperation {  // ...  address paymaster;  bytes paymasterData;}接下来，我们将调整入口点的 handleOps 方法，以无缝整合新的代付合约机制。其核心流程如下：  针对每笔操作：          在操作发送者的钱包上执行 validateOp。      若操作指定了代付合约地址，则调用其 validatePaymasterOp。      一旦以上任一验证未通过，立即丢弃该操作。      在操作发送者钱包上调用 executeOp，精确追踪 gas 使用量，并向执行器转账 ETH 以结算 gas 费。若存在代付者，则使用代付者的 ETH；反之，则仍以钱包自有资金支付。      执行器同时调用代付合约和用户的合约钱包来确定交易是否被赞助代付合约也需要像钱包一样先通过入口点的 deposit 方法存入 ETH，然后才能支付操作费用。代付者质押在之前的讨论中，我们提到打包器需要通过模拟来规避执行未通过验证的操作，因为这种情况下打包器不仅要为 gas 费买单，还无法获得钱包的补偿。引入代付者后，情况类似：打包器同样需要避免提交未通过代付者验证的操作。初看之下，我们或许可以对 validatePaymasterOp 采用与 validateOp 相同的限制（仅允许访问钱包及其关联存储，并禁止使用特定操作码），打包器可以在模拟钱包 validateOp 的同时，一并模拟用户操作的 validatePaymasterOp。然而，这里潜藏着一个微妙的问题：先前对钱包存储的限制确保了不同钱包的操作验证相互隔离。但代付合约的存储却被同一数据包中所有使用该代付者的操作共享，这意味着一个 validatePaymasterOp 的行为可能导致包中其他使用相同代付合约的操作验证失败。更为严重的是，恶意代付者甚至可以利用这一特性发起 DoS 攻击。为应对这一挑战，我们需要引入一个信誉系统：让打包器追踪每个代付者最近验证失败的频率，并通过限制或禁止使用该代付者的操作来惩罚频繁失败的代付者。然而，如果恶意代付者能轻易创建多个实例（即女巫攻击），这一信誉系统将形同虚设。因此，我们需要引入 ETH 质押机制。在入口点中新增处理质押的方法：contract EntryPoint {    // ...    function addStake() payable;    function unlockStake();     function withdrawStake(address payable destination);}质押存入后，只有在调用 unlockStake 并等待一定延迟后，才能提取。这些方法区别于先前讨论的 deposit 和 withdrawTo，后者可随时取出。  此处的质押不会被罚没。其目的是迫使潜在攻击者锁定大量资金，从而提高发起大规模攻击的门槛。postOp 方法目前，代付合约仅在操作实际运行前的验证阶段被调用。然而，代付者可能需要根据操作结果做出差异化处理。举例而言，一个支持使用 USDC 支付 gas 费的代付者需要准确了解操作实际消耗的 gas 量，以精确收取 USDC。因此，我们为代付合约新增 postOp 方法，入口点将在操作完成后调用该方法，传递实际使用的 gas 量。同时，为了让代付者能在 postOp 中利用验证阶段的结果数据，我们允许验证返回任意 context 数据：contract Paymaster {    function validatePaymasterOp(UserOperation op) returns (bytes context);     function postOp(bytes context, uint256 actualGasCost);}以上述场景为例，代付合约在批准执行前会检查用户是否有足够 USDC 支付操作费用。然而，操作执行过程中完全有可能转走钱包的所有 USDC，导致代付者最终无法获得付款。  是否可以通过预先收取最大 USDC 金额，随后退还未使用部分来规避这一问题？理论上可行，但实践中颇为复杂，因为这需要两次 transfer 调用，不仅增加 gas 成本，还会产生两个不同的 transfer 事件。代付合约需要一种机制，既能使执行完成的操作失效，又确保代付者仍能获得应得的费用。我们的解决方案是允许入口点调用两次 postOp。入口点将 postOp 调用作为执行钱包 executeOp 的一部分，这意味着 postOp 的回滚也将导致 executeOp 的结果回滚。在这种情况下，入口点将再次调用 postOp，此时 executeOp 尚未执行，代付者可以获取应得的费用。为了提供更多上下文，我们为 postOp 添加 hasAlreadyReverted 标志参数，指示当前是否处于回滚后的第二次运行：contract Paymaster {    function validatePaymasterOp(UserOperation op) returns (bytes context);    function postOp(bool hasAlreadyReverted, bytes context, uint256 actualGasCost);}小结为了支持 gas 费代付，我们引入了一种新的实体类型：代付者，即部署了具有以下接口的智能合约：为了支持 gas 费代付机制，我们引入了一种全新的实体类型：代付者，本质上是部署了以下接口的智能合约：contract Paymaster {    function validatePaymasterOp(UserOperation op) returns (bytes context);    function postOp(bool hasAlreadyReverted, bytes context, uint256 actualGasCost);}用户操作新增两个关键字段，赋予钱包指定代付者的能力：struct UserOperation {    // ...    address paymaster;    bytes paymasterData; }代付者可以通过与钱包相同的方式，将 ETH 存入入口点合约。入口点随之更新 handleOps 方法：除了通过钱包的 validateOp 进行验证外，还会调用代付合约的 validatePaymasterOp 对操作进行验证。验证通过后执行操作，最后调用代付合约的 postOp。为妥善解决代付者验证模拟中的潜在问题，我们引入了质押系统，用于锁定代付者的 ETH。新增的入口点合约方法如下：contract EntryPoint {    // ...    function addStake() payable;    function unlockStake();    function withdrawStake(address payable destination);}引入代付者后，我们已经实现了账户抽象的绝大部分核心功能。钱包创建一个始终悬而未决的问题是：用户究竟如何创建其钱包合约？传统的合约部署需要使用 EOA 发送一笔没有接收者的交易，并附带合约部署代码。然而，我们的初衷恰恰是让用户无需 EOA 就能与区块链交互。如果用户必须先创建 EOA 才能部署钱包合约，那么这套设计就失去了其根本意义。我们的理想场景是：尚未拥有钱包的用户能够创建链上钱包并支付 gas 费，可以选择自行使用 ETH 支付，或通过代付者代为支付。最关键的是，用户无需创建 EOA 就能完成这一过程。另外，用户在创建普通 EOA 时只需在本地生成私钥并声明账户即可，无需发送任何交易。我们希望合约钱包能具备同样的灵活性：在实际部署前就能确定地址并接收资产。确定性合约地址简而言之，我们需要在实际部署合约前就确定其最终地址。  尚未部署但已预先确定的地址，被称为反事实地址 (counterfactual address)。实现这一目标的关键在于 CREATE2 操作码。它能根据以下三个输入，在不实际部署合约的情况下确定合约地址：  调用 CREATE2 的合约地址  任意的 32 字节盐值  合约的 init codeinit code 是一段特殊的 EVM 字节码，其执行会返回另一段将被部署的智能合约代码。值得注意的是，即便多次使用相同的 init code，也不保证最终部署的合约代码完全相同，因为 init code 可以读取存储或使用 TIMESTAMP 等操作码。借助 CREATE2，我们允许用户提供 init code，并由入口点在合约尚未存在时进行部署。为此，我们在用户操作中新增字段：struct UserOperation {    // ...    bytes initCode;}随后更新入口点 handleOps 方法的验证逻辑：  若操作包含非空 initCode，则使用 CREATE2 部署对应合约。  继续执行标准验证流程：          调用新创建钱包的 validateOp 方法      若存在代付者，调用代付合约的 validatePaymasterOp 方法      这一方案实现了我们的核心目标：用户可以部署任意合约，并提前知晓部署地址。部署可由代付者赞助，或用户自行支付（将 ETH 存入待部署合约地址）。然而，这种方案存在潜在风险：  代付者难以通过分析字节码判断是否代付  用户提交的部署代码难以全面验证，存在用户借助第三方工具部署合约时可能引入后门的风险  init code 可能导致打包器模拟成功但实际执行失败我们需要设计一种更安全的机制，让用户能在不提交任意字节码的情况下部署合约，为所有参与方提供更充分的保障。工厂合约我们不再直接使用任意字节码并调用 CREATE2，而是引入了“工厂” (factory) 合约，允许用户选择专门用于创建不同类型钱包的合约。例如，某个工厂合约专门生成保护 NFT 的钱包，另一个则负责生成需要 3/5 多签才能操作的钱包。工厂合约提供了创建合约的方法：contract Factory {   function deployContract(bytes data) returns (address);}  工厂返回新创建合约的地址，用户可以通过模拟该方法预测合约的部署位置，从而实现我们最初的目标。在用户操作中增加字段，如果要部署钱包，则指定使用的工厂及要传递的相关数据：struct UserOperation {  //...  address factory;   bytes factoryData;}用户可以调用工厂合约，创建不同类型的合约钱包这样解决了之前的两个问题：  代付者可以选择仅为来自特定工厂的部署付费。  使用经过审计的工厂合约，用户得到的一定是无后门、针对特定功能的合约钱包，无需逐一审查字节码。最后一个问题是部署代码可能在模拟时成功但执行时失败。与之前代付合约 validatePaymasterOp 方法类似，解决方案是：打包器将限制工厂仅能访问其关联存储和正在部署的钱包存储，禁止调用特定方法，并要求工厂使用入口点的 addStake 方法质押 ETH。打包器可根据工厂合约模拟失败的频率来限制或禁用该工厂。至此，我们的架构就实现了 ERC-4337 中的所有核心功能。聚合签名在当前实现中，打包中的每个用户操作都需要单独验证，这种低效的验证方式会产生不必要的 gas 消耗，导致签名验证成本居高不下。为此，我们可以引入密码学中的聚合签名技术，通过单一签名同时验证多个操作，从而显著降低 gas 开支。聚合签名方案允许将多个使用不同密钥签名的消息合并为一个统一的聚合签名。若该聚合签名通过验证，则意味着所有原始签名均有效。BLS (Boneh-Lynn-Shacham) 签名算法是支持签名聚合的典型方案。这种优化对于 Rollup 尤其有价值，因为数据压缩正是 Rollup 的核心目标，而签名聚合能够有效压缩签名部分，进一步提升数据压缩效率。关于签名聚合带来的空间节省，可以参考 Vitalik 在相关推文中的阐述：Some quick theorycrafting of how much data space we can save by improving compression (especially in rollups).ERC4337 signature aggregation ( https://t.co/wFJI3BEq6g ) is essential to this. pic.twitter.com/nrlcOpVJKb&mdash; vitalik.eth (@VitalikButerin) August 4, 2022聚合器并非包中的所有用户操作签名都能聚合。由于钱包可以采用任意逻辑验证签名，因此一个打包中可能包含多种不同的签名方案。这些异构签名无法直接聚合，导致最终的打包可能被划分为多个操作组，每组采用不同的聚合方案，甚至存在不聚合的情况。为此，我们引入多个“聚合器” (aggregator) 合约，在链上表示不同的签名聚合方案。一个聚合方案由两部分组成：聚合（如何将多个签名合并）和验证（如何校验聚合签名）。因此，聚合器合约需要提供以下两个关键方法：contract Aggregator {    function aggregateSignatures(UserOperation[] ops) returns (bytes aggregatedSignature);    function validateSignatures(UserOperation[] ops, bytes signature);}钱包可自主定义其签名方案，并决定兼容的聚合器。若钱包希望参与聚合，则需对外暴露选择聚合器的方法：contract Wallet {    // ...    function getAggregator() returns (address);}通过 getAggregator 方法，打包器可将使用相同聚合器的操作归类，并调用对应聚合器的 aggregateSignatures 方法计算聚合签名。操作组的结构如下：struct UserOpsPerAggregator {    UserOperation[] ops;    address aggregator;    bytes combinedSignature;}  若打包器掌握特定聚合器的链下知识，还可通过硬编码本地签名聚合算法进行优化，避免执行 EVM 中的 aggregateSignatures 代码。相应地，入口点合约新增 handleAggregatedOps 方法，其功能与 handleOps 基本一致，但接收按聚合器分组的操作，主要差异在于验证流程：contract EntryPoint {    function handleOps(UserOperation[] ops);    function handleAggregatedOps(UserOpsPerAggregator[] ops);    // ...}handleOps 通过调用每个钱包的 validateOp 方法执行验证，而 handleAggregatedOps 则使用各组聚合器的 validateSignatures 方法验证聚合签名。执行者用聚合器将用户操作分组，并发送至入口点，以同时进行验证为防止模拟失真问题，我们同样对聚合器施加了必要限制：限制其可访问的存储和可用操作码，并要求在入口点合约中质押 ETH。至此，我们已完整实现 ERC-4337 的架构，仅在方法名和参数等细节上存在微小差异。总结本文从用户需求和实际使用场景出发，逐步阐述了账户抽象的架构设计思路和演进过程，最终呈现了 ERC-4337 的全部核心功能。希望本文的解析能帮助读者更深入地理解账户抽象这一复杂概念，并从中获得启发。附录：与 ERC-4337 的差异1. 验证时间范围钱包希望用户操作仅在特定时间段内有效，以防止恶意打包器囤积操作，并在对自身最为有利的时刻将其打包。由于我们在验证期间禁用了 TIMESTAMP 以避免模拟失真，钱包无法直接通过检查时间戳来限制操作有效期。ERC-4337 给了 validateOp 返回值，钱包可以利用该值选择时间段：contract Wallet {   function validateOp(UserOperation op, uint256 requiredPayment) returns (uint256 sigTimeRange);   // ...}这个返回值由两个连续的 8 字节整数组成，精确定义了操作的时间窗口。值得注意的是，在验证失败时，钱包应返回一个哨兵值而非直接回滚，这有助于更准确地估算 gas 消耗，因为 eth_estimateGas 无法准确反馈回滚交易的 gas 使用情况。2. 任意调用数据我们的钱包接口设计为：contract Wallet {   function validateOp(UserOperation op, uint256 requiredPayment);    function executeOp(UserOperation op);}而在 ERC-4337 中，这一设计被更灵活的机制替代。用户操作引入了 callData 字段，作为传递给钱包的通用调用数据：struct UserOperation {   // ...   bytes callData;}对于典型的智能合约调用，callData 的前 4 字节用作函数标识符，其余部分则为函数参数。这意味着除了必需的 validateOp 方法外，钱包可以完全自定义其接口，用户操作可以调用钱包中的任意方法。同理，工厂合约也不再有固定的 deployContract 方法，而是通过操作的 initCode 字段接收任意调用数据。3.压缩数据原先的用户操作结构包含代付者地址和相关数据的独立字段：struct UserOperation {   // ...   address paymaster;   bytes paymasterData;}ERC-4337 对此进行了优化，将这两个字段合并为单一的 paymasterAndData：前 20 字节表示代付者地址，其余部分为相关数据：struct UserOperation {   // ...   bytes paymasterAndData;}  类似地，工厂合约的 factory 和 factoryData 也被整合为 initCode，进一步简化了数据结构。参考：[1] ERC-4337: Account Abstraction Using Alt Mempool[2] You Could Have Invented Account Abstraction[3] ERC 4337: account abstraction without Ethereum protocol changes"
  },
  
  {
    "title": "区块链去匿名化技术研究",
    "url": "/posts/deanonymization/",
    "categories": "Blockchain",
    "tags": "blockchain, privacy",
    "date": "2023-10-25 12:00:00 +0800",
    





    
    "snippet": "1 背景加密货币是区块链技术的典型应用，依靠分布式共识协议和密码学技术，用户间无需信任授权即可安全地进行交易。自比特币诞生至今，十多年来加密货币生态飞速发展，迄今为止，市场上活跃的加密货币已超过 5000 种。加密货币巨大的市场规模以及交易账户的匿名性和去中心化特性，导致该市场产生了黑客盗窃、恶意攻击、洗钱、暗网交易等诸多违法行为。由于加密数字货币难以被证明是投资品或者消费品，现有的证券法或...",
    "content": "1 背景加密货币是区块链技术的典型应用，依靠分布式共识协议和密码学技术，用户间无需信任授权即可安全地进行交易。自比特币诞生至今，十多年来加密货币生态飞速发展，迄今为止，市场上活跃的加密货币已超过 5000 种。加密货币巨大的市场规模以及交易账户的匿名性和去中心化特性，导致该市场产生了黑客盗窃、恶意攻击、洗钱、暗网交易等诸多违法行为。由于加密数字货币难以被证明是投资品或者消费品，现有的证券法或消费者保护法对其无法适用。与其他资产相比，加密货币全球“7*24”小时交易的特点使得对其进行有效监管需要更全面和更深层次的全球监管协调。同时，传统的监管技术已经无法适用于加密货币，这些因素共同导致了加密市场在全球范围内普遍缺乏有效的监管。因此，针对加密货币及区块链平台的数据分析、去匿名化技术在学界和业界得到了广泛关注，识别异常交易、追踪非法资金流动，为全球监管提供支持，提高整个加密市场的透明度、安全性和可审计性具有重要意义。此外，由于传统金融数据的保密性，金融数据挖掘的研究时常受限，而加密货币链上交易记录可访问、可验证、不可变，其中包含着丰富的用户行为模式，为进行金融数据分析和模式挖掘研究提供了宝贵的机会。当前，针对加密货币流通环节的交易分析及去匿名化研究可以划分为两个主要的研究方向：身份识别和交易溯源。具体而言：  身份识别任务主要从区块链的应用层展开，通过对公共交易信息进行分析，挖掘属性和行为特征，进而确定账户的身份类型，如：交易所、矿池、钓鱼/欺诈等。  交易溯源任务主要从区块链的网络层展开，通过对P2P网络的流量进行监测和分析，获取交易在节点间的传播路径，进而推测交易始发节点，实现匿名的交易到节点IP地址的映射。2 身份识别早期的区块链身份识别方法主要集中在人工特征工程上，这类方法虽然有效，但仍存在一些缺点和挑战。首先，人工特征工程依赖于特征设计者的先验知识，尽管可解释性强，但难以全面捕捉大规模、高维度的区块链交易数据中的底层关系和模式，导致特征利用率低且表达不足。同时，不同区块链平台的技术差异也导致人工特征在平台间的可重用性较低。当前，从图分析的角度进行加密货币交易数据分析是主流的研究方向。加密货币交易涉及复杂的关系网络，包括参与者间的交互和交易拓扑等。图神经网络（GNN）类方法通过深度学习和节点间的消息传递机制能够有效地捕捉这些复杂的非线性关系，从而更好地建模整个系统的行为，此类方法也在各种下游应用中展现出了比随机游走、矩阵分解和深度神经网络等方法更好的性能。以下对主流的身份识别方法的介绍将以两个具有代表性的模型——$I^2GL$ 和 $Ethident$ 为例展开。$I^2GL$ 是一种基于图卷积网络（GCN）的身份识别方法，$Ethident$ 是结合了图注意力机制和对比学习方法的身份识别框架。用户身份的多样性和智能合约实施的复杂性，使得针对以太坊的身份识别和数据分析工作更具挑战。因此，对区块链账户身份识别方法的介绍将主要围绕以太坊平台进行。2.1 $I^2GL$：基于GCN的身份识别方法$I^2GL$ 是一种基于图卷积网络的区块链账户身份推断方法。其基本思想是将区块链账户及其关联交易呈现为一个图，通过图学习技术将账户表征为具有低维特征向量的节点，根据标记的训练集推断节点身份。如图1所示，$I^2GL$ 包含三个阶段：图构建、图学习和节点分类。图1 模型总体架构2.1.1 图构建$I^2GL$ 首先根据以太坊区块中包含的原始交易数据构造交易图。具体而言：      交易图构建为有向图 $G = (V, E)$，其中每个节点 $v \\in V$ 表示一个合约账户或外部账户，账户的总数为 $N = \\vert V\\vert$。        集合 $E$ 包含边，每条边可以表示为一个五元组，即 $E = {(v_i, v_j, w, h, r) \\mid v_i, v_j \\in V; w \\in \\mathbb{R}^+ \\cup {0}; h \\in \\mathbb{Z}; r \\in \\mathbb{R}}$。        $(v_i, v_j)$ 表示交易的方向从 $v_i$ 到 $ v_j$；权重 $w$ 表示转账的加密货币数量；$h$ 表示交易所在的区块高度，在图中起到时间戳的作用；参数 $r$ 是表示不同类型交易的边的类型，如转账、创建或调用智能合约。  交易图可以被表示为三类矩阵：节点表示矩阵、邻接矩阵、时间密度矩阵。节点表示矩阵图2 节点表示节点表示矩阵是为了对账户的总体结构和附加信息进行编码，如图2所示，每个节点被表示为一个五维向量 \\(\\vec{x}_i\\)，其中特征包括：      入度：表示节点收到的交易数。        出度：表示节点发出的交易数。        加权入度：表示节点接收的加密货币总量。        加权出度： 表示从节点发送的加密货币总量。        节点类型： 节点是智能合约还是普通用户账户。  邻接矩阵图3 邻接矩阵由于交易图中的边可以表示诸如转账、合约创建、合约调用等多种行为，直接在图中进行加权度量难以有效提取不同行为的所包含的特征。因此，如图3所示，$I^2GL$ 将原始交易图划分为多个子图，使用一组邻接矩阵\\(\\{A_1, A_2, \\ldots, A_R\\} \\in \\mathbb{R}^{N \\times N}\\)，描述交易图中 $N$ 个节点之间的 $ R $ 种边类型。具体而言，主要考虑了四种交易类型：  带权CALL，如加密货币转账  不带权CALL，如智能合约调用  CREATION，如智能合约部署  REWARD，如挖矿奖励值得注意的是，对于类型1交易，由于账户间转账金额差异巨大，直接使用转账额作为权重可能导致训练过程的下溢或上溢，因此使用启发式方法将转账额划分为三类：（1）小额转账，其交易额小于1 ETH；（2）中等额度转账，其交易额介于1 ETH和10 ETH之间；（3）大额转账，其交易额大于10 ETH。时间密度矩阵图4 时间密度矩阵$I^2GL$ 还将交易的时间特征纳入了考量，相比其他将交易网络建模为静态图的分析方法，能更好地利用交易图中所包含的信息。例如，相较于普通节点，钓鱼/欺诈节点的交易时间方差明显较小，说明这些节点在短时间内活跃度更高。使用时间密度矩阵能更好地捕捉这类特征。用一组矩阵 \\(\\{T^1, T^2, \\ldots, T^R\\vert T^r\\in \\mathbb{R}^{N \\times N}\\}\\) 来描述交易密度，给定序列 \\(\\{h^r_{ij1}, h^r_{ij2}, \\ldots, h^r_{ijm}\\vert h^r_{ijk}&gt;0\\}\\) 作为节点 \\(v_i\\) 和 \\(v_jv_j\\) 之间类型为 \\(r\\) 的 $m$ 个交易的区块高度，交易密度 \\(t^r_{ij} \\in T^r\\) 可以计算为：\\(\\begin{equation}t^r_{ij} = g\\left(\\sqrt{Var \\left[\\frac{1}{m}\\sum_{k=1}^{m} h^r_{ijk}\\right]}\\right) \\tag{1}\\end{equation}\\)2.1.2 图学习基于图构建中的矩阵，GCN传播规则如下：\\(\\begin{equation}H^{(l+1)} = \\delta\\left(\\sum_{r \\in R} \\left(T_r \\odot (D_r)^{-1}A_r\\right)H^{(l)}W_r^{(l)}\\right), \\quad \\forall r \\tag{2}\\end{equation}\\)其中 \\(H^{(l)}\\) 是第 $l$ 层的激活矩阵，\\(H^{(0)} = X\\)，\\(W_r^{(l)}\\) 是属于第 $l$ 层神经网络的用于处理 $r$ 类型交易的权重矩阵，\\(\\delta(\\cdot)\\) 表示激活函数，\\(A_r\\) 是类型 $r$ 的邻接矩阵，\\(D_r\\) 是类型 $r$ 的对角矩阵，\\(T_r\\) 是类型 $r$ 的时间密度矩阵，符号 \\(\\odot\\) 表示哈达玛积。从节点角度看，GCN模型的第 $l$ 层输入为 \\(H^{(l)} = \\{h^{(l)}_1, h^{(l)}_2, \\ldots, h^{(l)}_N \\vert h^{(l)}_i \\in \\mathbb{R}^{N \\times d^{(l)}}\\}\\)， \\(h^{(l)}_i \\in \\mathbb{R}^{d^{(l)}}\\) 是神经网络第 $l$ 层中节点 \\(v_i\\) 的隐藏状态，则上式可以表达为：\\(\\begin{equation}h^{(l+1)}_i = \\delta\\left(\\sum_{r \\in R} \\sum_{j \\in N_i^{r}} \\frac{t^r_{ij}} {\\hat{c}_{i,r}} W_r^{(l)} h^{(l)}_j\\right) \\label{A}\\tag{3} \\end{equation}\\)其中\\(N_i^{r}\\) 表示与节点 \\(v_i\\) 相关联的类型为 $r$ 的邻居节点集合，\\(t^r_{ij}\\) 是从节点 \\(v_i\\) 到 \\(v_j\\) 的类型 \\(r\\) 的交易密度，\\(\\hat{c}_{i,r}\\) 是表示非对称相似性的系数。这一形式可以很好地展示 $I^2GL$ 对交易图节点二阶相似性和非对称相似性的保留。二阶相似性图5 二阶相似性从式\\(\\eqref{A}\\)中可以看出，节点的下一层隐状态是通过聚合该节点一阶邻域（直接相连节点）的当前层隐状态计算得到的，因此从第二层开始，每个节点就整合了其二阶邻域（与其一阶邻域相连的节点）的特征信息。如图5所示，节点 \\(v_a\\) 与 \\(v_c\\) 并不相邻，但有着相似的邻域结构。直接依靠邻接矩阵 \\(A_r\\) 中的边权重 \\(a_{ij}^{r}\\) 得到的一阶相似性难以提取到这种联系。但通过一阶相邻节点 \\(v_b\\)，节点 \\(v_a\\) 可以聚合到节点 \\(v_c\\) 的特征，反之亦然。由此即可捕捉到节点的二阶相似性，进而提高身份推断的准确性。非对称相似性图6 非对称相似性式\\(\\eqref{A}\\)中引入了系数 $\\hat{c}_{i,r}$，定义为：\\(\\begin{equation}\\hat{c}_{i,r} = \\frac{1}{g(d^r_i) \\cdot |N^r_i|}  \\tag{4}\\end{equation}\\)其中，$d^r_i = \\sum_j a^r_{ij}$，$g$ 是一个压缩函数，\\(\\vert N_i^{r}\\vert\\)用于归一化。可以看出，$\\hat{c}_{i,r}$ 与节点 $v_i$ 的出度相关，因而可以区分节点 $v_i$ 指向 \\(v_j\\) 和 \\(v_j\\) 指向 $v_i$ 之间的非对称关系。例如图6所示，在交易图中，假设节点 \\(v_a\\) 是一个用户，而节点 \\(v_b\\) 和 \\(v_c\\) 是交易所账户。边 \\((v_a, v_b)\\) 代表存款过程，边 \\((v_c, v_a)\\) 是提款过程，边 \\((v_b, v_c)\\) 代表交易所内部账户之间的存款移动。在对称模型中，这三条边权重应该相等，即 \\(w_{ab}^r = w_{bc}^r = w_{ca}^r\\)。但实际场景中，边 \\((v_a, v_c)\\) 和 \\((v_c, v_a)\\) 具有不同的语义，存款和提款行为在分析时应该有所区分，不能一概而论。因此，需要引入系数$\\hat{c}_{i,r}$来增大节点的入边和出边在图嵌入中的差异，以保留节点间的非对称相似性。2.1.3节点分类图7 节点分类$I^2GL$ 使用交叉熵损失函数作为训练目标：\\(\\begin{equation}L = -\\sum_{i=1}^{T} \\sum_{j=1}^{m} y_{i,j} \\log p_{i,j} + \\lambda\\lVert \\theta \\rVert^2  \\tag{5}\\end{equation}\\)其中，\\(T\\) 是训练样本数量，\\(y_{i,j}\\) 是节点 \\(v_i\\) 属于类别 \\(j\\) 的真实概率，\\(\\theta\\) 是所有参数的集合，\\(\\lambda\\) 是L2正则化的系数。最后一层GCN的输出是一个概率矩阵 \\(P = \\{ \\vec{p}_1, \\vec{p}_2, \\ldots, \\vec{p}_N \\}\\)，其中向量 \\(\\vec{p}_i\\) 包含将节点 \\(v_i\\) 分类到 $m$ 个类别中的概率。2.1.4 效果论文以2018年1月1日到2018年3月31日期间的116,293,867笔交易，作为图构建的输入。通过解析交易，获得了16,599,825个活跃账户，包括14,450,993个外部账户、2,148,831个智能合约账户。图8 身份分类结果对比如图8所示，在身份识别任务中对比DeepWalk、PARW、rGCN等方法，$I^2GL$ 在精准率、召回率和F1值等指标方面均有明显优势。GCN技术对图全局结构的感知使得分类效果明显基于随机游走的方案，同时$I^2GL$ 还针对区块链交易图的结构和特性进行提出了优化方案，更全面地保留了图中的信息和特征，使模型的性能得到了较大提升。$I^2GL$ 作为一种基于图卷积网络的区块链账户身份识别方法主要的创新之处可以概括为以下四个方面：  多类型邻接矩阵：构建了多种类型的邻接矩阵来描述区块链交易图中节点之间的各类交易活动，如转账、合约创建等。这种异构性的建模是其区别于传统单一邻接矩阵的创新点。  多层图卷积网络：通过构建多层的图卷积网络，递归地捕捉到节点间的高阶相似性，与单层网络方法相比能更好地捕捉交易图结构的特征。  非对称相似性：引入了非对称系数，保留了交易图中边的方向所包含的语义，编码了节点间的非对称关系。  交易密度矩阵：根据不同类型账号活跃时间分布的差异，构建了交易密度矩阵，反映了节点间联系的时间变化特征。图9 模型变体的性能对比图9对比了$I^2GL$ 不同变体的性能，清晰地显示了每种技术所起的作用：      对比使用单一邻接矩阵和单一类型的 $I^2GL_{sR}$ 和使用多邻接矩阵的 $I^2GL_{mR}$。在都不使用交易密度矩阵的条件下，如图9(a)所示，通过引入多邻接矩阵，模型的分类精准率大幅提高，表明异构活动对于保留交易图特征的重要性。        对比单层卷积网络 $I^2GL_{1L}$ 和具有两层卷积网络的完整 $I^2GL$ 。如图9(b)所示，通过保留二阶相似性，F1值提高了13%，表明二阶相似性在交易图结构中的重要性。        对比无非对称系数、无交易密度矩阵的 $I^2GL_{nT;nA}$和无交易密度矩阵的 $I^2GL_{nT}$ 。如图9(c)所示， $I^2GL_{nT}$ 在召回率和F1值方面分别提高了56%和31%。        对比无交易密度矩阵的 $I^2GL_{nT}$ 和完整 $I^2GL$ 。如图9(d)所示， $I^2GL$ 在所有指标上均优于 $I^2GL_{nT}$ 。  总体而言，$I^2GL$ 是GCN方法应用在以太坊身份识别领域的一次较早的尝试，其采用的研究框架和总体思路有一定的代表性，该模型融合了对区块链交易图结构和属性的考量，从多个维度提出优化方法，并取得了较好的应用效果，这是其在方法学上的价值所在。但同时，$I^2GL$ 模型仍存在一些问题：  全图训练，计算消耗和时间成本较高  泛化能力不足，无法直接为新加入的节点产生嵌入  表达能力一般，卷积操作不可学习，直接由图结构决定2.2 $Ethident$：行为感知型的身份识别框架$Ethident$ 是一个针对以太坊的身份识别框架，旨在解决图分析方法在面对大规模的区块链交易图时所面临的计算成本高、可扩展性差、泛化能力不足等问题。具体而言：  作者使用以太坊数据构建了账户交互图及其轻量版本，并基于此设计了子图采样策略，将账户身份识别视为子图级的分类任务，从交互图中提取目标账户的邻域子图，以便进行图神经网络的小批量训练，从而消除了交易图中节点和边的频繁更新对全图学习的不利影响，实现了可扩展的身份识别。      文章提出了名为 HGATE 的分层图注意力编码器，能够有效地表征节点级的账户特征和子图级的行为模式。    设计了命名为 $Ethident$ 的行为感知型的以太坊账户识别框架，将数据增强和对比自监督机制用于账户识别，以缓解在监督学习过程中可能导致模型泛化性不佳的标签稀缺问题，并学习账户的行为模式表示。2.2.1 账户交互图$Ethident$ 的核心思路是使用图分析方法从图分类的角度来识别以太坊中的账户身份。区块链的交易图通常用 $G = (V, E, X, E, Y)$ 表示，其中 $V = {v_1, v_2, \\ldots, v_n}$ 是账户节点的集合， $E \\subseteq {(v_i, v_j) \\vert  v_i, v_j \\in V}$ 是交互边的集合， $X \\in \\mathbb{R}^{n \\times F_1}$ 是节点特征矩阵， $E \\in \\mathbb{R}^{m \\times F_2}$ 是边特征矩阵，$\\vert E\\vert = m$。$Y = {(v_i, y_i) \\vert v_i \\in V}$ 表示有标签节点集合。节点 $v$ 的子图可以表示为 $g_v \\subset G$。对于给定的交易图 $G$，子图级的账户身份识别就是学习一个函数 $f(g_v) \\rightarrow y$，将账户子图 $g_v$ 的模式映射到身份标签 $y$。原始的区块数据包含了合约调用和交易的相关信息，据此可以构建账户交互图（Account Interaction Graph, AIG）：一个有向、加权和异构多重图 \\(G = (V_{\\text{EOA}}, V_{\\text{CA}}, E_{\\text{t}}, E_{\\text{c}}, Y)\\)，其中 \\(V_{\\text{EOA}}\\) 和 \\(V_{\\text{CA}}\\) 分别是外部账户和合约账户的集合， \\(E_{\\text{t}} = \\{(v_i, v_j, d, w) \\vert  v_i, v_j \\in V_{\\text{EOA}}\\}\\) 是从交易信息构建的有向边集， \\(E_{\\text{c}} = \\{(v_i, v_j, d, f) \\vert  v_i \\in V_{\\text{EOA}} \\cup V_{\\text{CA}}, v_j \\in V_{\\text{CA}}\\}\\) 是从合约调用信息构建的有向边集。边属性 \\(d, w, f\\) 分别表示时间戳、转账数量和函数调用。 标记节点集 \\(Y = \\{(v_i, y_i) \\vert  v_i \\in V_{\\text{EOA}}\\}\\) 表示具有身份标签 \\(y\\) 的外部账户集合。图10 AIG和lw-AIG的构建过程如图 10(a) 所示，原始的 AIG 是一个具有密集连接的异构多重图，节点和边附加了各类信息，显著增加了信息挖掘的复杂性。因此，需要对 AIG 进行简化，使其成为一个更为均匀和稀疏的同质图。这一过程可以从节点和边两个层面进行：      交互合并与边特征构建在交互合并过程中，如图2(b)所示，节点间的多个有向交互（交易或合约调用）将被合并为一条边，用边属性 $t$ 表示合并交互的数量，边属性 $\\tilde{w}$ 表示合并交互的总交易金额。同时，移除时间戳 $d$ 和函数调用 $f$ 的两个原始边属性。最终，对于任意交易边 $(v_i, v_j) \\in E_t$，其边特征向量为 $e_{ij} = [t, \\tilde{w}]$。        节点特征构建具有不同行为模式的账户对智能合约有不同的调用偏好。因此，使用有关合约调用的信息构建账户特征，如图2(c)所示。具体而言，设 $n$ 和 $F$ 分别为AIG中的EOA和CA的数量，构建一个外部账户特征矩阵 $X \\in \\mathbb{R}^{n \\times F}$ 来表示对合约调用的偏好，如下所示：\\(X = [x_1; \\ldots ; x_i ; \\ldots ; x_n]\\)，其中 $x_i = [t_1, \\ldots , t_j , \\ldots , t_F]$，其中\\(t_j = \\begin{cases} t &amp; \\text{如果有} \\, t \\, \\text{次对} v_{ca_j} \\, \\text{的调用} \\\\ 0 &amp; \\text{如果没有对} v_{ca_j} \\, \\text{的调用} \\end{cases}\\)  简而言之，通过特征构建过程，AIG 被转换成了轻量账户交互图（Lightweight Account Interaction Graph，lw-AIG）：一个有向、加权和同质图 \\(G = (V_{\\text{EOA}}, \\widetilde{E}_{t}, X, E, Y)\\)，其中 \\(\\widetilde{E}_{t} = \\{(v_i, v_j, t, \\widetilde{w}) \\vert v_i, v_j \\in V_{\\text{EOA}}\\}\\)， \\(X\\) 是从合约调用信息构建的节点特征矩阵， \\(E\\) 是边特征矩阵。边属性 \\(t\\) 表示从 \\(v_i\\) 到 \\(v_j\\) 的有向交互次数，边属性 \\(\\widetilde{w}\\) 表示从 \\(v_i\\) 到 \\(v_j\\) 的总交易金额。也就是说，lw-AIG中的节点特征反映了合约调用信息，而边特征反映了交易信息。$Ethident$ 框架由子图采样模块、子图增强模块、分层注意力编码器和训练模块构成。对于目标账户 $v_i$，$Ethident$ 的输入是从 lw-AIG 中采样的账户交互子图 $g_i$，输出预测的身份标签 $\\hat{y}_i$。2.2.2子图采样AIG 是一个规模庞大的图，即使经过简化，仍保留着大量的外部账户节点，不适合进行GNN的全批量训练。因此，$Ethident$ 中将账户身份识别视为子图级的分类任务，其核心思路在于：  不同类型账户的局部结构中隐含了不同的行为模式；  由目标账户及其邻域组成的子图包含丰富的信息，能为身份识别提供关键的行为模式；  子图即中心目标节点的感受野，比整个图小得多，可以进行小批量训练。基于此思路，$Ethident$ 中采用了 TopK 子图采样方法。TopK 采样根据不同的边属性：金额 $\\tilde{w}$、交互次数 $t$、交易平均金额 $\\frac{\\tilde{w}}{t}$ 获取 h-跳交互子图。具体而言，对于目标账户节点$v_i$，根据边属性之一对其相邻节点进行排序并采样前 K 个，然后对采样到的每个账户再次进行 TopK 采样，随后递归执行。递归采样可以表示为：\\(\\begin{equation}V_k = \\bigcup_{v \\in V_{k-1}} \\text{topK}(N_v, K, E[v, N_v, i]), \\quad i \\in \\{0, 1, 2\\} \\tag{6} \\end{equation}\\)其中 $V_k$ 是在第 $k$ 跳采样的节点集合，$V_0 = {v_i}$，$N_v$是节点 $v$ 的1-跳邻居集，$K$ 是每跳采样的邻居数，$E[v, N_v, i]$ 是采样依据的候选边属性，$i$ 是所使用的边属性的指示符。图11 根据不同边属性进行子图采样的过程经过 $h$ 次迭代，可从 lw-AIG 中采样得到节点集合 $V_i = \\bigcup_{k=0}^h V_k$，由 $V_i$ 即可得出目标账户 $v_i$ 在lw-AIG中的子图 $g_i$ 。图11 展示了根据不同边信息进行子图采样的过程。2.2.3 分层图注意力编码器图12 分层注意力编码器 HGATE$Ethident$ 的核心是GNN编码器：HGATE。该编码器接受目标节点的子图作为输入，通过分层注意力机制学习账户及其行为模式的表达，输出子图的嵌入表示，如图5所示。该编码器通过预测头实现账户识别，如图12(a)所示。编码器 $f_{\\theta}$ 的细节：  邻居特征对齐： 编码器以账户为中心，每个账户 $v_i$ 的邻居特征由其相邻账户特征 $x_j$ 和交互特征 $e_{ij}$组成，表示为$[x_j | e_{ij}]$。目标账户特征 $x_i \\in \\mathbb{R}^F$ 和其邻居特征 $[x_j | e_{ij}] \\in \\mathbb{R}^{F+2}$ 的维度不同，因此需要通过线性变换和非线性激活来对齐特征维度，如图12(b)所示。这一过程可以通过以下参数为 $\\theta_x$ 的全连接层实现：\\(\\begin{equation} \\tilde{x}_j = \\text{LeakyRelu}(\\theta_x \\cdot [x_j \\| e_{ij}])  \\tag{7} \\label{C} \\end{equation}\\)用于账户嵌入的节点级注意力：这一模块是为了保留输入子图中账户的相关性，通过关注邻域中最相关的部分来学习账户表示。当在交互子图中识别目标账户时，不同的相邻账户产生的影响也不同。举例来说，如果某个账户与目标账户之间发生的交易金额高、交易次数多，或与其合约调用偏好相近，那么可以认为该账户保留了更多与目标账户身份相关的信息，因而在识别目标账户时能起到更重要的作用。节点级注意力机制就是基于这一点，通过组合目标账户的邻居特征与其重要程度（即注意力），来学习输入子图中每个账户的隐藏表示。形式化的定义：对于输入子图 $g$ 中的任意账户 $v_i$，节点级注意力机制会学习其邻居节点 $v_j$ 的注意力分数：\\(\\begin{equation}a^l_{ij} = \\text{LeakyRelu}(\\theta^l_{n} \\cdot [h^l_{i} || h^l_{j}]) \\tag{8} \\end{equation}\\)通过由 $\\theta_{ln}$ 参数化的线性变换和非线性LeakyRelu激活，计算节点 $v_j$ 的隐藏特征对节点 $v_i$ 在第 $l$ 层的重要度，计算得到的注意力分数 $a$ 用 Softmax 函数归一化。随后通过邻域上下文聚合更新目标账户的特征：\\(\\begin{equation}h^{l+1}_{i} = \\text{Elu}\\left(\\alpha^l_{ii} \\cdot \\theta^l_{\\alpha} \\cdot h^l_{i} + \\sum_{j \\in N(i)} \\alpha^l_{ij} \\cdot \\theta^l_{\\alpha} \\cdot h^l_{j}\\right) \\tag{9} \\end{equation}\\)通过由 $\\theta^l_{\\alpha}$ 参数化的线性变换和非线性Elu激活，计算最终的输出特征。节点级注意力机制是用于节点嵌入的。如图12(d)所示，HGATE 用多个注意力层来捕捉账户特征，输入是初始账户嵌入 $h_0$，由账户和交互特征（式$\\ref{C}$）经全连接层生成。多个注意力层迭代地对邻居特征进行传递、变换、聚合并更新节点表示。经过 $k$ 次迭代后，最终输出的账户嵌入 $h_k$ 就包含了 $k$-跳 内的交互影响。      用于模式嵌入的子图级注意力池化： 该模块旨在通过提取子图级特征，表征输入子图中目标账户的行为模式。    账户的行为模式与其身份密切相关，即具有不同身份的账户通常表现不同，具有不同的子图模式。这点很好理解，举例来说：                  对于“交易所”子图，中心节点应该具有极高的中心性，并与邻居账户互动频繁，表明存在高交易量的交易订单。                    对于“欺诈”或“赌博”子图，存在两个明显的特征标志着用户的高投入和低回报：                              中心节点与邻居之间的双向边（相互交易）很少，中心节点的入度很高，出度很低；                                中心节点的入边（投资）包含与加密货币相关的较大特征值，而出边（回报）的特征值较小。                                由此可见，不同账户对表征目标账户行为的子图模式也会产生不同的贡献。传统做法通常使用求和、均值或最大池化来捕捉图级别的特征，导致特征平滑和表达能力不足。因此文章提出了子图级的注意力池化模块，来学习账户子图的表示，如图12(a)所示。    具体而言，对于子图 $g$，首先通过对子图中所有账户嵌入进行全局最大池化，得到初始子图级别嵌入$s$，这里池化层的输入就是上一步生成的最终账户嵌入$h_k$ ：\\(\\begin{equation}s = \\text{MaxPooling}(h_k) \\tag{10}\\end{equation}\\)之后通过与节点级注意力层相似的方式，按重要度聚合所有账户的特征来更新$s$。也就是对于初始子图嵌入 $s$，使用注意力机制来学习账户 $v_j$ 在子图中的重要度，得到注意力分数：\\(\\begin{equation}a_j = \\text{LeakyRelu}(\\theta_s \\cdot [s || h^k_{j}]) \\tag{11} \\end{equation}\\)通过由$\\theta_s$参数化的线性变换和非线性LeakyRelu激活，计算账户 $v_j$ 的隐藏特征对初始子图嵌入$s$的重要性。随后用 Softmax 函数计算归一化的注意力分数 \\(\\beta_j\\)。最后，注意力池化执行以下更新过程：\\(\\begin{equation} g = \\text{Elu}(\\beta_s \\cdot \\theta_{\\beta} \\cdot s + \\sum_{j \\in V_g} \\beta_j \\cdot \\theta_{\\beta} \\cdot h^k_{j}) \\tag{12}\\end{equation}\\)其中，$V_g$ 是子图 $g$ 的节点集通过由 $\\theta_{\\beta}$ 参数化的线性变换和非线性Elu激活，计算最终子图嵌入 $g$，该嵌入表征了目标账户的行为模式。  2.2.4 子图对比学习为了解决账户标签稀缺问题，学习表达能力更强的模式嵌入，$Ethident$ 引入了对比自监督学习。      图增强： 对比学习在很大程度上依赖于数据增强策略来生成不同视图。论文中使用三类图增强方法来生成子图的增强视图。                  结构级增强                  节点丢弃： 每个节点有一定概率$P$从子图中被丢弃。          边移除： 每条边有一定概率$P$从子图中被移除。                    属性级增强                  节点属性掩码： 节点特征的每个维度有一定概率$P$被设置为零。          边属性掩码： 边特征的每个维度有一定概率$P$被设置为零。                    基于采样的增强由于每个子图都是通过前述的三种子图采样策略之一从 lw-AIG 中得到的，可以使用其他两种采样方法为该子图生成基于采样的增强视图。在图增强过程中，为每个目标账户子图 $g_i$ 生成两个增强视图 $ \\hat {g}^1_i $和 $ \\hat {g}^2_i$，并将目标账户的身份标签分配给它们作为伪标签：\\(\\begin{equation}\\displaylines{D_{\\text{aug}1} = \\{(\\hat{g}^1_{i}, y_i) | \\hat{g}^1_{i} = T_1(g_i); (v_i, y_i) \\in Y\\}  \\\\\\ D_{\\text{aug}2} = \\{(\\hat{g}^2_{i}, y_i) | \\hat{g}^2_{i} = T_2(g_i); (v_i, y_i) \\in Y\\} \\tag{13}}\\end{equation}\\)            子图对比： 增强视图 $ \\hat{g}^1_{i} $ 和 $\\hat{g}^2_{i}$ 被输入到编码器$ f_\\theta $ 中，产生整个子图的表示$ g^1_{i}$和$g^2_{i} $，然后通过投影头 $f_{\\phi}$ 映射到对比的嵌入空间，得到$z^1_{i}$和$z^2_{i}$。$\\theta$ 和 $\\phi$ 分别是图编码器和投影头的参数。子图级对比的最终目标是通过最小化对比损失来最大化对比空间中两个相关增强视图之间的一致性：   \\(\\begin{equation}  L\\_{\\text{self}} = \\frac{1}{N} \\sum_{i=1}^{N} L_i \\tag{14}   \\end{equation}\\)  其中，$N$ 是一个批次中子图的数量（即批次大小）。每个子图的损失可以计算为：  \\(\\begin{equation}  L_i = -\\log \\frac{\\exp(\\text{cos}(z^1\\_{i}, z^2_{i})/\\tau)}{\\sum_{j=1, j \\neq i}^{N} \\exp(\\text{cos}(z^1_{i}, z^2_{j})/\\tau)} \\tag{15}   \\end{equation}\\)  账户子图 $g_i$ 的两个相关视图 $z^1_{i}$ 和 $z^2_{i}$ 被视为正对，而批次中的其余视图对被视为负对。  目标是最大化正对间的一致性。对比学习使得相同类型的账户具有更一致的表示，而不同类型的账户具有更明显的差异。  2.2.5 模型训练通过一个预测头 $f_{\\psi}$ 实现账户识别，将子图表示映射到反映账户身份的标签，得到一个分类损失：\\(\\begin{equation}L_{\\text{pred}} = -\\frac{1}{N} \\sum_{i=1}^{N} y_i \\cdot \\log(f_{\\psi}(g_i)) \\tag{16}\\end{equation}\\)其中，$L_{\\text{pred}}$是交叉熵损失。自监督的子图对比是先导任务，用作子图分类任务的正则化。编码器 HGATE 与先导任务和子图分类任务一起进行联合训练。损失函数包括自监督和分类任务的损失函数：\\(\\begin{equation}L = L_{\\text{pred}} + \\lambda \\cdot L_{\\text{self}} \\tag{17}\\end{equation}\\)其中，超参数$\\lambda$控制自监督项的影响。2.2.6 效果$Ethident$ 从与账户的行为模式和身份相关的节点和边信息中学习，并使用分层注意力机制有效地刻画了节点级别的账户特征和子图级别的行为模式，在账户识别上表现出色。文章以“2015-07-03”到“2020-05-04”时间段内的1000万个区块为数据集（包含309,010,831笔交易和175,351,541次合约调用，涉及90,193,755个外部账户和16,221,914个合约账户），对“ICO钱包”、“矿池”、“交易所”和“钓鱼/欺诈”四类身份进行识别，并与人工特征工程方法、图嵌入方法、基于GNN的方法进行比较。结果显示 $Ethident$ 在所有数据集上明显优于手动特征工程和图嵌入方法，在 F1 分数方面相对改进了2.09% ∼ 18.27%，表明学习到的子图特征更能捕捉账户的行为模式，相比人工或浅层拓扑特征更为有效。与基于GNN的方法相比， $Ethident$ 在最佳基线上相对改进了1.13% ∼ 4.93%。针对实验结果可以得出一些观察和分析：交互子图中的模式对行为模式的分辨是账户去匿名化的关键，不同类别的账户在其交互子图中体现了不同的行为模式，如图13所示。以下是对不同类型账户的子图模式的解释，可以发现，不同的账户身份适合的采样策略也有所不同：图13 不同类型账户的交互子图      ICO-wallet（ICO钱包）： 初创币发行（ICO）是一种通过发行代币为区块链项目筹集资金的融资方法。ICO 项目一般通过预售代币来换取大量以太币，并在一段时间后向支持者提供一定的投资回报。关键的行为模式表现为从中心 ICO 账户到周围支持者的大量出边，带有一定数量的投资回报。由于投资行为通常涉及更高的交易金额，根据金额信息（Amount）对交互子图进行采样可以最大程度地保留ICO账户的行为模式。        Mining（矿池）： 矿池是一个合作挖矿团队，共享算力挖掘区块。矿池将收到系统的大量挖矿奖励，并将其分配给下属矿工。关键的行为模式表现为从中心矿池节点到周围矿工节点的大量出边，带有一定数量的奖励。由于以太坊的区块奖励在一段时间内是固定的，同一矿池的矿工通常具有相对稳定的平均挖矿收入，因此使用平均金额（avgAmount）信息来引导交互子图的采样更合适。        Exchange（交易所）： 交易所是提供资产交易撮合和清算服务的平台。交易所账户通常与其客户频繁互动，以处理大量的交易订单，并在交互图中作为具有极高中心性的中心节点（即极高的入度和出度）。因此，根据交互次数（Times）信息对交互子图进行采样更为有益。        Phish/Hack（钓鱼/欺诈）： 钓鱼者和黑客都参与非法欺诈活动，通过传播大量包含病毒、木马等的网站、电子邮件或链接，欺骗接收者直接汇款或提供系统特权的敏感信息。中心的“钓鱼/欺诈”账户通过各种欺诈手段收到大量以太币，并将其分散给其他“钓鱼/欺诈”账户以掩盖行踪。关键的行为模式具有一个明显特征：中心节点与周围节点之间的双向边（相互交易）很少，中心节点具有较高的入度和较低的出度。由于非法欺诈活动，如虚假代币交换或勒索软件通常设定了特定的阈值金额或固定赎金，这可以在平均金额信息中反映出来，因此使用根据平均金额（avgAmount）信息采样的子图可能更利于识别“钓鱼/欺诈”账户。  子图对比学习的效果针对 $Ethident$ 中子图对比学习的有效性，文章列举了一些观察结果以及可解释的分析。      图增强总体有效，结构级增强策略效果更明显： 如图14所示， $Ethident$ 相对于其无增强版本总体性能有所提升。总体而言，与其他增强对比起来，使用“NodeDrop 节点丢弃”或“EdgeRemove 边移除”作为增强视图更可能产生正增益。另外，对于频繁调用合约的交易所账户，其节点特征中将会有更多的非零值，因而属性掩码成为了有效的增强策略。    此外，注意到不同采样策略的子图数据集对各种增强视图的组合敏感，应用中需要灵活选择采样策略和增强组合。  图14 不同增强策略带来的F1分数增益(%)2) 对比自监督提高了模型在账户特征学习中的泛化能力：图15中可视化了 Ethident 和 Ethident(w/o GC) 学到的子图嵌入，不同的颜色表示不同的标签。与仅使用预测损失 $L_{pred}$ 的 Ethident(w/o GC) 相比，在应用对比约束 $L_{self}$ 后，实现了更明显的跨类别可分离性和类内紧凑性，这说明了其在学习行为模式差异方面的有效性。此外，Ethident 用相对更清晰的边界分离了不同的模式，表明对比自监督可以在标签稀缺的情况下有效提高模型的泛化能力。图15 Ethident / Ethident(w/o GC) 子图嵌入可视化2.3 总结与思考本部分以 $I^2GL$ 和 $Ethident$ 为例介绍了图神经网络（GNN）在区块链身份识别领域的应用。与传统方法和其他现有技术相比，GNN 模拟复杂关系的优势使其在区块链交易图分析方面表现突出。区块链上的交易和合约调用等行为构成了一个具有多重依赖关系的复杂网络。GNN 可通过在相互连接的节点之间传播信息来有效处理这种复杂结构，不仅可以捕捉直接关系，还能够捕捉高阶关系，因而能为身份识别提供更全面的视角。此外，GNN 具备学习数据非线性表示的能力，能识别隐含的微妙关系，这些关系可能会被线性模型或基于规则的系统所忽略。图16 GCN与GAT      $I^2GL$ 代表了基于图卷积网络（GCN）的方法。GCN 采用的图卷积机制是广泛应用于图神经网络中的信息聚合技术之一。总体来说，GCN 通过图卷积算子为图中的各个节点学习特征表示。图卷积算子允许网络从节点的邻居中聚合信息，以学习更复杂且信息丰富的图结构表示。在多层图卷积网络中，传播规则基于公式：    \\(\\begin{equation}H^{(l+1)} = \\sigma \\left(\\tilde{D}^{-\\frac{1}{2}} \\tilde{A} \\tilde{D}^{-\\frac{1}{2}} H^{(l)}W^{(l)}\\right) \\tag{18}\\end{equation}\\)其中，\\(\\tilde{A} = A + I_N\\) 是带有自连接的无向图G的邻接矩阵。$I_N$ 是单位矩阵，$\\tilde {D}_{ii} = \\sum _j \\tilde {A}_{ij}$，即$\\tilde{A}$的度矩阵，$W^{(l)}$ 是特定层的可训练权重矩阵。 $\\sigma(\\hat{A}\\cdot)$ 表示激活函数。 $H^{(l)} \\in \\mathbb{R}^{N \\times D}$ 是第 $l$ 层的激活矩阵；$H^{(0)} = X$ 。可以看到，其中的卷积操作由给定的图结构决定，除了节点级的线性变换 $H^{(l)}W^{(l)}$ 外，卷积操作是不可学习的。对于类似 $I^2GL$ 中有向图（即非对称邻接矩阵），$\\tilde{A}$ 可以通过入度对角矩阵的逆 $\\tilde{D}^{-1}$ 来进行归一化，即：\\(\\begin{equation}H^{(l+1)} = \\sigma(\\tilde{D}^{-1}\\tilde{A}H^{(l)}W^{(l)}) \\tag{19}\\end{equation}\\)        $Ethident$ 代表了基于注意力的图神经网络（GAT）。GAT 中使用了注意力机制，该机制允许模型在消息传递过程中集中关注图中最相关的节点或边。在这类模型中，信息聚合的权重是根据节点或边之间的相似性动态学习的，与图卷积网络中的固定权重不同。这使得GAT能够更有效地建模图数据中的复杂关系和依赖。计算注意力系数（α）和每个节点的隐藏状态（h）的公式如下：\\(\\begin{equation}\\alpha_{ij} = \\frac{\\exp\\left(\\text{LeakyReLU}\\left(\\mathbf{a}^\\top [\\mathbf{W}h_i \\,||\\, \\mathbf{W}h_j]\\right)\\right)}{\\sum_{k \\in \\mathcal{N}_i} \\exp\\left(\\text{LeakyReLU}\\left(\\mathbf{a}^\\top [\\mathbf{W}h_i \\,||\\, \\mathbf{W}h_k]\\right)\\right)}，h_i' = \\sigma\\left(\\sum_{j \\in \\mathcal{N}_i} \\alpha_{ij} w h_j\\right) \\tag{20}\\end{equation}\\)其中 $W$ 是与线性变换相关联的权重矩阵，应用于每个节点。$a$ 是单层 MLP 的权重向量。这种自注意机制使得 GAT 在图数据中能够更灵活地捕捉节点之间的关系，从而提高建模效果。在此基础上，$Ethident$ 通过分层注意力机制HGATE有效地提取了节点级账户特征和子图级行为模式特征。相比GCN使用简单的图卷积和池化操作，HGATE可以通过注意力机制聚焦于与目标账户识别相关的关键行为模式信息。同时，$Ethident$ 还采用了基于子图的识别框架，通过子图采样和小批量训练避免了全图训练，大大减少了计算和内存消耗，使模型更适合在大规模区块链图数据上运行。图增强和子图对比学习机制使模型能够学习鲁棒特征，在标签稀缺的情况下提升了模型的泛化能力。  思考：      $I^2GL$ 相对于一般 GCN 模型的亮点之一是其将交易的时间属性纳入了考量。其根据区块链的特点，将区块高度这一区块链独有的数据作为时间戳，引入时间密度矩阵，从而更全面地捕捉和利用了账户活跃时间分布的特征，提高了账户身份识别的精度。    在 $Ethident$ 中，最初根据区块原始数据构建的账户交互图（AIG）中包含交易的时间信息：从交易信息构建的有向边集 \\(E_{\\text{t}} = \\{(v_i, v_j, d, w) \\vert v_i, v_j \\in V_{\\text{EOA}}\\}\\) 中的边属性 \\(d\\) 即表示时间戳。但随后，出于复杂性的原因，AIG 被简化为了 lw-AIG ，节点间的多个有向交互被合并为一条边，用边属性 $t$ 表示合并交互的数量，边属性 $\\tilde{w}$ 表示合并交互的总交易金额，同时移除了时间戳 $d$ 和函数调用 $f$ 的两个原始边属性。$Ethident$ 后续的工作都是基于 lw-AIG 进行的，因此交易的时间分布特征被忽略了，这可能对部分身份类型的识别产生不利影响，如钓鱼/欺诈账户的交易频率显著高于普通账户。保留交易的时间特征是可优化的方向之一。        $Ethident$ 通过手动合并边、移除属性等方法将异构的原始账户交互图（AIG）转化为同构图，然后在此基础上应用分层注意力机制和子图采样分类。手动构建同构图的转换过程中难免会失去部分异构图的结构信息，针对区块链交易图这样典型的异构图，可以考虑能直接作用于异构图的 HGNN（Heterogeneous Graph Neural Network）方法。    HGNN 中的 HAN（Heterogeneous Graph Attention Network）思路总体与$Ethident$ 类似：根据预先定义的元路径（meta-path）获取异构图的子图，然后应用分层注意力机制，节点级注意力学习节点与其邻居之间的注意力值，语义级注意力学习异构图中不同元路径的注意力值。基于两个级别的学到的注意力值，使模型学到的节点嵌入能更好地捕捉图中的复杂结构和语义信息。    而 GTN（Graph Transformer Networks）方法更进一步，无需预定义元路径，而是从给定的数据和任务中自动识别有用的元路径，通过图变换层（Graph Transformer，GT）将异构图转换为由元路径定义的多个新图，元路径可具有任意的边类型和长度。这可以有效防止人工预定义元路径或采样策略造成的信息损失，同时也能更好地表达异构图的复杂语义关系，端到端学习可以使模型自动学习对给定任务最优的图结构。    目前，尚未出现将 HGNN 方法应用在区块链身份识别领域的研究，是值得关注的方向之一。    图17 GTN方法说明  总体而言，区块链去匿名化在应用层的主要任务是账户身份识别。该领域当前主流的方法是基于图神经网络的 GCN、GAT 方法，本部分以具有代表性的$I^2GL$和$Ethident$为例，介绍了相关研究的原理及效果，并在此基础上对模型的优缺点以及可改进的方向进行了总结与思考。3 交易溯源相较于基于应用层的账户身份识别任务，基于网络层进行的交易溯源任务能够提供更直接和实时的身份信息：通过分析网络层传输的交易信息，发现特定交易在加密数字货币交易网络中的传播路径，进而推测交易的始发节点，实现匿名交易和交易始发节点IP地址的关联，达到溯源目的。现有的网络层去匿名化方法通常利用区块链底层网络中的交易传播信息，以利用伪名和IP地址之间的可链接性。Koshy等人对不同的交易中继模式进行了分类，并设计了启发式方法来假设交易的所有权。Dan Kaminsky等人提出了女巫攻击，假设新交易的第一个转发节点的IP地址归原始发送者所有。Biryukov等人提出了一种基于邻居节点的交易可追踪机制，并在后续工作中，开发了一种TOR中间人攻击和基于“地址cookie”的IP地址暴露方法。总体而言，此类攻击的思路想似：当节点执行交易时，需要将交易发送给其对等方，对等方会进一步传播交易。因此，攻击者若能监听所有交换消息，即可将每个交易映射到其创建者的IP地址，从而有效地去匿名化节点。要实现这一目的，攻击者需要使用一个看似普通的“超级节点”，连接到所有活跃的比特币节点并监听其中继的交易。但这类攻击方法需要庞大的计算资源且十分易于识别，并可能对区块链底层网络造成严重干扰，同时还可能被扩散广播机制、Dandelion机制等应用层的对策减轻其攻击效果。$Perimeter$ 是一种更隐秘、更难以防范的网络层去匿名化方法，通过利用AS级互联网基础设施的权限监听节点的部分连接来去匿名化节点交易，成功克服了上述方法存在的问题。因此本部分将以 $Perimeter$ 为例，介绍针对加密货币网络层的交易溯源方法。3.1 $Perimeter$：网络层的去匿名化攻击$Perimeter$ 依赖的攻击向量是对互联网基础设施的访问。任何加密货币的连接都需要通过互联网路由，因而可被多个自治系统（AS）和互联网交换点（IXP）访问。由此，恶意的AS或IXP可以将其对互联网基础设施的访问与应用层信息相结合，执行跨层级的去匿名化攻击。这样的攻击是完全被动的，无需建立新连接，所以比一般攻击更加隐秘；同时，由于攻击者的能力依赖于互联网路由协议（BGP），而非应用层协议，所以也更难以防范。$Perimeter$ 由两个阶段组成：      首先，攻击者窃听受害者的连接，以收集受害者向其对等体传播的交易信息。        随后，攻击者分析收集到的信息以识别出受害者的交易。  在此过程中，攻击者不需要建立连接，而是通过直接读取每个数据包的有效负载来监听受害者的连接，因此以该攻击是不可检测的，且对于NAT节点等不接受连接的节点同样有效。受害者发起的交易通常会呈现出独特的传播模式，例如在比特币中，相较于转发交易来说，受害者生成的交易会被其发送给异常多的节点。依据这一特点，敌手可以通过异常检测来识别受害者发起的交易。3.1.1 概述攻击目标：攻击者的目标是去匿名化特定节点，即将受害节点的IP地址映射到其创建的交易。具体而言，攻击者的目标是得出一个交易集，其中尽可能多地包含受害节点创建的交易（即最大化真正例），并尽量减少包含其他交易的数量（即最小化假正例）。该交易集称为受害者的匿名集（anonymity set）。攻击者：攻击者控制一个自治系统（AS）或互联网交换点（IXP），具有拦截受害节点部分连接的能力，且知晓受害节点的IP地址。攻击过程：  窃听连接： 攻击者利用其AS或IXP的身份，通过截取受害者节点的连接，观察受害节点的交易传播情况。这个过程被称为“包围（surrounding）”，攻击者在受害者周围创建了一个逻辑圈，据此可获取传入和传出的信息。这是一个完全被动的过程，攻击者只是观察本来就要经其转发的流量，因此是不可检测的。  分析数据： 攻击者收集到受害者向其对等体传播的交易信息。随后，攻击者计算关于这些交易的统计信息，例如受害者或其对等体发送或接收交易的次数。  交易区分： 攻击者使用异常检测技术来区分受害者的交易。通过比较受害者的交易与正常行为的统计差异，识别受害者的交易。图18 Perimeter攻击示例示例：如图16a所示，网络由七个自治系统（AS0至AS6）组成，节点A至G运行比特币客户端。每对节点之间的流量都按照BGP计算的AS路径进行转发。AS2是攻击者，其目的是去匿名化Alice的交易。AS2拦截了节点A与节点B、C、D和E之间的连接，并知晓Alice节点的IP。因此，AS2的目标是将节点A映射到其生成的交易（在本例中是TX #33）。  AS2窃听受害者的连接，并从节点A传播的交易中创建了初始的匿名集：TX #15、TX #11、TX #35、TX #33。从应用角度来看，AS2在受害者周围创建了一个逻辑圈，如图16b所示。  AS2排除非受害者交易来减小匿名集的大小。由于比特币节点只接收它请求的交易，并且只会请求它还不知道的交易。因此，AS2可将TX #35从匿名集中排除，因为AS2观察到节点A从节点C接收了TX #35。对于TX #15和TX #11，由于AS2没有截取节点A与节点F和G之间的连接，因此不能使用此方法排除。  AS2使用异常检测来识别受害者的交易。例如，向节点A请求TX #33的节点数量明显高于其他交易，即TX #33传播模式异常，由此即可识别出TX #33是Alice发起的，从而达成去匿名化。3.1.2 原理区分比特币流量攻击者通过比特币客户端常用的TCP端口，即8333端口识别比特币流量。此外，攻击者还可以在数据包有效负载上搜索已知的比特币消息类型，例如“inv”或“getdata”，来识别比特币流量创建初始匿名集比特币消息可能被分割在多个数据包中，而这些数据包在传输时可能会被重新排序、丢失和重传，因此攻击者利用 GoPacket 等工具重组消息流。随后，将三类消息（即“inv”、“getdata”和“tx”）中包含的交易哈希加入匿名集，并对每个交易哈希统计“inv”、“getdata”和“tx”的发送和接收数量。分析数据攻击者的目标是将初始匿名集缩小到仅包含受害者创建的交易。两个主要挑战是：受害者节点传播的交易数远超其创建的交易数量、攻击者没有用于训练的基准数据。因此，攻击者将问题转化为一个无监督的异常检测问题，即在没有标签的数据集中识别与正常情况不同的数据点。利用受害者的交易相对于其传播的所有交易而言占比极小的特点，攻击者可以直接用其获取的流量进行训练，学习最常见的传播模式，从而将受害者的交易识别为异常。文章选用孤立森林（Isolation Forest，IF）进行异常检测。IF的核心思路是：异常更易于分离，在一般情况下划分异常点需要的分割次数比正常数据点少。IF通过随机选择特征和随机分割值生成一组决策树来对数据点进行分区，相较于基于距离的方法，如最近邻和基于聚类的方法，IF通过直接在数据中隔离异常值来识别异常，无需先定义正常行为再计算点的距离来区分异常，通过在处理高维数据时更为高效。而相对于神经网络方法，如自编码器，IF可解释性更强，且对参数调整不太敏感。选择特征比特币节点间的交易传播过程可以分为三个步骤，如图17所示：  拥有新交易的节点 A 通过仅包含交易哈希的 inv (inventory) 消息向其通知节点B。  如果节点 B 尚不知晓 inv 消息中包含的交易，它将通过 getdata 消息回复以请求整个交易。  节点 A 通过 tx 消息响应 getdata 消息发送完整的交易。收到完整交易后，节点B以相同的方式将交易传播给其邻节点C。图19 比特币交易传播$Perimeter$ 使用与时间和节点间交互相关的特征：“getdata”消息的数量、“tx”消息的数量、请求/宣传比。      受害者每个交易收到的“getdata”消息数量： 相当于受害者发送交易的次数。比特币客户端只会为其之前未收到过的交易发送“getdata”。因此，受害者创建的交易可能接收到更多的“getdata”，因为其对等体已从其他路径收到该交易的概率很低。        受害者每个交易收到的“tx”消息数量： 如果受害者从其对等体之一收到了一笔交易，则受害者不可能是其创建者。因为受害者接收了某交易，说明它已向其对等体请求了该交易，即受害者先前并不知晓该交易。受害者为一笔交易收到的“tx”消息数量等于受害者发送的“getdata”消息数量。        请求/宣传比：即接收到“inv”消息的客户端中返回“getdata”的比例，此特征类似于“getdata”的数量，关键的区别在于其考虑到了扩散机制可能导致受害者推迟向某些对等体宣传其交易，以至于其已从其他路径获知该交易。受害者的交易将具有较高的请求/宣传比，因为受害者比其对等体更早知晓交易。  3.1.3 效果文章阐述了$Perimeter$攻击在现实互联网环境下的可行性分析：分析方法：  收集比特币和以太坊节点的IP地址数据  基于BGP路由信息，建立自治系统级拓扑网络  模拟BGP路由计算不同的AS和IXP可以拦截的连接比例。图20 多数比特币客户端都易受到多个潜在攻击者的PERIMETER攻击对于35%的比特币客户端，至少有5个不同的攻击者可以拦截其30%的连接比特币网络的主要结果：  有50%以上的节点都能被至少4个不同的网络攻击者拦截30%的连接。  只需10个网络提供商(AS或IXP)合谋就能拦截85%的节点，潜在攻击者包括亚马逊、阿里巴巴、DigitalOcean、OVHcloud等大型云服务提供商，以及一些大型IXP。如果其共谋，可以去匿名化绝大部分的比特币交易。结论：  攻击模型与现实互联网条件是匹配的。  利用网络位置进行被动攻击是比较现实和可行的，比特币和以太坊网络面临真实的去匿名化威胁。文章在模拟环境和实际比特币网络中进行了实验，以评估$Perimeter$攻击的有效性。主要实验和结果如下：模拟实验：  使用现实世界互联网测量结果调整的参数建立了模拟的比特币网络。  模拟了10000个交易，其中100个是受害者节点生成的。  分别假定攻击者拦截受害者节点连接的25%、50%、75%和100%。  结果显示，即使只拦截25%的连接，攻击者也能以100%的真正例率和极低的假正例率去匿名化受害者节点。图21 模拟实验中，攻击者在拦截25%的连接时能以100%的准确率去匿名化受害者比特币主网实验：  使用受控的一个比特币节点作为受害者节点。  收集总共约30000个交易，其中10个是受害者节点生成。  同样测试拦截受害者节点连接的25%、50%、75%和100%。  结果显示，拦截50%连接时，即可以90%的真正例率和0.003%的假正例率成功去匿名化节点。图22 主网实验中，当攻击者拦截50%的连接时能以90%的准确率去匿名化受害者总的来说，实验验证了即使只能拦截部分连接，$Perimeter$攻击也能准确高效地对比特币节点进行去匿名化。证明了网络层的攻击者利用其互联网基础设施的有利位置对节点进行去匿名化是一种可行和实际的威胁。应对方式：几种可以应对$Perimeter$攻击的方法包括：  隐藏节点状态：即使已知一个交易，节点也主动向其他节点请求该交易，迷惑攻击者判断节点是否已知该交易。  考虑路由的交易请求：从不同路径的节点请求交易，避免攻击者关联请求。  考虑路由的交易广播：在Diffusion等机制中加入路由相关的额外延迟，增加攻击难度。  使用Tor或VPN：隐藏节点真实IP，但要考虑攻击者同时关联其他通信来识别节点用户。需要在多个层面采取上述综合措施，才能有效保护节点的匿名性和隐私性。总而言之，$Perimeter$ 作为一个新的攻击向量，能从网络层有效地对比特币交易进行去匿名化。相比其他网络层攻击方法，$Perimeter$更隐蔽更难以防范，因为此方法只是利用了网络基础设施的有利位置被动监听，不需要建立连接，也不依赖特定的传播机制，对Diffusion等各种防御手段仍然有效。同时$Perimeter$也能被推广到以太坊等其他加密货币中。4 总结本文从应用层和网络层两个维度出发，介绍了针对当前区块链平台的两大类去匿名化技术：应用层的身份识别和网络层的交易溯源。典型的应用层方法基于图神经网络，通过建模账户之间的复杂交互关系，学习账户的行为模式并由此推断其身份。网络层方法则利用网络基础设施的权限，被动监听节点连接，根据交易在节点间的传播规律推测源IP地址。在此基础上，笔者认为网络层和应用层去匿名化方法仍有进一步结合的可能性。例如在数据层面，网络层输出每个交易所对应的一组可能的源IP地址，而这可以作为应用层模型的一个新特征，加入到账户身份识别的输入中，提供附加的语义信息。例如，针对一个潜在的欺诈账户，如果通过网络层分析确定其交易来自某IP段，而此IP段又对应一些已知的欺诈集群，那么就可以增加该账户被判定为欺诈节点的概率或置信度。通过这种数据和语义上的补充，应用层识别的不确定性可以利用网络层的判定进行一定程度的修正，从而提高最终去匿名化的精确度和可靠性。当前，区块链去匿名化技术仍存在一定的争议。去匿名化在合法合规、防范非法活动等场景下有着积极意义，且有助于揭示区块链网络存在的潜在风险、提升系统的安全性。但同时，此类技术也可能涉及对用户隐私的侵犯，并一定程度上引入了中心化风险。因此，去匿名化技术的使用需要根据具体情况进行审慎的权衡和考虑。"
  },
  
  {
    "title": "以太坊数据结构详解",
    "url": "/posts/ethds/",
    "categories": "Blockchain, Ethereum",
    "tags": "ethereum",
    "date": "2023-09-19 12:00:00 +0800",
    





    
    "snippet": "以太坊的实现中涉及了多种数据结构和编码规则，全面深入地理解这些内容对研究者和开发者来说至关重要。目前，该领域的主要信息来源是以太坊黄皮书、以太坊开发者文档以及网络上大量的非正式博客文章。黄皮书内容全面，但难称详略得当，易读性较低，且省略了智能合约数据的结构和存储信息。以太坊开发者文档的内容组织散乱，依赖众多外部博客和视频链接的补充完善。以太坊创始人 Antonopoulos 和 Wood 出...",
    "content": "以太坊的实现中涉及了多种数据结构和编码规则，全面深入地理解这些内容对研究者和开发者来说至关重要。目前，该领域的主要信息来源是以太坊黄皮书、以太坊开发者文档以及网络上大量的非正式博客文章。黄皮书内容全面，但难称详略得当，易读性较低，且省略了智能合约数据的结构和存储信息。以太坊开发者文档的内容组织散乱，依赖众多外部博客和视频链接的补充完善。以太坊创始人 Antonopoulos 和 Wood 出版了 Mastering Ethereum 一书，但该书主要面向终端用户，并未深入讲解以太坊内部的技术细节。为弥补相关信息的空缺，本文将对以太坊数据结构进行汇总，目标是在保持简洁性的同时涵盖足够的细节，在一定的形式化视角基础上，通过实例和插图提升内容的可读性。本文将首先介绍相关的基本数据结构，随后阐述其扩展形式 Merkle Patricia Trie 的结构和编码规则。之后是对区块的结构概述，以及对数据表示中 Trie 结构的详细介绍。基础数据结构Patricia Trie高效搜索的标准方法之一是使用树 (Tree) 结构，树有多种形式，其中最为常见的是二叉搜索树。二叉搜索树可以构成前缀树 (Prefix Tree)，在这种树中，具有相同前缀的键会共享从根节点到叶节点的路径。在前缀树中搜索键时，依次按顺序逐个检查字符，确认树中是否存在与当前字符匹配的子节点。如果存在，则向下遍历到该子节点，然后继续匹配下一个字符，重复这一过程。图 3 中左侧是用于二进制编码的前缀树示例。如图例所示，该树编码了四个二进制键，从根节点到叶节点形成了四条路径。搜索树的缺点主要体现在其遍历效率上，在处理长公共前缀的键时，必须遍历多个子节点，特定情况下树会退化为链表，使查找时间复杂度从 $O(log⁡n)$ 增加到 $O(n)$。为了解决该问题，Fredkin 提出了 Trie Memory1。他将数据表示定义为表格形式，其中表的列代表节点，行代表分支。行数取决于分支数，例如，表示二进制数据需要两行（0 和 1），表示十六进制字符串则需要十六行（0 到 F）。列数则相当于将数据表示为树时的深度。该表的每格可包含对子节点的引用或字符串。引用子节点用目标列号表示。若存储字符串，则相当于键的后缀，终止当前路径。  Trie 这一名称源于其用途：信息检索 (Information Retrieval)查找过程从逐字符遍历输入键开始。首先，在当前字符所在的行中查找。如果该行包含一个字符串，则路径终止，键已找到。如果该单元格包含指向另一个节点（即另一列）的链接，则该列包含所有编码键的所有可能分支。为了找到特定的键，从输入键中取出下一个字符，并找到与该字符匹配的行。这一行和当前列一起确定一个表格单元格。该单元格再次被分析，算法递归进行。如果单元格为空（即既不包含下一列的链接也不包含字符串），则表示输入键没有在表中编码。图 1 是 Memory Trie 的示例，其中编码了四个十六进制的键。所有键的前四个字符相同，因此前四列引用了下一列。随后出现分支，即列 5 包含了对列 6、7、8 的引用。列 5 本身表示键的下一个字符所对应的行——行 0、2、F 对应编码键的第五个字符。列 6、7、8 包含了键的后缀，路径终止。图1 Memory Trie 示例Trie 在速度上很高效，因为可表示为二维数组，从而通过索引实现快速访问。但当存储的键值对分布稀疏时，Trie 会产生较大的内存开销，导致大量的空间浪费。另外，原始 Trie 结构只能加速查找以相同前缀开头的键，或通过逆序存储来处理键的公共后缀，但无法解决键在中间部分有相同路径的情况。Morrison 在论文2中提出的 Patricia Trie 解决了这一缺陷。Patricia Trie 对原始 Trie 结构进行了压缩优化，每个节点不仅存储路径信息，还记录了可以在匹配键时跳过的位数。在原始 Trie 中，键的公共前缀部分会通过多个节点表示，而在 Patricia Trie 中，这些公共路径会被压缩成一个单一节点。由此 Patricia Trie 减少了不必要的节点存储，在加速查找过程的同时，减少了内存开销。图 3 右侧展示了 Patricia Trie 的例子，数据集中的所有键都以“11”开头，因此可以跳过两位，从键的第三位开始进行分支。值得注意的是，Patricia Trie 最初是为二叉树提出的，图 2 将图 1 中的示例表示为了 Patricia 格式，其中元组‘4(5)’表示在访问列 5 之前，已经跳过了四列。图2 Memory Patricia Trie 示例Merkle TreeRalph Merkle 提出的 Merkle Tree 3是一种用于快速验证数据集一致性的密码学数据结构。其核心特征是每个节点包含其子节点的哈希值，叶子节点则链接到编码数据集的实际值。该结构通常采用非对称哈希函数来保护原始数据。Merkle Tree 的核心价值在于允许各方在不交换完整数据集的情况下验证数据一致性，这在分布式环境中尤为重要，因为直接比对大量复制数据往往不切实际。Merkle Tree 为数据集提供了唯一的哈希标识，有效防止了恶意或无意的数据修改。参与者可以通过比对哈希值来验证数据的完整性和一致性。由于每个子集都被哈希化，参与者可以仅交换子哈希，无需处理完整数据集。任何节点的篡改都会导致从该节点到根节点的哈希链不匹配，从而暴露错误。在区块链这样的分布式无信任环境中，Merkle Tree 提供了理想的解决方案。它不仅提供了基于哈希的有效性证明，而且哈希值的小体积特性使其非常适合在互联网环境中传输，有效解决了区块链中数据验证和同步的关键挑战。图 3 中间是 Merkle Tree 的示例。节点自下而上均被哈希化，叶子节点仅对其自身进行哈希，而所有其余的父节点递归地对其子节点的哈希进行哈希。当需要在各方之间验证数据一致性时，可以验证任何节点的哈希值。值得注意的是，Merkle Tree 在最初设计中仅包含哈希值，实际数据存储在外部。以太坊进一步创新，提出了将哈希和数据统一封装在树结构中的方法，这一部分将在本文后续章节详细阐述。图3 Trie, Merkle Tree, Patricia TrieMerkle Patricia TrieMerkle Patricia Trie (MPT) 是融合并扩展了 Patricia Trie 的高效存储和 Merkle Tree 的防篡改验证特性的数据结构4。该结构巧妙地将主键的公共路径组合在单一节点中，同时以 Merkle 证明的方式对每个节点进行哈希处理，实现了数据的高效组织和安全验证。以太坊开发者在设计 Merkle Patricia Trie 时，考虑到了数据持久化问题。虽然黄皮书中提到“对存储的数据没有明确假设”，但随后又指出“实现将维护节点数据库”。这种设计思路表明 MPT 不仅是数据结构，也是数据库架构。这种设计简化了非结构化数据库（如 Geth 使用的 LevelDB）中结构化数据的存储，但同时也带来了一些挑战。研究表明，以太坊面临存储膨胀问题5，且由于持续计算哈希值导致读写放大，性能也有所下降6。MPT 的独特之处在于允许将键的公共部分和后缀组合在一个节点中，仅在必要时进行分支。这种设计借鉴了 Patricia Trie 的思想，但扩展为了 16 个分支，并采用十六进制字符串代替位进行存储。与传统 Patricia Trie 不同，键的公共部分直接存储在节点中，而非仅存储跳过的位数。为实现 Merkle 证明，当 MPT 引用子节点时，引用的是子节点的哈希值。这种创新结构不仅提高了数据存储和检索的效率，还增强了数据的安全性和完整性，为区块链技术中的数据管理提供了强有力的支持。Merkle Patricia Trie 定义了三种类型的节点，结构如下：  分支节点 (Branch)：包含 17 项 $[𝑖_0, 𝑖_1, …, 𝑖_{15}, 𝑣𝑎𝑙𝑢𝑒]$，用于处理键分叉，前 16 项对应十六进制字符 0-F，允许最多 16 个可能的分支。此设计类似于 Memory Trie 结构中通过表格列构建分支的方式。第 17 项用于存储值，仅在该节点是某个键的终止节点时使用。  扩展节点 (Extension)：包含 2 项 $[𝑝𝑎𝑡ℎ, 𝑣𝑎𝑙𝑢𝑒]$，实现了路径压缩功能。当多个键共享公共前缀时，这部分前缀被存储在扩展节点中，避免了沿唯一路径的重复遍历。这一特性类似于 Patricia Trie  中跳过特定位数的机制。  叶节点 (Leaf)：包含 2 项 $[𝑝𝑎𝑡ℎ, 𝑣𝑎𝑙𝑢𝑒]$，标志着树中路径的终点。同样采用压缩方法，将键的公共后缀在 $𝑝𝑎𝑡ℎ$ 中存储，借鉴了 Memory Trie 的特性。图 4 是 Merkle Patricia Trie 的示例，其中键以十六进制数表示。所有键共享前缀“1111”，因此根节点被创建为扩展节点以分组此前缀。随后的分支节点在字符‘0’、‘2’和‘F’处处理键的分叉。键“1111”在此终止，其值存储在分支节点中。其余键形成叶节点，存储各自的后缀和对应值。值得注意的是，实际实现中，父节点到子节点的每个链接都是通过子节点的哈希 $hash(node)$ 表示来引用的。但 MPT 结构中的节点是多元素（结构化）记录，而哈希函数是对二进制字符串计算的。因此必须将结构化的节点编码为字节数组，以便计算哈希值。这一过程将在后续章节详细阐述。图4 Merkle Patricia Trie 示例数据结构编码Trie 结构的编码是将结构化节点转换为适合哈希计算和持久化存储的字节数组的过程。这一过程主要包括两个步骤：十六进制前缀编码（Hex Prefix Encoding, HP）和递归长度前缀编码（Recursive Length Prefix, RLP）。十六进制前缀编码 HP在 Merkle-Patricia Trie 中，存储的数据被编码为字节序列。叶节点和扩展节点使用 HP 编码函数进行处理。HP 编码在以太坊黄皮书中有形式化定义，其核心目的是将任意数量的半字节 (nibble) 编码为二进制流，使两个 4 位的半字节组成一个 8 位的字节，即半字节被编码为序列：$\\lbrace 16𝑥_𝑖 + 𝑥_{𝑖+1} \\lvert 𝑖 ∈ [0,\\ldots,\\lVert𝑥\\rVert−1) \\rbrace$HP编码的特点包括：  添加前缀半字节：这个半字节的倒数第二位用于区分叶节点（1）和扩展节点（0）；最低位则表示编码数据长度的奇偶性（偶数为0，奇数为1）。  长度调整：为确保解码时能准确重建原始值，存储的半字节数量必须为偶数。因为标准计算机无法仅存储半个字节。如果原始半字节数为奇数，在前缀半字节后插入一个值为‘0’的半字节作为填充。若为偶数，则直接在前缀半字节后继续编码路径。如下表所示：            Hex      Bits      Node Type      Path Length                  0      0000      Extension      Even              1      0001      Extension      Odd              2      0010      Leaf      Even              3      0011      Leaf      Odd      如下例所示，括号表示包含两个半字节的字节，为偶数长度数据添加了一个填充零作为第二个半字节，而奇数长度数据则紧跟在第一个半字节之后：            Prefix      Payload      Node Type      Path Length                  $[0,0]$      $[x_1,x_2][x_3,x_4]$      Extension      Even              $[0,x_1]$      $[x_2,x_3][x_4,x_5]$      Extension      Odd              $[2,0]$      $[x_1,x_2][x_3,x_4]$      Leaf      Even              $[3,x_1]$      $[x_2,x_3][x_4,x_5]$      Leaf      Odd      以下两例分别展示了用 HP 编码的偶数和奇数长度字符串。例 1 编码了奇数个字符，因此前缀为‘1’或‘3’，后面紧跟有效负载，两个字符存储在一个字节中。例 2 包含偶数个字符，因此前缀为‘0’或‘2’，后跟附加的填充‘0’，然后是有效负载。      $[5, 6, 7, 8, 9] → [15, 67, 89] ∨ [35, 67, 89]$        $[4, 5, 6, 7, 8, 9] → [00, 45, 67, 89] ∨ [20, 45, 67, 89]$  递归长度前缀编码 RLP在Merkle Patricia Trie中，节点结构经过 HP 编码后被压缩为字节数组，但仍保留多数组结构。为了实现高效的持久化存储和哈希计算，需要将这些多维数组结构进一步扁平化为单一字节数组。以太坊黄皮书中定义了递归长度前缀函数来解决这个问题。RLP 函数的核心功能是将输入的一组数组序列化为一个扁平的字节数组，本质是将所有子数组转换为一个长数组，在每个原始子数组的序列前添加其长度信息和一个位掩码，用于标识该子数组原本是数组还是字符串。展平结构的每一级开始处会添加所有子数组的总长度信息。RLP 根据数据类型和长度，采用不同的编码策略，以实现最优的空间利用，主要规则如下：      单字节值（≤ 127），直接存储，无需修改。对字节的 $b$：\\[RLP: b \\rightarrow b\\]        短字符串（≤ 55字节），对字符串 $s$：\\[RLP: s \\rightarrow [\\text{0x80} + \\lVert s \\rVert, s]\\]    其中，$\\lVert𝑠\\rVert$ 表示字符串 $𝑠$ 的长度，字符串本身紧跟在此前缀之后。可以看到，RLP 对短字符串的编码非常高效，只需要一个额外的字节来存储前缀和字符串长度。        短数组（总长度 ≤ 55字节），对一组数组 $S$：\\[RLP: S \\rightarrow [\\text{0xc0} + \\lVert ex(S) \\rVert, ex(S)]\\]    其中，函数 $𝑒𝑥$ 对集合 $𝑆$ 中的每个数组进行 RLP 编码：\\[ex: S \\rightarrow (RLP(s_i) | s_i \\in S)\\]    这种数组的编码也非常高效，仅需要一个字节来存储前缀和总长度。        长字符串（&lt; 2^64 字节），对字符串 $s$：\\[RLP: s \\rightarrow [\\text{0xb7} + numB(\\lVert s \\rVert), \\lVert s \\rVert, s]\\]    函数 $\\text{𝑛𝑢𝑚𝐵}:𝑁 \\rightarrow 𝑁$ 计算存储输入值所需的字节数。例如，长度小于 0xFF 的字符串需要一个字节来表示其长度，长度在 0xFF 到 0xFFFF 之间的需要两个字节等。这种情况下 RLP 在表示字符串长度时需要更多字节，但如果字符串不是特别长，该函数仍然较为高效。例如，长度为 0xFFFF 的字符串仅需三个额外字节来编码（1 个用于前缀，2 个用于长度）。        长数组（&lt; 2^64 字节），对一组数组 $S$：\\[RLP: S \\rightarrow [\\text{0xf7} + numB(\\lVert ex(S) \\rVert), \\lVert ex(S) \\rVert, ex(S)]\\]    对于长嵌套数组，RLP 函数首先添加前缀，接着是表示长度所需的字节数，之后是长度本身，最后是递归 RLP 编码的有效负载。        超长负载（≥ 2^64 字节）：不支持编码。  下面通过四个示例来说明RLP函数的应用：      对简单字符串“ABCD”进行编码，使用前缀 0x80 加上字符串的长度 4：\\[ABCD \\rightarrow [\\text{0x80} + 4, A, B, C, D]\\]        对包含“AB”和“CDE”的两个数组进行编码，使用前缀 0xc0 加上剩余编码负载的长度：\\[[AB] [CDE] \\rightarrow [\\text{0xc0} + 7, \\text{0x80} + 2, A, B, \\text{0x80} + 3, C, D, E]\\]        对长字符串进行编码。假设有长度为 300 个字符的字符串，缩写为“A ··· B”。为了表示长度，需要用十六进制形式的 0x12C ≡ 300。该数值需要两个字节，使用长字符串的前缀 0xb7 并加上 2，随后存储表示长度的两个字节，最后是字符串本身：\\[A ··· B \\rightarrow [\\text{0xb7} + 2, \\text{0x1}, \\text{0x2C}, A, \\cdots , B]\\]        递归地存储两个数组 [A ··· B] [ABCD]，其中一个包含长字符串，另一个包含短字符串。首先自底向上编码，应用前面示例中长字符串和短字符串的两条规则，创建两个字节序列 [0xb7 + 2, 0x1, 0x2C, A, ··· , B] 和 [0x80 + 4, A, B, C, D]。接下来先使用长数组的前缀：0xf7，并计算新的有效负载的长度，0x134 ≡ 308，该数字仍需两个字节存储。因此最终的编码包括前缀 0xf7 加上 2，字节 0x1, 0x34 以及两个已编码的数组，即：  \\[\\begin{align}   [A ··· B] [ABCD] \\rightarrow &amp;[\\text{0xf7} + 2, \\text{0x1}, \\text{0x34},\\notag \\\\   &amp;\\text{0xb7} + 2, \\text{0x1}, \\text{0x2C}, A, \\cdots , B, \\notag \\\\   &amp;\\text{0x80} + 4, A, B, C, D] \\notag  \\end{align}\\]RLP 编码的优势在于其高效性和灵活性。与 JSON 等文本标记语言相比，RLP 仅需少量前缀字节就能表示复杂的数据结构，大大降低了存储和传输开销。此编码方法是 Merkle Patricia Trie 中实现 Merkle 证明的基础。通过将节点转换为统一的字节数组格式，RLP 为后续的哈希计算提供了标准化的输入，从而支持了区块链中的数据完整性验证机制。图5 以太坊编码 Merkle Patricia Trie 示例图 5 中的示例是对图 4 的补充，展示了在以太坊中使用的节点编码。图中，扩展节点和叶子节点中的路径首先进行 HP 编码，随后各个节点通过 RLP 编码转换为字节数组。由此即可对整个节点进行哈希处理。父节点通过哈希值引用其子节点，从而为每个节点添加了 Merkle 证明。键值存储持久化Trie 节点经 HP 和 RLP 编码后转化为字节数组，适合存储于键值数据库。为解决哈希值无法揭示原始数据的问题，以太坊同时存储了节点的哈希值和 RLP 编码值，其中哈希函数采用 Keccak 函数。每个节点在数据库中存储为一个$𝑘𝑒𝑦 → 𝑣𝑎𝑙𝑢𝑒$对，存储格式为：\\[keccak(RLP(node)) → RLP(node)\\]其中 $𝑘𝑒𝑦 ≡ 𝑘𝑒𝑐𝑐𝑎𝑘(𝑅𝐿𝑃(𝑛𝑜𝑑𝑒))$ ，$𝑣𝑎𝑙𝑢𝑒 ≡ 𝑅𝐿𝑃(𝑛𝑜𝑑𝑒)$。每个节点的内容根据节点类型而有所不同：  扩展节点 $≡ [𝐻𝑃(𝑝𝑟𝑒𝑓𝑖𝑥 + 𝑝𝑎𝑡ℎ), 𝑘𝑒𝑦]$  分支节点 $≡ [𝑏𝑟𝑎𝑛𝑐ℎ𝑒𝑠, 𝑣𝑎𝑙𝑢𝑒]$  叶节点 $≡ [𝐻𝑃(𝑝𝑟𝑒𝑓𝑖𝑥 + 𝑝𝑎𝑡ℎ), 𝑣𝑎𝑙𝑢𝑒]$其中，扩展节点中的 $𝑘𝑒𝑦$ 是子节点的哈希键。在分支节点中， $braches= (𝑘𝑒𝑦_𝑖 \\vert 0 ≤ 𝑖 ≤ 15)$，如果分支存在，$𝑘𝑒𝑦_𝑖$ 为子节点的哈希键，否则为空。对于这两种节点类型，键通过已提到的公式计算：$𝑘𝑒𝑦 = 𝑘𝑒𝑐𝑐𝑎𝑘(𝑅𝐿𝑃(𝑛𝑜𝑑𝑒_𝑐))$，$𝑛𝑜𝑑𝑒_𝑐$ 为子节点。需要注意的是，键的哈希表示是基于其 RLP 表示，从整个节点计算出来的。因为节点包含了子节点的哈希（除非是叶子节点），由此就构建起了数据结构的 Merkle 部分，即当前节点的哈希基于子节点的哈希计算而来。另外，以太坊还使用了节点内联优化。如果子节点的 RLP 编码小于 32 字节，则不计算其哈希，而是将此节点直接存储在父节点内。此技术可以进一步压缩短节点的数据结构。简单序列化编码 SSZSSZ (Simple Serialize) 是为以太坊 2.0 专门设计的序列化方法。其开发始于 2018 年，是以太坊研究团队为解决以太坊 1.0 中使用的 RLP 编码存在的一些限制而提出的。  SSZ 使用固定偏移量，使得在解码消息的个别部分时无需解码整个结构，这对共识客户端非常有用，令其可以高效地从编码消息中获取特定信息。SSZ 还专门设计为能够与 Merkle 协议集成，从而在 Merkle 化过程中带来相关的效率提升。由于共识层中的所有哈希值都是 Merkle 根，这种设计带来了显著的改进。此外，SSZ 还保证了数值的唯一表示形式。目前，以太坊执行层客户端数据的序列化仍使用 RLP 编码，而共识层客户端以及 P2P 网络通信数据的序列化则使用 SSZ 编码，在以太坊 consensus-specs 仓库中维护相关规范。为了使 SSZ 在共识和通信中都能发挥作用，给定类型 $T$ 的对象 $O1$ 和 $O2$，SSZ 应满足：  对合性：$deserialise⟨T⟩(serialise⟨T⟩(O1)) = O1$（通信所需）  单射性：$serialise⟨T⟩(O1) = serialise⟨T⟩(O2)$ 则 $O1 = O2$（共识所需）属性 1 表示：当序列化某一类型的对象，然后再对结果反序列化时，得到的对象与开始时的对象完全相同。此属性是通信协议所必需的。属性 2 表示：如果序列化同一类型的两个对象并得到相同的结果，那么这两个对象是相同的。换言之，两个不同的同类型对象，其序列化结果一定不同。此属性是共识协议所必需的。除了这些基本功能要求外，SSZ 的目标相对简单：生成紧凑的序列化结果，并与 Merkle 化兼容，以及能够在不反序列化整个对象的情况下快速访问序列化中的特定数据位。与 RLP 不同，SSZ 不是自描述的。RLP 数据可被解码成一个结构化对象，而无需预先了解该对象的具体形态，而 SSZ 则必须事先明确知道正在反序列化的对象类型。SSZ 中定义了以下基本类型：      基本类型：          无符号整数：uint8, uint16, uint32, uint64, uint128, uint256              字节：byte，8 位不透明数据容器，在序列化和哈希时等同于 uint8            布尔值：boolean，取值为 True 或 False            复合类型：                  向量：Vector[T, N]，有序的、固定长度的同质集合，其中 T 是任意 SSZ 类型，N 是元素个数，例如 Vector[uint64, N]                    列表：List[T, N]，有序的、可变长度的同质集合，其中 T 是任意 SSZ 类型，N 是最大长度，例如 List[uint64, N]                    容器：Container[f1: T1, f2: T2, ..., fn: Tn]，有序的异质集合（包含多种类型的值），其中 Ti 是任意 SSZ 类型，fi 是字段名        例如：        class ContainerExample(Container):    foo: uint64    bar: boolean                            联合：Union[T1, T2, ..., Tn]，包含给定子类型之一的联合类型，其中 Ti 是任意 SSZ 类型，例如 Union[None, uint64, uint32]                    位向量：Bitvector[N]，有序的、固定长度的布尔值集合，其中 N 是包含位数                    位列表：Bitlist[N]，有序的、可变长度的布尔值集合，其中 N 是最大长度            SSZ 序列化的目标是将任意复杂的对象表示为字节字符串。基本类型序列化非常简单，元素只需转换为十六进制字节即可。而对于复合类型，序列化过程则更为复杂，因为复合类型包含多个可能具有不同类型或不同大小的元素，当元素都具有固定长度时（即无论其实际值如何，元素的大小始终不变），序列化过程只是将复合类型中的每个元素按顺序转换为小端字节串，然后将其进行拼接。序列化后的对象以与反序列化对象中出现的顺序相同的方式表示固定长度元素的字节列表。对于具有可变长度的类型，实际数据会在序列化对象中被其位置的“偏移量 (offset) ”所替代。实际数据会被添加到序列化对象末尾的堆中。偏移量值是实际数据在堆中的起始索引，起到指向相关字节的指针作用。可参考图 6。图6 SSZ 序列化编码图解 | 图源：protolambda以下通过实例详细说明 SSZ 序列化，数据来自以太坊信标链上 Slot 3080831 Attestation 87 对应的 IndexedAttestation ：IndexedAttestation 容器如下：class IndexedAttestation(Container):    attesting_indices: List[ValidatorIndex, MAX_VALIDATORS_PER_COMMITTEE]    data: AttestationData    signature: BLSSignature里面包含 AttestationData 容器：class AttestationData(Container):    slot: Slot    index: CommitteeIndex    beacon_block_root: Root    source: Checkpoint    target: Checkpoint其中又包含两个 Checkpoint 容器：class Checkpoint(Container):    epoch: Epoch    root: Root使用相关信息构建 IndexedAttestation 对象，并计算其 SSZ 序列化：attestation = IndexedAttestation(    attesting_indices = [33652, 59750, 92360],    data = AttestationData(        slot = 3080829,        index = 9,        beacon_block_root = '0x4f4250c05956f5c2b87129cf7372f14dd576fc152543bf7042e963196b843fe6',        source = Checkpoint (            epoch = 96274,            root = '0xd24639f2e661bc1adcbe7157280776cf76670fff0fee0691f146ab827f4f1ade'        ),        target = Checkpoint(            epoch = 96275,            root = '0x9bcd31881817ddeab686f878c8619d664e8bfa4f8948707cba5bc25c8d74915d'        )    ),    signature = '0xaaf504503ff15ae86723c906b4b6bac91ad728e4431aea3be2e8e3acc888d8af'                + '5dffbbcf53b234ea8e3fde67fbb09120027335ec63cf23f0213cc439e8d1b856'                + 'c2ddfc1a78ed3326fb9b4fe333af4ad3702159dbf9caeb1a4633b752991ac437')print(attestation.encode_bytes().hex())此对象序列化结果为：e40000007d022f000000000009000000000000004f4250c05956f5c2b87129cf7372f14dd576fc152543bf7042e963196b843fe61278010000000000d24639f2e661bc1adcbe7157280776cf76670fff0fee0691f146ab827f4f1ade13780100000000009bcd31881817ddeab686f878c8619d664e8bfa4f8948707cba5bc25c8d74915daaf504503ff15ae86723c906b4b6bac91ad728e4431aea3be2e8e3acc888d8af5dffbbcf53b234ea8e3fde67fbb09120027335ec63cf23f0213cc439e8d1b856c2ddfc1a78ed3326fb9b4fe333af4ad3702159dbf9caeb1a4633b752991ac437748300000000000066e9000000000000c868010000000000整理序列化结果如下：第一列是距字节串开头的字节偏移量。在每一行前标出了对应的数据结构部分，并将类型别名转换为其底层 SSZ 类型。注意整数类型是小端字节序，如 7d022f0000000000是十六进制数 0x2f027d ，十进制为 3080829。Part 1 (固定长度元素)      指向 attestation.attesting_indices 实际数据开头位置 0xe4 的 4 字节偏移量00 e4000000   attestation.data.slot: Slot / uint6404 7d022f0000000000   attestation.data.index: CommitteeIndex / uint640c 0900000000000000   attestation.data.beacon_block_root: Root / Bytes32 / Vector[uint8, 32]14 4f4250c05956f5c2b87129cf7372f14dd576fc152543bf7042e963196b843fe6   attestation.data.source.epoch: Epoch / uint6434 1278010000000000   attestation.data.source.root: Root / Bytes32 / Vector[uint8, 32]3c d24639f2e661bc1adcbe7157280776cf76670fff0fee0691f146ab827f4f1ade   attestation.data.target.epoch: Epoch / uint645c 1378010000000000   attestation.data.target.root: Root / Bytes32 / Vector[uint8, 32]64 9bcd31881817ddeab686f878c8619d664e8bfa4f8948707cba5bc25c8d74915d   attestation.signature: BLSSignature / Bytes96 / Vector[uint8, 96]84 aaf504503ff15ae86723c906b4b6bac91ad728e4431aea3be2e8e3acc888d8afa4 5dffbbcf53b234ea8e3fde67fbb09120027335ec63cf23f0213cc439e8d1b856c4 c2ddfc1a78ed3326fb9b4fe333af4ad3702159dbf9caeb1a4633b752991ac437Part 2 (可变长度元素)      attestation.attesting_indices: List[uint64, MAX_VALIDATORS_PER_COMMITTEE]e4 748300000000000066e9000000000000c868010000000000上例中，attesting_indices 列表是一个可变长度的集合，因此在 Part 1 中被表示为一个偏移量，该偏移量指向实际数据的位置，即从序列化数据的起始位置算起的第 0xe4 字节（第 228 字节）。列表的实际长度可以通过整个字节串的长度（252 字节）减去列表的起始位置（228 字节）再除以 8 字节来计算，由此就得到了三个验证者索引的列表。所有剩余的项都是固定长度的，均被直接编码，包括递归编码的固定长度的 AttestationData 对象及其固定长度的 Checkpoint 子对象。图7 IndexedAttestation 容器的序列化区块及交易区块链本质上是一个分布式状态机。状态机是一个由一组状态和状态之间的转移规则所组成的系统，系统的状态随着输入的变化而改变。区块在这个系统中扮演着状态转换的角色。每个区块包含了一定时间段内所有已确认的交易数据以及前一区块的哈希值，由此形成了链式结构。每当一个新区块被添加到链上，区块链的全网状态便随着区块中所包含的交易得以更新。而交易是状态机输入的具体表现形式，是触发状态变更的基本单元，每笔交易描述了特定的状态转换操作，如资产转移或合约调用。交易的执行推动着全局状态的演进。图8 以太坊的状态转换 | 图源：以太坊开发者文档区块链的整体思想是共通的，但各个区块链协议组织和存储数据的方式可能不同。以太坊使用的通用数据结构是 Merkle Patricia Trie。整个网络的当前状态（如账户余额、智能合约的存储数据）、交易数据等都被保存在相应的 Trie 结构中。每个区块的区块头中都包含指向各种 Merkle Patricia Trie 的链接。后续将详细介绍各种基于 Trie 的结构，这里主要对区块以及执行负载的结构进行简要介绍。图9 The Merge 前后的区块结构对比如图 9 所示，以太坊完成 The Merge 后，共识机制由工作量证明 (PoW) 转变为权益证明 (PoS)，区块结构也相应地发生了变化。以太坊 Bellatrix 升级中为信标链引入了新容器 ExecutionPayload 和 ExecutionPayloadHeader，PoW 区块中的大部分内容作为执行负载被添加进了 PoS 区块中，与共识层内容集成。当前以太坊的区块结构如下：class BeaconBlock(Container):    slot: Slot    proposer_index: ValidatorIndex    parent_root: Root    state_root: Root    body: BeaconBlockBody            Field      Description                  slot      区块所属的 Slot              proposer_index      选中提议该区块的验证者 ID              parent_root      前一个信标区块的哈希              state_root      状态对象的根哈希              body      信标区块体，包含多个字段      其中，信标区块体结构如下：class BeaconBlockBody(Container):    randao_reveal: BLSSignature    eth1_data: Eth1Data  # Eth1 data vote    graffiti: Bytes32  # Arbitrary data    # Operations    proposer_slashings: List[ProposerSlashing, MAX_PROPOSER_SLASHINGS]    attester_slashings: List[AttesterSlashing, MAX_ATTESTER_SLASHINGS]    attestations: List[Attestation, MAX_ATTESTATIONS]    deposits: List[Deposit, MAX_DEPOSITS]    voluntary_exits: List[SignedVoluntaryExit, MAX_VOLUNTARY_EXITS]    sync_aggregate: SyncAggregate    # Execution    execution_payload: ExecutionPayload  # [New in Bellatrix] [Modified in Deneb:EIP4844]    # Capella operations    bls_to_execution_changes: List[SignedBLSToExecutionChange, MAX_BLS_TO_EXECUTION_CHANGES]  # [New in Capella]    blob_kzg_commitments: List[KZGCommitment, MAX_BLOB_COMMITMENTS_PER_BLOCK]  # [New in Deneb:EIP4844]            Field      Description                  randao_reveal      由区块提议者提供的 BLS 签名，用于选出下一区块提议者              eth1_data      有关存款合约的信息              graffiti      任意数据字段，通常用于标记区块              proposer_slashings      将被罚没的提议者列表，包含对提议者不当行为的证明              attester_slashings      将被罚没的验证者列表，包含对验证者不当行为的证明              attestations      验证者对当前区块的投票              deposits      新的验证者存款信息              voluntary_exits      验证者自愿退出的申请              sync_aggregate      服务轻客户端的验证者子集              execution_payload      包含执行层的完整信息 【Bellatrix 升级引入】【Deneb 升级扩展】              bls_to_execution_changes      验证者更改提款凭证的请求 【Capella 升级引入】              blob_kzg_commitments      对执行负载中 blob 数据的承诺 【Deneb 升级引入】      本文主要介绍以太坊的数据结构，主要关注执行层相关内容，因此不对共识层相关内容过多赘述。execution_payload 包含执行层的完整信息，其结构如下：class ExecutionPayload(Container):    # Execution block header fields    parent_hash: Hash32    fee_recipient: ExecutionAddress  # 'beneficiary' in the yellow paper    state_root: Bytes32    receipts_root: Bytes32    logs_bloom: ByteVector[BYTES_PER_LOGS_BLOOM]    prev_randao: Bytes32  # 'difficulty' in the yellow paper    block_number: uint64  # 'number' in the yellow paper    gas_limit: uint64    gas_used: uint64    timestamp: uint64    extra_data: ByteList[MAX_EXTRA_DATA_BYTES]    base_fee_per_gas: uint256    # Extra payload fields    block_hash: Hash32  # Hash of execution block    transactions: List[Transaction, MAX_TRANSACTIONS_PER_PAYLOAD]    withdrawals: List[Withdrawal, MAX_WITHDRAWALS_PER_PAYLOAD]  # [New in Capella]    blob_gas_used: uint64  # [New in Deneb:EIP4844]    excess_blob_gas: uint64  # [New in Deneb:EIP4844]            Field      Description                  parent_hash      前一区块的哈希值              fee_recipient      接收交易费用的地址              state_root      执行此区块后全局状态的根哈希              receipts_root      交易收据树的哈希值              logs_bloom      用于快速检索日志的布隆过滤器              prev_randao      替代了mixHash字段，用于验证者随机选择的值 【EIP-4399 引入】              block_number      当前区块的编号              gas_limit      该区块的 gas 上限              gas_used      该区块实际使用的 gas 量              timestamp      区块创建的 Unix 时间戳              extra_data      原始字节形式的任意附加数据              base_fee_per_gas      每单位 gas 的基础费用              block_hash      整个执行负载的哈希，不包括block_hash字段本身              transactions      待执行交易列表              withdrawals      提款对象列表 【EIP-4895 引入】              blob_gas_used      该区块中的交易消耗的 blob gas 总量 【EIP-4844 引入】              excess_blob_gas      当前区块链上 blob gas 使用量与预设目标的差值 【EIP-4844 引入】      巴黎升级中，根据 EIP-3675，原本 PoW 区块中的 ommersHash、difficulty、mixHash、nonce、ommers 等在 PoS 机制下无作用或无意义的字段被置为特定常量：            Field      Constant value      Comment                  ommersHash      0x1dcc4de8dec75d7aab85b567b6ccd41ad312451b948a7413f0a142fd40d49347      = Keccak256(RLP([]))              difficulty      0                     mixHash      0x0000000000000000000000000000000000000000000000000000000000000000                     nonce      0x0000000000000000                     ommers      []      RLP([]) = 0xc0      而 EIP-4399 中，将DIFFICULTY 操作码 (0x44) 更新并重命名为 PREVRANDAO，返回由信标链提供的随机信标的输出。PREVRANDAO 所暴露的值存储在 ExecutionPayload 中，原本用于存储 mixHash 值的位置，负载中的 mixHash 字段也被重命名为 prevRandao，如图 10 所示。图10 prevRandao字段的引入随后的上海升级中，EIP-4895 为了实现质押提取功能，将共识层的验证者奖励和退出请求顺利传递到执行层，在 ExecutionPayload 中引入了 withdrawals 字段，在共识层和执行层之间建立无缝的数据传输。坎昆升级中，被称为“Proto-Danksharding”的 EIP-4844 引入了“Blob-carrying Transaction”，以解决以太坊数据可用性问题，降低 Layer 2 Rollup 扩容方案的成本。为实现 Blob 存储费用的动态调整、平衡数据可用性和存储成本间的关系，EIP-4844 在  ExecutionPayload 中添加了 blob_gas_used 和 excess_blob_gas 两个字段。以上是对以太坊 PoS 区块中执行负载 ExecutionPayload 的结构及变更过程的简要介绍，区块中相应的 ExecutionPayloadHeader 内容与之基本相同，仅将交易列表、提款列表替换为了交易根哈希 transactions_root 和提款根哈希 withdrawals_root。state_root、receipts_root 和 transactions_root 字段分别指向相应的全局状态树 (World State Trie)、收据 (Receipts Trie) 和交易树 (Transactions Trie)，后续章节将详细阐述，同时也会对logs_bloom 过滤器、gas_limit 和 gas_used 字段进行说明。  虽然共识层的 ExecutionPayloadHeader 和执行层区块头在概念上是相互映射的，但在共识客户端和执行客户端的实现中二者的编码方式不同，为了维护向后兼容性，执行层保持了原有的 RLP 编码方式，共识层的内部操作则使用 SSZ 编码。EIP-6404 、EIP-6465、EIP-6466 已提议将交易树、提款树、收据树的 MPT 承诺转为 SSZ，以提升跨层一致性和轻客户端的验证效率，为此需要将容器的序列化算法由 RLP 格式更改为 SSZ 格式，进而会影响到执行层区块头中的相应字段。目前这几项 EIP 仍处于评议阶段。数据表示图11 以太坊数据结构如前所述，交易的执行是区块链中的基本过程。以太坊在此基础上进行了扩展，引入了智能合约这一关键特性。智能合约为一种特殊的程序，同样通过交易进行创建和执行，能够执行代码并将值存储至全局变量中，这就意味着以太坊需要存储的不仅仅是交易记录，还包括智能合约代码及相关的状态值。以太坊采用了 Merkle Patricia Trie 结构来实现这点，其具体细节将在后续部分阐述。以太坊使用以下数据结构：收据树 (Receipts Trie)、交易树 (Transactions Trie)、全局状态树 (World State Trie) 和账户存储树 (Account Storage Trie)。交易树收据树和交易树用于记录每笔交易的执行结果。这两种树的特点是，一旦交易执行完毕，其内容就不可更改。这确保了所有已执行的交易被永久记录，无法撤销，从而保证了区块链的不可篡改性。与之形成鲜明对比的是全局状态树，它随着区块链状态的变化而不断更新，实时反映最新的账户余额和智能合约状态。在 MPT 中，每笔交易都被编码为一个键值对，以交易索引 $index$ 作为键，交易 $T$ 本身作为值，两者皆通过 RLP 编码处理。这种编码方式确保了数据的高效存储和快速检索，可以表示为：\\[𝑅𝐿𝑃 (𝑖𝑛𝑑𝑒𝑥) → 𝑅𝐿𝑃 (𝑇)\\]以太坊执行层规范目前定义了以下几种主要的交易类型，以 Geth 客户端代码为例进行简要说明：// Transaction types.const (\tLegacyTxType     = 0x00\tAccessListTxType = 0x01\tDynamicFeeTxType = 0x02\tBlobTxType       = 0x03)      传统交易 (Legacy Transaction)：    type LegacyTx struct {\tNonce    uint64          // nonce of sender account\tGasPrice *big.Int        // wei per gas\tGas      uint64          // gas limit\tTo       *common.Address `rlp:\"nil\"` // nil means contract creation\tValue    *big.Int        // wei amount\tData     []byte          // contract invocation input data\tV, R, S  *big.Int        // signature values}        传统交易是最早的交易类型，在以太坊诞生之初就存在。其特点是使用固定的 gasPrice 来计算交易费用。传统交易中不包含 chainID，因此在跨链时可能存在重放攻击风险。        访问列表交易 (Access List Transaction)【EIP-2930 引入】：    type AccessListTx struct {\tChainID    *big.Int        // destination chain ID\tNonce      uint64          // nonce of sender account\tGasPrice   *big.Int        // wei per gas\tGas        uint64          // gas limit\tTo         *common.Address `rlp:\"nil\"` // nil means contract creation\tValue      *big.Int        // wei amount\tData       []byte          // contract invocation input data\tAccessList AccessList      // EIP-2930 access list\tV, R, S    *big.Int        // signature values}        访问列表交易引入了 accessList 字段，允许预先声明交易将访问的账户和存储槽，预热状态访问，减少EVM执行时的动态gas成本。此外还添加了 chainID，提高了跨链安全性。但仍然使用传统的  gasPrice 机制进行交易的优先级排序和手续费支付。        动态费用交易 (Dynamic Fee Transaction)【EIP-1559 引入】：    type DynamicFeeTx struct {\tChainID    *big.Int\tNonce      uint64\tGasTipCap  *big.Int // a.k.a. maxPriorityFeePerGas\tGasFeeCap  *big.Int // a.k.a. maxFeePerGas\tGas        uint64\tTo         *common.Address `rlp:\"nil\"` // nil means contract creation\tValue      *big.Int\tData       []byte\tAccessList AccessList  \t// Signature values\tV *big.Int `json:\"v\" gencodec:\"required\"`\tR *big.Int `json:\"r\" gencodec:\"required\"`\tS *big.Int `json:\"s\" gencodec:\"required\"`}        动态费用交易采用双层手续费模型：将 gasPrice 拆分为基础手续费 (baseFee) 和小费 (tip)。其中基础手续费会随着网络需求自动调整，并会被销毁（使以太币成为了通缩资产），小费由用户设置，用于激励矿工优先处理交易。使用 gasTipCap 和 gasFeeCap 来控制交易费用。旨在优化网络的手续费模型，解决高峰期手续费波动过大的问题。动态费用交易继承了访问列表功能。        Blob 交易 (Blob Transaction)【EIP-4844 引入】：    type BlobTx struct {\tChainID    *uint256.Int\tNonce      uint64\tGasTipCap  *uint256.Int // a.k.a. maxPriorityFeePerGas\tGasFeeCap  *uint256.Int // a.k.a. maxFeePerGas\tGas        uint64\tTo         common.Address\tValue      *uint256.Int\tData       []byte\tAccessList AccessList\tBlobFeeCap *uint256.Int // a.k.a. maxFeePerBlobGas\tBlobHashes []common.Hash  \t// A blob transaction can optionally contain blobs. This field must be set when BlobTx\t// is used to create a transaction for signing.\tSidecar *BlobTxSidecar `rlp:\"-\"`  \t// Signature values\tV *uint256.Int `json:\"v\" gencodec:\"required\"`\tR *uint256.Int `json:\"r\" gencodec:\"required\"`\tS *uint256.Int `json:\"s\" gencodec:\"required\"`}        Blob 交易是专为 Layer 2 扩展解决方案设计的交易类型，目的是为了降低 Rollup 解决方案的数据可用性成本，提升以太坊的可扩展性。引入了 blobFeeCap 和 blobHashes 字段，用于处理暂时存储大量数据的 Blob (Binary Large Object)。Blob 交易继承了动态费用机制和访问列表功能。  每种交易类型都代表了以太坊协议的一次重要升级：传统交易为以太坊奠定了基础，EIP-2930 通过访问列表优化了复杂交易的 gas 使用，EIP-1559 彻底改革了以太坊的经济模型和费用机制，EIP-4844 则为以太坊未来的可扩展性提升铺平了道路。本文的主要目标是为读者提供以太坊数据结构的概览。考虑到篇幅和重点，这里不会对各种复杂的交易类型进行详尽分析。为了使概念更加清晰易懂，接下来的讨论将以最基本的传统交易（下称交易）为例，深入阐述其结构和特点。交易中包含多个字段：            Field      Description                  nonce      交易的序号，用于跟踪交易顺序并防止重复交易              gasPrice      当前 gas 价格 (wei per gas)              gasLimit      交易执行的最大 gas 限额              to      接收方地址，合约创建交易中为空              value      转移的资金量或新合约的初始余额              data      合约创建的代码或消息调用的输入数据              v, r, s      用于编码发送方签名的值      一笔交易可以执行以下三类操作：  账户间资金转移  智能合约创建  智能合约调用不同操作使用的字段略有不同：      普通转账交易 to 字段指定接收方地址，data 字段为空，value 字段指定要发送的 ETH 数量。    合约创建交易 to 字段为空，data 字段包含合约的字节码，value 字段通常为 0（除非要在创建时向合约发送ETH）。  合约调用交易 to 字段指定要调用的合约地址，data 字段包含函数选择器和编码后的参数，value 字段如果函数是 payable 的，可以指定发送的 ETH 数量，否则为 0。// TransitionDb 执行状态转换func (st *StateTransition) TransitionDb() (*ExecutionResult, error) {    // ... (代码略)    if contractCreation {        ret, _, st.gas, vmerr = st.evm.Create(sender, st.data, st.gas, st.value)    } else {        ret, st.gas, vmerr = st.evm.Call(sender, st.to(), st.data, st.gas, st.value)    }    // ... (代码略)}合约创建智能合约通过一种特殊类型的交易部署到链上，图 12 以序列图的形式展示了这一过程。图12 合约创建及调用交易执行后，会在交易树 (Transaction Trie) 中创建一个包含所有相关字段的记录。对于合约创建而言，Trie 中的 data 字段会被填充为于生成新合约的 EVM 字节码。字节码通常由智能合约编译器预先生成，并随交易一起传输。由于是新合约部署，交易树中的to字段保持为空。交易树中的 to 字段初始为空，因为合约部署的一部分任务就是创建新账户，to 字段的值尚未确定。智能合约部署后，系统立即在全局状态树中创建一个新的合约账户（图中显示为创建了一个新参与者）。这个新账户将合约的字节码存储在 codeHash 字段中。此外，系统还会创建一个新的存储空间，由一个存储树表示，该树的根节点保存在 storageRoot 字段中。如果合约创建时包含初始状态（即全局变量），也会存储在该树中。从编程语言的角度来看，初始化代码是以构造函数的形式实现的。构造函数会与其方法体中的所有指令一同执行，因此构造函数可以通过程序变量来设置合约的初始状态。调用构造函数的结果就是合约本身，因为函数返回了新合约的主体，并通过 account 字段的 codeHash 与之关联，此过程可以类比为面向对象编程中创建对象实例。构造函数可以接受参数，参数以二进制形式附加到 data 字段中。新创建的合约可以附带初始资金，资金存储在 balance 字段中。这一特性使得合约能够像人类用户一样持有资金，但同时也带来了潜在的安全风险，如臭名昭著的DAO攻击事件所示。在该事件中，攻击者利用合约中的漏洞，窃取了大量积累在单一账户中的以太币。消息调用一旦部署完成，合约可以被多次执行。合约执行一般是指其方法的执行，这一过程通过一个带有非空 data 字段的合约调用交易触发。这样以特定格式封装了方法签名和数据的合约执行被称为消息调用，此概念类似常见的 RPC（远程过程调用）或 RMI（远程方法调用）。再次参考图 12 中的 UML 图示，图中底部的人类参与者发起了三个带有消息调用的交易，交易从相应的账户中提取 codeHash，并执行智能合约的某个方法。合约调用交易必须在 to 字段中包含一个账户地址，以确定在查找 codeHash 时使用哪个具体合约账户。调用合约的交易将方法签名和输入数据合并到 data 字段中。方法签名以哈希存储，后跟输入数据。输入数据采用抽象二进制接口（ABI）格式进行编码，这种格式允许将方法签名和输入数据作为一个二进制序列在单个字段中编码。合约间的方法调用通过内部交易实现。这种机制与外部交易调用方法相同，但有两个主要区别：首先，内部交易不会记录在区块链上。其次，内部交易没有独立的 gasLimit，所消耗的 gas 费用由发起原始调用的交易承担，这一规则在嵌套调用合约时同样适用。这种设计使得创建合约库成为可能。这些库本身不收取费用，但当其被整合到其他合约中时，使用者需要考虑相关的成本。v, r, s交易的数字签名由三个值 (v, r, s) 构成。其中，r 和 s 是通过 ECDSA 签名算法计算得出的。以太坊特别采用了名为 secp256k1 的 ECDSA 实现。ECDSA 是基于椭圆曲线的对称函数，利用私钥和公钥对生成签名。第三个值 v 是恢复标识符，为一字节大小，用于从签名中获取公钥。将此公钥通过 Keccak 哈希算法处理后，其哈希值的前 20 个字节即等同于一个账户地址。因此，妥善保管私钥至关重要，因为私钥用于签署交易，并可证明用户作为账户所有者的身份。收据树每笔交易的执行结果都记录在交易收据树中。该树为每笔交易创建一个条目，至少包含一个指示交易执行是否正确的结果代码。此外，树中还可包含智能合约生成的日志消息，用于更详细地调试合约执行过程。通过分析收据树，交易发起者可以观察交易的执行时间，并可通过日志消息进行深入分析。以太坊节点提供了一个便捷的 API，允许用户注册事件并在收到交易收据时收到通知。因此，用户通常先提交交易，然后通过这个 API 等待收据，因为交易的确切执行时间是无法预知的。收据 Trie 的结构如下：键为交易索引 $𝑖𝑛𝑑𝑒𝑥$，值为收据 $𝑅$，组成键值对：\\[RLP(𝑖𝑛𝑑𝑒𝑥) → RLP(𝑅)\\]收据在 Geth 客户端中的定义如下：type Receipt struct {    Type              uint8    PostState         []byte    Status            uint64    CumulativeGasUsed uint64    Bloom             Bloom    Logs              []*Log    // ...其他字段省略}type Receipts []*Receipt      Status：指示交易是成功还是失败（如 gas 不足而失败）    CumulativeGasUsed：执行当前交易后相应区块中累计使用的 gas 总量，即所有前序交易和当前交易消耗的 gas 总和  Logs：此交易产生的日志集合  Bloom ：包含日志条目哈希的 Bloom 过滤器，用于加速日志搜索布隆过滤器 (Bloom Filter)布隆过滤器是一种空间高效的概率性数据结构，用于提升以太坊中与智能合约交互相关的日志检索效率。布隆过滤器通过概率方法快速判断某个特定元素（如交易或事件）是否属于某集合，从而优化区块链上日志查询的过程。具体而言，布隆过滤器由一个大小为 \\(m\\) 的位数组和 \\(k\\) 个独立的哈希函数组成。每个哈希函数将一个元素映射到数组中的一个位上。当将一个元素添加到布隆过滤器时，该元素会通过 \\(k\\) 个哈希函数处理，哈希函数会将对应的位设置为1。要检查某个元素是否在集合中，将该元素使用相同的 \\(k\\) 个哈希函数进行哈希处理。如果哈希位置中有任意一位是0，则该元素肯定不在集合中；如果所有位都为1，则元素可能在集合中，但需要进一步验证（存在哈希碰撞的可能性）。也就是说，布隆过滤器不会漏报，但可能误报（假阳性）。布隆过滤器占用的空间远小于直接存储所有日志条目，可存储于主存、缓存中，或在分布式系统中靠近客户端存储，该特性令其特别适合在区块链环境中使用。在耗费大量资源查询存储之前，客户端可先查询过滤器，布隆过滤器能够快速确认主键的不存在性，从而有效减少无效查询。日志消息 (Message Log)关于日志消息：收据树可包含智能合约生成的日志消息。在 Solidity 合约中，日志消息通过 emit 关键字发出，而在字节码层面，则由 EVM 指令 LOG0 至 LOG4 表示。日志在 Geth 客户端中的定义如下：type Log struct {\t// address of the contract that generated the event\tAddress common.Address `json:\"address\" gencodec:\"required\"`\t// list of topics provided by the contract.\tTopics []common.Hash `json:\"topics\" gencodec:\"required\"`\t// supplied by the contract, usually ABI-encoded\tData []byte `json:\"data\" gencodec:\"required\"\t// ...其他字段省略}  Address：触发日志的地址，即执行该智能合约的账户  Topics：用于标识事件类型的主题  Data：包含日志消息本体的字节序列日志消息可包含任意数据，且每条消息可分配一个主题。客户端可以选择只监听特定主题的消息，无需处理所有消息。图13 日志事件更新收据树日志条目的创建过程可通过图 13 的 UML 序列图来理解。用户首先需提交执行智能合约的交易，即通过消息调用执行合约方法。该交易的 to 字段必须包含目标账户，该账户在全局状态树中代表智能合约。合约执行后，如果方法中包含 emit 关键字，相应消息将进入日志条目。全局状态树不同于静态的交易树和收据树，全局状态树是动态可变更的数据结构，用于记录区块链的最新状态。全局状态树维护了地址与账户之间的映射，树中的路径会链接到账户信息记录。账户地址是一个 160 位的标识符，由用户签名公钥的前 20 个字节生成。以太坊中包含两类账户：外部拥有账户 (Externally Owned Account，EOA) 和合约账户 (Contract Account)。两者在全局状态树中都是叶子节点，结构相同，但可通过某些字段的值来区分。Geth 客户端的实现中 Trie 结构如下：type Trie struct {    root  node    owner common.Hash    db *Database    // ... (其他字段)}  root：树的根节点  owner：trie 的所有者（通常是状态根哈希）  db：底层数据库，用于持久化存储具体到状态树，实现中管理整个以太坊状态的核心结构是 StateDB：type StateDB struct {    db         Database    prefetcher *triePrefetcher    trie       Trie    hasher     crypto.KeccakState    accounts       map[common.Address][]byte    accountsOrigin map[common.Address][]byte    storages       map[common.Address]map[common.Hash][]byte    storagesOrigin map[common.Address]map[common.Hash][]byte    stateObjects        map[common.Address]*stateObject    stateObjectsPending map[common.Address]struct{}    stateObjectsDirty   map[common.Address]struct{}    journal        *journal    validRevisions []revision    nextRevisionId int    // ... (其他字段)}这个结构包含了几个关键组件：  trie：即全局状态树  accounts 和 storages：账户和存储数据的缓存  stateObjects：当前活跃的账户状态对象  journal：状态修改日志，用于支持回滚操作每个账户 $A$ 通过其地址 $address$ 进行映射，地址进一步通过哈希生成键值对：\\[keccak(address) \\rightarrow RLP(A)\\]实现中账户状态由 StateAccount 结构表示：type StateAccount struct {    Nonce    uint64    Balance  *big.Int    Root     common.Hash // merkle root of the storage trie    CodeHash []byte}  Nonce：对 EOA 表示发送交易次数；对合约账户表示创建次数。  Balance：账户可用资金。  StorageRoot：账户存储树的 Merkle 根。EOA 此字段为空。  CodeHash：账户合约代码的哈希，具体的 EVM 字节码存储在底层数据库中，以该哈希为键。EOA此字段为空。EOA 通常由用户持有，存储用户资金，类似标准银行账户。合约账户包含智能合约，通过 CodeHash 链接合约字节码，并通过 StorageRoot 引用账户存储树，用于持久化合约数据。全局状态树通过状态转换进行操作，交易类型必须与账户类型匹配。全局状态树是不断变化的结构。对于涉及 EOA 的交易，账户余额根据交易规定更新。对于涉及智能合约账户的交易，存储在 CodeHash 中的合约根据 Data 字段执行。合约执行通常涉及状态变化，即更新合约全局变量，这会触发 StorageRoot 更新，进而修改状态树。实现中，stateObject 结构代表了一个账户的完整状态：type stateObject struct {    address  common.Address    addrHash common.Hash    data     types.StateAccount    db       *StateDB    trie Trie    code Code    originStorage  Storage    pendingStorage Storage    dirtyStorage   Storage    // ... (其他字段)}这个结构维护了账户的各种状态，包括存储树、合约代码和存储缓存。当需要访问或修改账户状态时，StateDB 首先尝试从 stateObjects 缓存中获取：func (s *StateDB) getStateObject(addr common.Address) *stateObject {    // 首先检查缓存    if obj := s.stateObjects[addr]; obj != nil {        return obj    }    // 从底层trie加载    enc, err := s.trie.TryGet(addr.Bytes())    if err != nil {        s.setError(err)        return nil    }    if len(enc) == 0 {        return nil    }    var data types.StateAccount    if err := rlp.DecodeBytes(enc, &amp;data); err != nil {        // ... 错误处理    }    // 创建新的stateObject并缓存    obj := newObject(s, addr, data)    s.setStateObject(obj)    return obj}对账户状态的修改会更新 stateObject 并标记为脏：func (s *StateDB) setStateObject(object *stateObject) {    s.stateObjects[object.Address()] = object    s.stateObjectsDirty[object.Address()] = struct{}{}}存储变更会更新 pendingStorage 和 dirtyStorage：func (s *stateObject) SetState(db Database, key, value common.Hash) {    // ... (变更记录和状态更新)    s.pendingStorage[key] = value    s.dirtyStorage[key] = value}当需要将状态变更持久化时，Commit 函数被调用：func (s *StateDB) Commit(deleteEmptyObjects bool) (common.Hash, error) {    // 提交所有脏状态对象    for addr := range s.stateObjectsDirty {        stateObject := s.stateObjects[addr]        if stateObject.suicided || (deleteEmptyObjects &amp;&amp; stateObject.empty()) {            s.deleteStateObject(stateObject)        } else {            stateObject.updateRoot(s.db)            s.updateStateObject(stateObject)        }    }    // 写入trie变更    root, err := s.trie.Commit(nil)    // ... (错误处理和清理)    return root, err}这个过程会更新所有脏状态对象，删除自毁或空的对象，并提交 Trie 的变更。存储树状态树通过账户及其字段 storageRoot 引用智能合约数据，storageRoot 是账户存储树的根哈希值，即 Geth 中账户的 Root 字段：type StateAccount struct {    Nonce    uint64    Balance  *big.Int    Root     common.Hash // merkle root of the storage trie    CodeHash []byte}存储树主要通过 stateObject 结构体来管理：type stateObject struct {    // ...    trie Trie // storage trie, which becomes non-nil on first access    // ...    originStorage  Storage // Storage cache of original entries to dedup rewrites    pendingStorage Storage // Storage entries that need to be flushed to disk, at the end of an entire block    dirtyStorage   Storage // Storage entries that have been modified in the current transaction execution    // ...}  trie：实际的存储树，延迟加载  originStorage：原始存储项的缓存  pendingStorage：待刷新到磁盘的存储项  dirtyStorage：当前交易执行中被修改的存储项存储树会通过智能合约字节码中的 SLOAD 和 SSTORE 指令直接进行修改。      SLOAD (0x54)                  功能：从存储中加载一个字（32字节）                    输入：从栈顶弹出一个存储位置（键）                    输出：将对应的存储值压入栈顶                    gas消耗：热访问100 gas，冷访问2100 gas（EIP-2929之后）                  SSTORE (0x55)                  功能：将一个字（32字节）保存到存储中                    输入：从栈顶弹出两个值，第一个是存储位置（键），第二个是要存储的值                    输出：无                    gas消耗：复杂，取决于操作类型（设置为非零、重置为零、修改现有值）            存储树中的每一个键都是存储在叶节点中的一个 Slot 的索引。该索引还要经过哈希处理。索引代表智能合约中的一个或多个全局变量，编译器在编译时确定每个变量的索引。索引 $index$ 与 Slot 的键值对在树中的存储可表示为：\\[𝑘𝑒𝑐𝑐𝑎𝑘 (𝑖𝑛𝑑𝑒𝑥) → 𝑅𝐿𝑃 (𝑠𝑙𝑜𝑡)\\]以如下代码和 UML 图 14 为例来说明存储树的修改过程。假设用户已经提交了执行该智能合约的交易，并且交易字段 data 包含了被调用方法的函数签名。假设此函数已经分别以值 30、20 和 10 被调用了三次。首先在交易中定位到一个账户，然后从字段 codeHash 中执行合约，并通过 storageRoot 更新与之关联的账户存储树。由于代码清单1中的智能合约包含一个全局变量 storedData，且该变量被所执行的方法修改，因此账户存储树依次随交易的处理被修改。contract ExampleContract {\tuint storedData\tevent Sent ( msg );\tconstructor () public {}\tfunction method ( uint x) public {\t\tstoreData = x\t\temit Sent ('Success ');\t}}图14 存储树更新存储树中的一个叶节点代表一个 Slot ， Slot 可以包含一个或多个变量。 Slot 的宽度为32字节，其内容取决于特定变量的数据类型，Solidity 编译器文档中对此有详细说明。有趣的是，以太坊区块链上的数据格式实际上是由 Solidity 语言定义的，Solidity 虽然是目前最流行的智能合约编程语言，但归根到底只是众多可能实现中的一种。换句话说，如果有其他编译器以不同方式存储数据，可能会生成与 Solidity 不兼容的数据存储格式。这意味着使用不同编译器或语言编写的合约可能无法正确读取或解释彼此的数据。存储在区块链上的数据本身不包含任何版本信息。这导致即使是 Solidity 自身也无法轻易改变其数据存储方式，因为这可能会破坏与现有合约的兼容性。目前，防止出现不兼容变化的主要机制是智能合约代码一旦部署就无法更新。这种不可变性确保了数据格式的稳定性，但也限制了合约的灵活性和可升级性。在此提供当前 Solidity 生成的变量格式概述：      静态大小变量：多个变量会被组合在一个 Slot 中，最大可占用 32 字节。32 字节无法容纳的值会占用一个新 Slot。结构体和数组会从新 Slot 开始，并占据整个 Slot，除非其项也被打包。编译器检测到的第一个变量将被分配索引 0x0，新 Slot 依次使用下一个可用索引。    图15 静态大小变量    图 15 中的示例展示了包含三个打包变量（int128、int8 和 bool）的 Slot，以及一个无法打包的变量（int256）。    合约使用继承时，其 Slot 会在存储中向后移动，以适应来自父类的状态变量，从下一个可用的 Slot 开始放置子类的变量。        Map：Map 首先选择一个可用的 Slot 索引 $index$，Map 中的每一个键会与该 Slot 索引进行拼接，然后计算哈希值：\\[index_{val} = keccak(index + key)\\]    这里的“+”表示字符串拼接。索引指向包含映射值的 Slot。注意，初始的 Slot 索引本身保持为空。图 16 中展示了从某索引开始的 Map 示例。    图16 Map变量        动态数组：为数组分配一个可用的 Slot 索引，该索引指向一个包含数组大小的 Slot（不同于 Map，数组 Slot 不为空）。该索引的哈希值指向数组的起始 Slot，所有数组元素从此索引开始顺序排列。要获取数组中的某个值，必须计算：\\[index_i = keccak(index + i)\\]    其中，\\(index_i\\) 指向数组索引 \\(i\\) 处的值的槽位。可以看到，数组和映射的存储方式基本相同，但数组有一个额外的优势，即如果多个变量可以放入一个槽位，则会被打包存储。图 17 展示了从某索引开始的数组示例。    图17 动态数组        字节数组和字符串：当长度小于 32 字节时，会被存储在一个 Slot 中，其中一个字节用于表示长度，剩下的 31 个字节用于存储数据。对于较长的字符串或数组，其存储方式与动态数组相同：一个 Slot 存储长度，后续的 Slot 存储具体数据。图 18 展示了短字符串的示例。    图18 字节数组和字符串  安全树安全树 (Secure Trie) 是一种 Trie 的封装，指 Trie 中的键（路径）经过了哈希处理，并不是一种独立的数据结构。以太坊的 Geth 实现在全局状态树和账户存储树中使用了这种封装，但“安全树”这一名称已被移除。// SecureTrie is the old name of StateTrie.// Deprecated: use StateTrie.type SecureTrie = StateTrie// StateTrie wraps a trie with key hashing. In a stateTrie trie, all// access operations hash the key using keccak256. This prevents// calling code from creating long chains of nodes that// increase the access time.type StateTrie struct {\ttrie             Trie\tdb               database.Database\tpreimages        preimageStore\thashKeyBuf       [common.HashLength]byte\tsecKeyCache      map[string][]byte\tsecKeyCacheOwner *StateTrie // Pointer to self, replace the key cache on mismatch}对于一个账户地址，存储在 Trie 中的键实际上不是 20 字节的地址，而是 32 字节的哈希值。这意味着当操作涉及账户地址时，必须先对该地址进行哈希处理，然后才能在 Trie 中查找或存储。同样，从账户存储树中获取的变量标识符也是经过哈希处理的，在执行 SLOAD 和 SSTORE 指令时，每个标识符都要先进行哈希处理，然后再在 Trie 中进行访问。这样做的目的是为了防止 DOS 攻击。攻击者可能会构造特定负载（例如账户地址），使 Trie 退化为包含长路径且分支最小的结构，从而导致相应 Trie 中的查找次数急剧增加。而通过对键进行哈希处理，可以随机化其字符串表示形式，从而令分支的分布更加均匀。当然，由于每次访问都需要进行哈希计算，并且需要维护额外的缓存来映射哈希键和原始键，这也会相应地增加一些开销。总结本文首先对以太坊的基础数据结构和编码方案进行了详细介绍，随后对当前以太坊的区块和交易结构进行了简要说明，最后结合 Geth 讲解了执行层实现中的关键结构。同时，本文也对以太坊各轮升级带来的变化进行了梳理，参考执行层和共识层规范给出了相关的定义和代码，并结合了一些实例和插图进行说明，希望能对读者有所帮助。参考            Edward Fredkin. Trie memory. Commun. ACM, 3(9):490–499, September 1960. &#8617;              Donald R. Morrison. Patricia—practical algorithm to retrieve information coded in alphanumeric. J. ACM, 15(4):514–534, October 1968. &#8617;              Ralph C. Merkle. A digital signature based on a conventional encryption function. In Carl Pomerance, editor, Advances in Cryptology — CRYPTO ’87, pages 369–378, Berlin, Heidelberg, 1988. Springer Berlin Heidelberg. &#8617;              Gavin Wood. Ethereum: A secure decentralised generalised transaction ledger shanghai version. &#8617;              Jae-Yun Kim, Jun-Mo Lee, Yeon-Jae Koo, Sang-Hyeon Park, and Soo-Mook Moon. Ethanos: Lightweight bootstrapping for ethereum. arXiv preprint arXiv:1911.05953, 2019. &#8617;              Pandian Raju, Soujanya Ponnapalli, Evan Kaminsky, Gilad Oved, Zachary Keener, Vijay Chidambaram, and Ittai Abraham. mlsm: Making authenticated storage faster in ethereum. In 10th {USENIX} Workshop on Hot Topics in Storage and File Systems (HotStorage 18), 2018. &#8617;      "
  },
  
  {
    "title": "IPFS 技术原理详解",
    "url": "/posts/ipfs/",
    "categories": "IPFS",
    "tags": "ipfs",
    "date": "2023-07-26 13:00:00 +0800",
    





    
    "snippet": "  本站已在 IPFS 上同步部署      您可运行节点使用本地网关访问：              IPNS KEY: k51qzi5uqu5dmfcprlafbp7idt7jujlsgp48hx8ful5k7iqm0el582yva4fgq3                    或通过公共网关访问：                        dweb.link/ipns/port...",
    "content": "  本站已在 IPFS 上同步部署      您可运行节点使用本地网关访问：              IPNS KEY: k51qzi5uqu5dmfcprlafbp7idt7jujlsgp48hx8ful5k7iqm0el582yva4fgq3                    或通过公共网关访问：                        dweb.link/ipns/portkey.fanwb.xyz                          dweb.link/ipns/k51qzi5uqu5dmfcprlafbp7idt7jujlsgp48hx8ful5k7iqm0el582yva4fgq3                          flk-ipfs.xyz/ipns/portkey.fanwb.xyz                          flk-ipfs.xyz/ipns/k51qzi5uqu5dmfcprlafbp7idt7jujlsgp48hx8ful5k7iqm0el582yva4fgq3                    在数字基础设施的演进过程中，规模经济和云计算正推动网络系统日益走向中心化。从域名解析到内容托管，从路由协议到证书颁发，各类网络服务逐渐集中于少数几家服务商手中，某些领域甚至已形成事实上的垄断。这种中心化趋势削弱了网络生态的多样性，令网络系统不可避免地面临着中心化系统所固有的单点故障风险，极端情况下的服务宕机可能会导致严重的后果和巨大的经济损失。亚马逊的电商平台在 2013 年的宕机期间每分钟的损失超过 66000 美元，充分印证了这一潜在危机。面对这一系统性风险，技术社区中兴起了一场“网络去中心化”运动。这一运动的核心理念是重新分配网络控制权，主要通过开源、社区驱动的软件实现，将传统的网络功能如域名查找、托管和认证等进行去中心化。目前已涌现出诸如 PeerTube 和 Mastodon 等项目，为去中心化探索新的可能。尽管这些努力值得肯定，但就实际影响而言，仍然相对有限，且多局限于特定应用场景。深入思考，网络平台的本质在于大规模存储和分发媒体对象。若能将这一核心功能去中心化，則可从根本上简化去中心化应用的复杂性。星际文件系统（IPFS）正是基于这一理念应运而生。可以将IPFS简明定义为：一个完全去中心化的、内容寻址的媒体对象存储和检索平台。当前，IPFS 已经得到了广泛应用，每周有超 300 万次 Web 客户端访问，对等网络中有超过 30 万个节点提供内容。IPFS 支持了各类去中心化网络应用，涵盖社交网络 (Discussify，Matters News)、数据存储 (Space，Peergos，Temporal)、内容搜索 (Almonit，Deece)、即时通讯 (Berty) 、内容流媒体 (Audius，Watchit) 以及电商 (Ethlance，dClimate) 等。主流浏览器如 Opera 和 Firefox 也已提供 IPFS 访问功能，进一步促进了其应用便捷性。本文将详细介绍 IPFS 的设计和原理，其核心依赖四个主要概念：  内容寻址：IPFS 将对象名称与主机位置分离，使对象可以由任何对等点提供；  去中心化对象索引：IPFS 依赖去中心化的 P2P 覆盖来索引对象的可用位置，减少技术或组织失效的影响；  不可变性及自认证：IPFS依赖加密哈希来自认证对象，消除基于证书的身份验证需求，提供可验证性；  开放参与：任何人都可以部署 IPFS 节点并参与网络，无需特殊权限。1 IPFS 基础首先概述 IPFS 的核心模块，即 IPFS如何寻址内容、如何寻址对等点、如何构建分布式索引来将内容标识符映射到存储该内容的对等点。1.1 内容寻址IPFS 的核心是基于内容的寻址方案，其中使用了基于哈希的内容标识符 (CID) 。HTTP 等基于位置的系统，将内容地址 (URL) 与提供内容的主机绑定。而 IPFS 的内容寻址方案则将 CID 作为基本原语将内容的名称与其存储位置解耦。正是这一设计使得内容存储、内容传递和地址管理的去中心化成为可能，同时还能够防止供应商锁定，并消除了由中心化实体处理地址分配的需求。图1 CID V1 结构说明图1 展示了 CID 及其结构的示例。CID 依赖于一组自描述的数据表示协议，由以下四个字段组成：  Multibase 前缀：二进制 CID 被编码的方式（“b”代表 base32）；  CID 版本标识符：表示CID版本（v1）；  Multicodec 标识符：表示数据的序列化方式（protobuf、json、cbor等）；  Multihash：寻址数据的自描述哈希摘要。Multihash 包括表示所使用的哈希函数（默认为 sha2-256）和实际哈希长度（默认为 32 字节）的元数据。当内容添加到 IPFS 时，会被分割成块（默认为 256 kB），每个块都会被分配一个 CID。块的 CID 是通过对其内容进行哈希并添加上述元数据产生的。随后 IPFS 构建文件的默克尔有向无环图 (Merkle DAG) ，原始内容发布者以该形式提供文件。Merkle DAG 是类似于 Merkle 树的数据结构，但没有平衡要求。根节点合并其后代节点的所有 CID，并形成最终的内容 CID（通常称为根 CID）。在 Merkle DAG 中，允许节点有多个父节点，这一特性使块去重成为可能。反过来，内容去重意味着相同的内容不需要重复存储或传输，从而节省存储和带宽资源。此外，Merkle DAG 对内容存储的位置是不可知的。因此，在网络中的节点上复制或删除文件时不需要进行更新。CID 基于哈希的结构决定了其具有不可变和自认证的特性，也就是说无法只修改内容而不变更其 CID。利用这一特性，可以通过比较 CID 与内容本身的哈希进行来进行自验证。但另一方面，对于动态变化的数字对象，这一特性会带来挑战，后文将详细讨论。1.2 对等点寻址加入 IPFS 网络并连接到一组引导对等点后，节点会生成一个公私钥对。IPFS 网络中的每个对等点都由其唯一的 PeerID 标识，该标识是其公钥的哈希值（表示为 Multihash）。除非手动更改，否则PeerID保持不变。在建立安全通信通道时，PeerID 用于验证保护通道的公钥与识别对等点的公钥是否相同。IPFS 使用 Multiaddresses 表示远程对等点的位置。Multiaddress 是一种自描述的、人类可读的、按层次分隔的协议选择序列。Multiaddress 这个术语源于其格式允许包含多个协议和地址类型。Multiaddress 代表对等点的一个可交互端点。IPFS 涵盖了从网络层到应用层的多种协议。图2 Multiaddress 结构说明图2 展示了 Multiaddress 的结构，其中包含了用于通信的网络和传输协议（IPv4 和 TCP）及其对应的基于位置的地址信息（IP 地址 1.2.3.4 和 TCP 端口号 3333），以及用于寻址特定对等点的协议 (p2p) 和其 PeerID (QmZyWQ14…) 。由此，Multiaddress 通过将多个层次的地址信息编码为路径表示来指向远程进程。Multiaddress 使用此结构有两个原因：  并非所有 IPFS 节点都共享相同的协议子集。Multiaddress 使节点能在尝试连接之前知道是否能够连接到远程对等点。  Multiaddress 的可扩展语法允许通过前缀化对等点地址进行中间中继通信。这一特点用于将消息代理到无法直接联系的浏览器节点。1.3 内容索引要发布或检索对象，就需要在 CID 和能够提供对象的 PeerID 之间创建映射（包括其Multiaddress）。为了以分布式的方式支持内容和对等点的发现，映射被索引到一个分布式哈希表 (DHT) 上，该表提供了简单的 PUT 和 GET 基元。IPFS 的 DHT 基于Kademlia，CID 和 PeerID 存在于一个共同的 256 位密钥空间中，使用其二进制表示的 SHA256 哈希作为索引键（见图1）。与原始的 Kademlia 规范相比，IPFS 进行了一些调整：  DHT 中的节点使用 256 位 SHA256 密钥而非 160 位 SHA1 密钥，以防止哈希碰撞。  维护 i = 256 个 k-节点的桶（其中 k = 20）来划分哈希空间。  采用可靠的传输协议如 TCP 和 QUIC，使连接管理更加简单直接。新节点以 DHT 服务器或 DHT 客户端的形式加入 DHT，其中 DHT 服务器具有公共IP连接性，而 DHT 客户端不可公开访问（如位于NAT后）。IPFS 通过一种称为 Autonat 的简单技术来区分 DHT 客户端和服务器，其工作原理如下：新节点默认作为客户端加入，并立即要求网络中的其他节点主动与其建立连接。如果超过 3 个节点能够连接新加入的节点，则该节点将升级为服务器节点，否则节点保持为客户端。DHT 服务器执行所有网络操作，即存储内容、存储映射记录并向请求的节点提供这些记录。而 DHT 客户端只从网络请求记录或内容，但不存储或提供任何内容。DHT 客户端/服务器的区分可以避免不可访问的对等点进入其他对等点的路由表，从而加快对象的发布和检索过程。2 IPFS 流程这一部分主要介绍 IPFS 如何发布内容，对等点如何查找并检索内容以及 IPFS 如何处理可变内容。图3 IPFS中的内容发布与检索2.1 内容发布为了在IPFS网络中提供内容，内容首先导入IPFS并分配一个 CID ① (见内容寻址部分)。内容导入本地 IPFS 实例后，不会复制或上传到任何外部服务器。要发布内容，主机需要生成一个提供者记录 (provider record) ，并将其推送到 DHT。该记录会将 CID 映射到主机的 PeerID。随后，通过计算 CID 的 SHA256 哈希与 PeerID 间的异或距离，找到最近的 k = 20 个对等点存储提供者记录。记录复制到 k 个对等点上可以确保即使某些对等点离线，记录仍然可用。记录副本数选择 20 是 IPFS 根据实际的网络情况，在复制开销和记录删除风险之间的一个折衷。一旦 IPFS 找到最接近的对等点 ②，会尝试在这些节点上存储提供者记录。此操作是通过建立连接并发起 RPC 来完成的 ③。该过程不会等待对等点的响应，而是以“即发即忘”的方式执行 RPC。值得注意的是，后续检索数据的所有对等点都会向 DHT 发布指向其自己节点的提供者记录，从而成为临时内容提供者。检索内容的对等点无需信任新的提供者，只需要验证收到的数据与请求的 CID 匹配即可。对等点还需要发布其对等点记录 (peer record) ，将其自己的 PeerID 映射到与之关联的 Multiaddress。由此可以通过请求对等点来获取其底层网络地址。对等点记录的发布与上述过程相同，但与内容发布无关，独立进行。除了复制因子 (k = 20) 之外，提供者记录还与另外两个参数相关：      重新发布间隔，默认设置为12小时，以确保即使最初负责保留提供者记录的 20 个对等点离线，提供者也会在 12 小时内分配新对等点；        过期间隔，默认设置为24小时，以确保发布者未离线且仍愿意提供内容。  这些参数设置是为了防止系统存储和提供过时的记录。2.2 内容检索一旦提供者和对等点记录被发布，用户就可以检索内容了。内容检索需要请求方执行以下四个步骤：  内容发现：识别托管内容 / CID 的 PeerID；  对等点发现：将 PeerID 映射到一个 Multiaddress（如一个IP地址）；  对等点路由：连接到对等点；  内容交换：获取内容。内容发现： IPFS 中的内容发现主要使用 DHT 完成。然而，在进入 DHT 查找前，请求节点会要求其已连接到的所有对等点提供所需的 CID ④。这一步是以机会性的方式进行的（使用后文提到的Bitswap协议）。如果对等点的直接邻居存储了所需的内容，这一机制能更快地解析内容。如果尝试不成功，内容发现会在1秒的超时后回退到 DHT。DHT实现多轮迭代查找，以将 CID 解析为对等点的 Multiaddress，此过程称为 DHT 遍历 (DHT walk) ⑤。当对等点 A 发出对 CID $x$ 的请求时，该请求被转发到其路由表中与 $x$ 最接近的 $\\alpha$ = 3 个对等点（按照原始的 Kademlia 规范）。接收到请求的对等点如果有请求的内容则回复该内容。如果没有所请求的内容，则回复以下之一：      指向拥有所请求项的 PeerID 的提供者记录，以及该对等点的 Multiaddress（若有）        PeerID 更接近于 $x$ 的对等点  该过程将持续进行，直到提供者记录中与被请求 CID 副本相关联的节点被返回为止。对等点发现： 在内容发现阶段之后，客户端知道了托管目标内容的 PeerID。随后，需要通过查找对等点记录将 PeerID 映射到物理网络地址。此过程称为对等点发现 (peer discovery) ，通过再次查询 DHT 来完成。为了进一步简化该过程，每个 IPFS 节点维护一个最多包含 900 个对等点的地址簿。在执行进一步的查找前，节点会检查是否已经拥有该 PeerID 的地址。对等点路由： PeerID 解析为对等点记录后，请求节点会获取到提供目标内容的对等点的 Multiaddress，随后使用该地址尝试连接到对等点。内容交换： 确定了提供内容的对等点后，使用块交换协议 Bitswap 来从对等点获取内容 ⑥。Bitswap 使用 wantlists 发出对内容块的请求。请求使用 IWANT-HAVE 消息发送。拥有被请求内容块的对等点用 IHAVE 消息回复。请求节点最后回复 IWANT-BLOCK 消息。接收到请求的内容块终止此交换过程。如前所述，Bitswap 协议也用于节点与其邻近节点之间机会性地发现可用内容。2.3 可变内容基于哈希的结构决定了 CID 是不可变和自认证的。利用这一特性可以消除对中心化协调者的需求，使其十分适合去中心化的命名空间管理，同时还令通用缓存（即从任何对等点缓存）成为可能。然而，这一特点在处理动态内容（如频繁变更的文档）时会成为问题。为了处理可变内容，在文件修改时保持内容名称一致，IPFS提供了一种名为星际命名系统 (IPNS) 的命名系统，支持基于发布者的公钥哈希 (PeerID) 而不是内容的哈希 (CID) 来发布内容。IPNS 的标识符是从节点的公钥派生的，因此每个节点只能创建一个 IPNS 标识符。IPNS 记录会将发布者公钥的 CID 映射到由相应私钥签名的另一个 CID，虽然标识符本身是静态的，但其所指的 CID 可以由发布节点任意修改，这使得 IPNS 标识符背后的 IPFS 对象具有了可变性。IPNS 标识符可以表示其他任意的 CID，例如块、文件或数据结构。IPNS 标识符在其路径名中使用 /ipns/ 前缀而非 /ipfs/ 前缀以作区分。2.4 IPFS 网关为了方便还没有安装 IPFS 客户端的用户访问 IPFS 托管的内容，IPFS 系统通过网关进行补充，提供了 (HTTP) 进入 IPFS 的入口。网关相当于一个桥梁：一侧是DHT服务器节点，另一侧是一个 nginx HTTP web 服务器，可以接收包含 CID 作为 URL 路径的 GET 请求，即 https://ipfs-gateway.io/ipfs/{CID}。运营网关不需要任何实体的授权或许可，只需要使用公网 IP 地址设置。需要注意的是，网关本质上对于整个存储和检索网络并非不可或缺，IPFS 网关是通过提供另一种从 IPFS 中检索文件的方法来支持整个网络。3 IPFS 的主要特性在应用程序中使用 IPFS 作为文件存储有诸多好处，例如内置的文件完整性检查、固有的内容去重，以及将文件检索与特定位置解耦的内容寻址。与此同时，由于其去中心化和无许可的性质，IPFS 也有一些特殊特性（如默认不提供授权/访问控制），在将 IPFS 集成到项目或应用程序中作为分布式存储时需要考虑这些特性。以下是关于 IPFS 的一些关键属性及其影响的讨论。3.1  名称持久性及文件完整性使用 Multihash 而非位置地址来标识内容，可以为网络提供更大的灵活性：      首先，资源可以更有效地利用，因为重复的文件甚至文件块都会被分配相同的标识符，因此可以进行链接和重用，以避免额外的资源浪费。而在传统的基于主机的寻址方案中，重复的文件可能以不同的文件名和不同的位置标识符被冗余存储。需要注意的是，虽然集中式云提供商也会应用数据去重方案，但这些方案一般不会应用在块级别，且不同方案使用的数据模型各不相同。        显式的内容寻址还简化了路径上和网络内缓存。因为可以使用 Multihash 验证文件块的完整性，因而无需信任第三方指向或提供正确的文件片段，由此可以消除对单个网络或内容提供商的依赖，避免了潜在的集中化。  IPFS 中名称持久性的特点之一是当内容本身更新时，内容标识符会发生变化。这与当前基于 HTTP 的模型不同，在 HTTP 模型中，URL 在其代表的内容发生变化时仍保持不变。因此，需要有额外的机制来处理动态内容，如使用星际命名系统 IPNS 或 libp2p 的 pubsub 协议（允许对等点彼此创建 pubsub 通道以动态广播和监听事件）来更新发布在 IPFS 上的内容。3.2 数据可审计性、抗审查性及隐私保护与 HTTP 相比，IPFS 在“隐私设计”原则方面做得更出色，因为它允许对存储数据进行更精确和全面的审计。例如，在用户撤销授权、需要删除个人数据的情况下，使用 HTTP 面临的问题是无法确定给定数据的所有副本是否已经从实体的服务器上删除（可能以不同的名称存储）。而在 IPFS 下，内容标识符的持久性使用户能更准确和彻底地了解包含个人数据的文件存储在何处。得益于默克尔链接，可以通过扫描CID 来验证数据是否存储在某个位置。但要注意的是，修改数据会产生不同的 CID，因此很难检测到修改后的数据版本（如添加了 padding 的数据）。尽管内容标识本身是不可变的，但实际数据可以在需要时被删除，这种数据可变性与特定元数据不可变性的结合，能更好地为基于 IPFS 构建的应用程序提供隐私保护的基础。同时，由于 IPFS 是一个没有中央索引实体的分布式 P2P 文件系统，因此无法在其中明确地审查内容。由于对等点没有组织成层次结构，网络中不存在可以禁止存储和传播文件或从其他对等点的存储中删除文件的权威节点。因此，从技术上无法强制审查特定内容。IPFS 提供的抗审查性是通过在请求过的不同对等点之间复制内容来实现的，这使得完全审查提供者节点的难度极大。此外，任何公共 IPFS 网关也可以检索并向用户提供特定内容，这进一步增强了抗审查性。另一方面，通过指纹查找存储相关文件的对等点会泄露其 IP 地址，这意味着这些对等点仍然可以被识别。关于隐私，这里有两个要点需要强调：  与任何无许可、公开的 P2P 网络一样，IPFS 是一个全球分布、公开的服务器，用于存储网络中发布的数据。也就是说，IPFS 目前主要的用例是提供公共数据（例如数据集或网站）的存储和访问。考虑到 IPFS 预计能够支持的模块化和广泛应用，IPFS 目前并不在协议层支持隐私，这样的隐私设计目前必须在应用层实现。在撰写本文时，增强隐私同时支持广泛应用的模块化方法仍有待研究。  IPFS 对等点可以控制其在网络中与他人共享的内容。默认情况下，对等点会向网络发布其缓存中的每个 CID，即已发布或先前请求并获取的内容。如果对等点希望请求历史保密，可以进行本地节点配置，停止提供请求过的内容。该对等点仍然能获取和使用内容，并将其保存在本地缓存中，但不会在被请求时提供内容（如通过Bitswap），也不会让网络知道本地拥有特定内容。3.3 网络分区容错性由于 IPFS 是一个去中心化的 P2P 网络，没有核心中央组件，因此可以在网络分区或离线情况下运行。虽然一些组件可能无法检索或更新，但分区不会完全影响内容发布和检索过程。因此，只要所需对象和提供者记录在同一个子图中已知且可用，IPFS 就能够容忍分区，并且如果提供者节点可访问，则不需要完整的互联网连接。此外，可以使用 IPFS 集群构建一组机器之间的私有 IPFS 网络，从而允许在本地网络中部署 IPFS。3.4 参与激励IPFS 的设计初衷是一个无许可、尽力而为的去中心化 P2P 网络，因此不包含任何激励机制。运行一个 IPFS 节点会产生带宽、存储和电力方面的基础设施维护成本，在检索到所需对象后，普通用户没有动力继续运行节点，这也导致了 IPFS 中的网络会话时间短，波动性高。用户甚至可以只依靠 IPFS 网关通过 HTTP 检索所需对象，完全不参与网络。第三方固定服务可以一定程度缓解缺乏激励的问题，这类服务通过固定文件的来保障其可用性并向用户收取费用。另一种解决该问题的方法是提供独占性，即提供其他地方无法获得的内容或功能，但这点较难实现，因为与集中化的基础设施相比，IPFS 尽力而为的存储方式在便捷性和用户体验方面更不稳定，这可能导致应用难以获得关键用户数量。Filecoin 是一个激励式的 P2P 网络，用于存储和检索基于 IPFS 的对象。其目标是提供比集中式云存储解决方案更便宜的分布式存储。Filecoin 使用与 IPFS 相同的构建模块，核心是通过 CID 进行内容寻址。与 IPFS 不同的是，IPFS 在其他对等点明确请求内容之前不会复制内容，而 Filecoin 对其参与者进行加密经济激励：在网络中存储和复制内容会获得加密货币奖励。由此 Filecoin 获得了更高的可用性、更快的检索速度并抵消了节点变动。通过支付费用，对等点可以相互之间签订存储交易来进行持久存储，并通过复制证明和时空证明进行证明。Filecoin 通过提供激励机制改进了 IPFS 尽力而为的存储和交付服务，且由于 Filecoin 是去中心化、基于区块链的，合约双方无需相互信任。4 IPFS 相关数据4.1 地理分布根据 Protocol Labs 的研究，截止 2021 年 9 月，IPFS 中共有 198,964 个对等点，在 DHT 中有 1,998,825 个 Multiaddress，覆盖了来自 152 个国家的 464,303 个唯一 IP 地址。图 4 显示了 PeerID 的地理分布。图4 IPFS 对等点地理分布可以发现，尽管 IPFS 覆盖范围很广，但仍存在某些地区的集中分布，美国 (28.5%) 和中国 (24.2%) 在对等点数量上占据了主导地位，其次是法国 (8.3%) 和韩国 (6.7%)。图5 每个 IP 地址上对等点数量的累积分布函数需要注意的是，单个 IP 地址可以托管多个 PeerID。图 5 绘制了每个主机上对等点数量的累积分布函数 (CDF)。虽然大多数 (92.3%) IP 地址只托管一个 PeerID，但排名前 10 的 IP 地址托管了近 6.6 万个不同的 PeerID，如果恶意对等点轮换 PeerID，会影响网络整体路由性能，存在拒绝服务攻击的风险。4.2 自治系统分布IPFS 对等点分布于 2715 个自治系统 (AS) 中。图 6 展示了每个 AS 中的 IP 地址数量，其中，排名靠前的少数 AS 容纳了大量 IPFS 主机。排名前 10 的 AS 涵盖了 64.9% 的 IP 地址，排名前 100 的 AS 涵盖了 90.6%，超过 50% 的 IP 地址仅分布于 5 个 AS中。这与 IPFS 对等点的地理分布存在显著差异，每个地区都主要由少数 AS 主导。图6 IP 地址在不同规模自治系统中的分布情况这一趋势的一个可能原因是 IPFS 节点广泛使用云基础设施。下表列出了每个云提供商中部署的节点数量。与预期相反，统计发现只有少数 (&lt;2.3%) 的 IPFS 节点托管在云上。这对去中心化存储和交付网络来说是一个重要发现，表明大多数用户托管了自己的部署。            排名      提供商      IP 地址      占比                  1      Contabo GmbH      2038      0.44 %              2      Amazon AWS      1792      0.39 %              3      Microsoft Azure/Coporation      1536      0.33 %              4      Digital Ocean      836      0.18 %              5      Hetzner Online      592      0.13 %              6      GZ Systems      346      &lt;0.10 %              7      OVH      341      &lt;0.10 %              8      Google Cloud      286      &lt;0.10 %              9      Tencent Cloud      258      &lt;0.10 %              10      Choopa, LLC. Cloud      244      &lt;0.10 %              12      Alibaba Cloud      180      &lt;0.10 %              13      CloudFlare Inc      140      &lt;0.10 %              27      Oracle Cloud      27      &lt;0.10 %              54      IBM Cloud      9      &lt;0.10 %                     Non-Cloud      453661      97.71 %      4.3 节点波动节点波动是指节点进出网络的行为。图 7 绘制了主要国家和地区的 DHT 节点在线时长的累计分布函数。图7 主要地区 DHT 节点在线时长的累计分布函数可以看到，节点在线时长通常较短，87.6% 的会话少于 8 小时，只有 2.5% 的会话超过 24 小时。这也解释了 IPFS 选择在相对较多节点（复制因子 k = 20）上复制记录的设计决策。另外，节点的稳定性会因地区而异，香港的中位在线时长仅为 24.2 分钟，而德国的数据是其两倍以上。IPFS 在设计上是高度去中心化的。尽管在某些地区存在地理聚集，但总体而言 IPFS 节点仍在全球范围内广泛分布。仅有不到 2.3% 的 IPFS 节点运行在云平台上，这一方面有益于网络的健壮性和去中心化，但同时也导致了网络节点的高波动率（只有 2.5% 的对等点在线超过 24 小时）。此外，虽然在 2715 个自治系统中存在 IPFS 节点，但排名前 10 的自治系统就占了 64.9%，未来 IPFS 网络仍需扩大部署，以避免自治系统中的中心化。5 总结本文概述了 IPFS 核心原理及其功能，介绍了 IPFS 如何结合多种网络协议和 P2P 理念构建去中心化云存储基础设施。其基础模块通过内容标识符 CID 实现了名称持久性、文件去重和完整性检查等功能，支持对等的文件分发和交付，同时提供抗审查、网络分区容错和去中心化等特性。总体而言，IPFS 节点目前已广泛分布在全球各地和多个自治系统中，具有较强的网络健壮性和去中心化程度，但同时，在访问控制、参与激励、内容可用性等方面 IPFS 仍面临着挑战。"
  },
  
  {
    "title": "RWA 代币化浪潮：万物皆可加密",
    "url": "/posts/rwa/",
    "categories": "RWA",
    "tags": "defi, rwa",
    "date": "2023-07-04 12:00:00 +0800",
    





    
    "snippet": "DeFi概览公链的核心价值主张是提供去中心化、可信的通用结算层来解决价值流转中的协调问题。自比特币作为“点对点的电子现金系统”诞生以来，区块链技术的应用版图不断拓展，但直到去中心化金融(DeFi) 的兴起，这一技术的巨大潜能才真正得以彰显。公链的特性赋予了 DeFi 许多得天独厚的优势，相较于传统金融体系 (TradFi)，DeFi 具有更高的开放性、透明性、可用性、效率、灵活性和可创新性。...",
    "content": "DeFi概览公链的核心价值主张是提供去中心化、可信的通用结算层来解决价值流转中的协调问题。自比特币作为“点对点的电子现金系统”诞生以来，区块链技术的应用版图不断拓展，但直到去中心化金融(DeFi) 的兴起，这一技术的巨大潜能才真正得以彰显。公链的特性赋予了 DeFi 许多得天独厚的优势，相较于传统金融体系 (TradFi)，DeFi 具有更高的开放性、透明性、可用性、效率、灵活性和可创新性。许多传统金融体系中的基本要素已经以链上形式被重塑，包括：      P2P支付（闪电网络，Flexa）        现货交易（Uniswap，Curve）        借贷市场（Aave，Compound）        衍生品（GMX，dYdX）        合成资产（Synthetix，Alchemix）        资产管理（Yearn，Beefy）        保险（Nexus Mutual，Unslashed）  经过近几年的蓬勃发展，尽管公众对加密行业还存在认知偏差，但 DeFi 生态系统已经证明了其具有抵抗极端市场波动、快速去杠杆化事件以及中心化加密机构崩溃的韧性。截至目前，DeFi 生态系统总锁定价值 (TVL) 峰值超 1800 亿美元，日交易量达数十亿美元，日收入达数百万美元。图1 DeFi生态总锁定价值  数据源：DefiLlama与传统金融体系相比，链上金融系统在许多方面都有优势。然而 DeFi 的历史快速增长，很大程度上是受资本轮动和通胀性代币激励带来的不可持续收益驱动。DeFi 目前所面临的主要限制因素，是其多数项目形成了一个与传统企业和服务几乎没有联系的封闭经济体系，这无疑是明珠弹雀，大材小用。2022 年，加密市场经历寒冬，诸如 Luna 和 FTX 崩溃的黑天鹅事件导致 DeFi 市场大幅衰退。大量项目面临着不良代币经济学所带来的通胀压力，代币价值下跌超 90%。Defillama 数据显示，过去一年中，DeFi 的总锁定价值受加密资产价格下跌的影响，从高峰时的 1800 亿美元下降至约 390 亿美元，跌幅超过 78%，其规模与传统资产相比不可同日而语。同时，DeFi 收益也大幅减少，轻松获得高额收益的时代已经一去不回。以头部借贷项目 Compound 和 Aave 为例，根据 LoanScan 的数据，Compound 和 Aave 上的 USDC 存款利率分别下降至 1.86% 和 5.22%，远低于美国国债利率。由于 TradFi 市场风险较低，DeFi 参与者纷纷开始退出，将资金转移到 TradFi 市场，以获得更好的风险回报比。图2 DeFi协议收益率 数据源：LoanScan这也引发了 DeFi 行业的讨论，市场参与者寻求更高、更可持续的收益，并探索流动性质押等新的趋势和机会。2023 年，现实世界资产开始吸引市场的注意，越来越多的 DeFi 协议开始涉足传统金融市场，如股票和债券融资。在此背景下，RWA 领域成为了关注的焦点，将现实世界资产代币化成为了弥合加密市场和传统金融市场差距的终极解决方案。RWA 101什么是RWARWA，即现实世界资产 (Real World Asset) 的代币化，是将存在于链下的有形或无形资产的所有权价值（以及任何相关权利）转化为加密代币的过程。由此可以在没有中心化中介的情况下传输和存储数字所有权，并在区块链上反映和交易资产的价值。任何具有明确货币价值的现实世界资产都可以通过 RWA 来表示。RWA 可以代表有形资产，如黄金和房地产，也可以代表无形资产，如知识产权、政府债券或分红协议等。            有形资产      无形资产                  房地产      碳排放额度              汽车      政府债券              贵金属      股票              艺术品      知识产权              俱乐部      投资基金      RWA 的概念在区块链行业并不新鲜，目前最成功的 RWA 是数字美元，例如 USDT 和 USDC 等美元稳定币，即美元的代币化、链上版本的美元。稳定币已成为加密行业不可或缺的关键基础设施。RWA的优势传统金融视角：与传统金融系统相比，公链和 DeFi 的核心价值主张为现实世界资产代币化带来了许多优势，主要包括：  提高效率：区块链账本作为可信数据源，减少了交易后的对账摩擦。原子结算使得资产的交付和支付可以同时完成，不再需要 T+2 的延迟。图3 传统金融与DeFi的对比  图源：Frigg      降低成本：传统金融体系 (TradFi) 依赖于中间人、背景调查和监管等中介系统。虽然这些中介系统在一定程度上提高了安全性和控制力，但也以牺牲市场效率和资产持有者的机会为代价。DeFi 通过减少或完全去除传统金融中的中介系统，能有效地将金融市场的后台去中心化。国际货币基金组织 (IMF) 2022 年全球金融稳定报告中指出，与传统金融系统相比，DeFi 的自治协议大大减少了对中介机构的需求，从而大幅降低相关的劳动和运营成本。而在传统金融系统中，由于中介系统较为复杂，这部分成本通常很高。    图4 不同金融系统的边际成本对比  图源：国际货币基金组织        增加透明度：公链可以实时审计，使得验证资产抵押品质量和系统风险暴露成为可能。通过显示链上活动的公共数据面板，还可以减少记录保管方面的争议。DeFi 市场参与者可以直接获得交易流向、资产所有权和市场价格等信息，这些信息在传统金融体系中往往是隐藏的。        内置合规性：复杂的合规性规则集可以直接编程到代币和涉及代币的应用中。还可以实现保护用户隐私的 KYC 工具，以在遵守相关法规的同时保护用户隐私。        促进创新：资产和应用逻辑不再相互分离，而是存在于共同的结算层中，这也为创造全新的金融产品提供了可能性。从碎片化的房地产基金到流动性的收入分享协议，代币化能够构建许多先前难以实现的产品。  DeFi 视角：RWA 也能对当前的 DeFi 生态发展起到推动作用。RWA 代表了可靠的现实资产价值，不会产生加密资产那样剧烈的价格波动，可以为 DeFi 协议提供高质量的抵押，增强背书能力，提升整体的稳定性和安全性。同时，RWA 通过将传统金融资产进行链上代币化，可以为机构投资者提供熟悉的投资工具，从而大规模吸引资金进入 DeFi，构建与传统金融的联通桥梁，将 DeFi 的潜在市场规模从数百亿美元扩大至数十万亿级。此外，RWA 还降低了机构投资者和风险偏好保守用户进入 DeFi 的门槛，这也是 DeFi 走向主流的关键。今年上半年，RWA 领域吸引了一些传统金融机构和加密原生机构的关注。首先，高盛宣布推出其数字资产平台 GS DAP，已协助欧洲投资银行 (EIB) 发行了一笔价值 2 亿欧元的两年期数字债券。随后，管理超过 1000 亿美元的私募股权公司 Hamilton Lane 将其 21 亿美元旗舰股权基金的一部分代币化，以便在 Polygon 网络上向投资者出售，并且电气工程巨头西门子发行了首笔价值 6000 万欧元的数字债券。其次，一些政府机构也涉足 RWA 领域，包括新加坡金融管理局 (MAS)，据称与摩根大通和星展银行合作。RWA现状数据随着越来越多的项目进入市场，RWA 生态呈现出多样性，规模稳步扩张。如图所示，当前 RWA 生态中的项目主要可划分为两大类：为 RWA 提供技术、监管和运营支持的基础设施类项目，以及专注于创造对各类资产（房地产、固定收益、股票等）的 RWA 需求的资产类项目。图5 RWA项目分类其中，基础设施类项目主要包括专门用于 RWA 的 Layer 1 区块链、使 RWA 上链的证券化/代币化项目以及确保投资者和发行方合规性的合规服务项目。而资产类大多是 DeFi 项目，可进一步分为三类:  基于美国债券、股票、房地产和艺术品等链下资产的固定收益类项目。这类项目可以使用现实世界资产作为抵押物，这是与其他 DeFi 借贷项目的主要区别。  基于公开市场资产的公共信贷类项目，这类项目为加密用户营造了类似美国债券等债券工具的加密投资基金。  基于无形资产（如碳信用额）的交易市场。图6 RWA借款人地域分布  图源：rwa.xyz根据 rwa.xyz 的数据，包括 Centrifuge、Maple、GoldFinch、Credix、Clearpool、TrueFi 和 Homecoin 在内的 8 个 RWA 借贷协议总共发放了 44.3 亿美元的贷款，借款人端的平均年化利率为 10.62%，主要为中低收入国家的用户，截至目前，尼日利亚的企业贷款最多，总计 21 笔，其次是墨西哥的 20 笔贷款及肯尼亚的 19 笔贷款。这些信贷类协议的收益高于大多数 DeFi 借贷，但 Maple Finance 在 2022 年机构动荡时违约了 6930 万美元的债务。图7 主要RWA协议借贷额  图源：rwa.xyz总体来说，RWA 市场还处在早期发展阶段，但已经出现了采用度提升和 TVL 增长的迹象。根据 Dune 的数据面板，以太坊上 $wCFG、$MPL、$GFI、$FACTR、$ONDO、$RIO、$TRADE、$TRU、$BST 等 RWA 持有地址的数量也在持续增加，当前已超过 4 万个。虽然绝对数量不高，但相较于一年前的 1.8 万个，持币地址数增长了一倍还多，增幅明显。图8 RWA生态持币地址数  数据源：Dune目前，根据 DeFi Llama 跟踪的协议数据，RWA 已成为 DeFi 中的第 10 大类别。需要注意的是，该数据很可能低估了当前 RWA 市场的规模，因为协议的统计范围有限，且私有链上的代币化操作难以获得即时数据。尽管如此，RWA 类别排名的上升和持币地址数的增加仍反映出 RWA 协议的应用越来越广泛。            Category      Protocols      Combined TVL                  1   Liquid Staking      109      $21.420b              2   Lending      276      $14.181b              3   Dexes      934      $12.622b              4   Bridge      45      $9.753b              5   CDP      97      $8.530b              6   Services      144      $4.946b              7   Yield      443      $3.006b              8   Derivatives      136      $1.346b              9   Yield Aggregator      107      $1.103b              10 RWA      23      $982.860m      Defi Llama 统计显示，截至 2023 年 7 月，RWA 领域的总锁定价值已经超过 7.7 亿美元，加密货币领域对 RWA 的兴趣正日渐浓厚。图9 RWA生态总锁定价值  图源：DefiLlama发展根据 BCG 的研究，到 2030 年，代币化资产规模预计将达到 16 万亿美元，占全球 GDP 的 10%，比 2022 年的 3100 亿美元大幅增长。即使达到 16 万亿美元，代币化资产仍只是当前全球资产总值（约为 900 万亿美元）的不到 1.8%，且未考虑未来全球资产价值增长。可以说 RWA 真正的目标市场是整个全球资产市场，因为任何可代币化的资产都可以通过 RWA 在链上表示。图10 2030年代币化资产规模预计将达到16万亿美元  图源：波士顿咨询但目前 RWA 领域要想进一步发展，需要满足两项主要的先决条件：  区块链基础设施的改进  监管法规的完善资产上链是 RWA 领域的关键点，要实现这点，一方面需要保证区块链上各协议间的安全性、隐私性和互操作性，另一方面也需要有明确的法律法规来定义链下资产和链上身份的合规性，保护用户的权益。这里主要就链上代币标准和审查制度进行简要探讨。代币标准在以太坊上，ERC-721 和 ERC-20 是最常见的代币标准，分别代表了非同质化代币和同质化代币标准。资产上链时，根据资产的不同属性，应遵循不同的代币标准。同质化代币和非同质化代币有以下特点：  同质化代币          可互换：每个代币单位具有相同的市场价值和有效性，换言之，代币持有者可以彼此交换资产，并就其具有相同价值达成共识。      可分割：资产可以分割成多份，精度由代币发行时的设置决定，分割后资产的价值与完整代币成比例。        非同质化代币：不能互换，不可替代，每个代币的价值都是独一无二的，每个代币都包含不同的信息和属性。多数资产可以采用同质化代币的标准，但对于债券或衍生品等资产，则更适合采用非同质化代币标准。随着 RWA 项目的不断发展，将出现更多样化的场景，简单的 ERC-20 和 ERC-721 将无法满足 RWA 代币化的需求。RWA 领域的 L1 链已经开始着手制定兼容 RWA 代币化的标准，例如 Polymesh。但目前多数 RWA 项目都建立在以太坊上，因此对 ERC 标准的进一步发展会更通用，理想的代币标准应当兼容 ERC-721 和 ERC-20 的功能，并能提供对用户隐私的保护。当前 ERC-3525 是主要的讨论对象，未来可能还会有更多的标准出现。出色的可操作性和灵活性，兼容 ERC-721 和 ERC-20 的功能。审查制度对于用作抵押的代币化现实资产，安全性至关重要。投资者在使用 DeFi 协议前，首要任务是进行尽职调查，选择使用优质开源代码、优先考虑有抵押贷款并提供严格的监管合规性的技术或服务。对于 RWA 相关项目团队，应提供两种解决方案:      最小化 KYC/AML 风险：在平台上对用户和/或交易进行 KYC 或 AML 检查，避免用户直接或间接与不当的对手方进行互动或交易。        密切监控：监控和检测 DeFi 用户可疑活动的产品和服务。需要有专门的合规团队根据识别、风险评估、验证和尽职调查来对用户进行审查。此外,还需要持续监控客户活动，以检测涉及欺诈或洗钱的行为。  RWA生态提供对 RWAs 的敞口的 DeFi 市场正在蓬勃发展，市场类型和 RWAs 所代表的底层资产类型都日益多样化。越来越多的加密原生公司和传统金融公司也参与其中。本节将对 RWA 生态进行分析，并介绍其中主要的协议和参与者。RWA主要市场围绕RWA资产类别存在各种各样的市场。在当今的 DeFi 中，RWA 是股权类 DeFi 市场、实物资产类 DeFi 市场和固定收益类 DeFi 市场的主要工具。股权和实物资产市场股权和实物资产市场在 RWA 领域相对较小，目前该领域中的协议不多。主要原因可能包含以下几个方面：  监管和合规要求： 公开股权和实物资产市场通常受到严格的监管和合规要求。在很多司法管辖区，这些市场只能由经过注册和审查的交易所提供。这使得在区块链上创建类似的合成资产更加困难，因为需要满足涉及实物资产的复杂法律和监管要求。  操作复杂性： 股票和实物资产需要实际的持有和交付，这在区块链上具有一定的操作复杂性。将股票和实物资产纳入区块链需要解决资产所有权的转移、存储和交付等问题，这些问题在合成资产领域相对较少。  流动性限制： 实物资产通常具有较低的流动性，交易和转让需要时间和成本。这导致了在区块链上进行股票和实物资产交易的挑战，因为投资者更倾向于更快速和高流动性的交易。  市场认可度： 在加密货币领域，合成资产和衍生品市场已经相对成熟，投资者习惯了这种方式。而股票和实物资产的区块链化在市场中尚属新颖，需要时间来建立投资者的信任和认可。Backed Finance 是为数不多提供公开股权 RWA 的协议，项目的目标是通过将传统金融市场中的股票等公开股权引入到区块链和 DeFi 生态系统中，为投资者提供更多的投资选择和机会。然而，由于公开股权的特殊性，Backed Finance 需要遵守严格的金融监管规定，确保其在代币化过程中符合法律和监管要求。例如，根据瑞士区块链法案，Backed Finance 必须在平台上完全备有相关的股权，并在兑现时能够实际地转移这些股权。这就要求 Backed Finance 在技术和法律层面都保持高度的透明度和合规性。最后值得一提的是，以 Synthetix 为代表的 DeFi 合成资产可以作为股权/实物资产 RWA 市场的一种替代方案。合成资产可以通过合约方式模拟实物资产的表现，投资者只需在区块链上持有合成股票代币即可获得类似于持有实际股票的收益，同时不必处理股票的实际买卖和存储问题。这在某种程度上解决了公开股权市场引入 DeFi 领域时面临的问题。固定收益市场固定收益市场在 RWA 领域占主导地位。与股权或实物资产市场相比，基于 RWA 的固定收益市场在交易流动性方面更为活跃，提供的产品更为丰富，市场参与也更加多样化。DeFi 中的私人信贷类 RWA 产品非常丰富，吸引了许多非传统的借款人和投资者。私人信贷类 RWA 产品由于可分割和流动的特性具备许多优势：      为借款人提供新的融资渠道    通过代币化，RWA 将原本整块的私人信贷投资打碎，降低流动性壁垒，使得借款人可以接触新的贷款人市场，从而更轻松地获得融资，尤其是那些在传统金融体系中可能难以获得贷款的个人或中小企业。        为投资者提供私人信贷市场准入    DeFi 投资者可以通过 RWA 参与传统上只对大型机构开放的私人信贷市场，获得投资机会。        借助 DeFi 的交易效率    RWA 建立在区块链和智能合约之上，使得私人信贷的交易、结算更高效透明。        实现风险分散    私人贷款可以通过 RWA 进行分散，有利于投资者管理风险。  RWA 私人信贷市场由许多协议构成，这些协议可以分为两组：提供基于资产抵押（或有担保）的私人信贷，以及提供无担保的私人信贷（抵押不足或无抵押）。资产抵押类 RWA 私人信贷协议与加密原生借贷协议类似，由借款人选择一种加密资产进行借贷，存入抵押品，并向流动性提供者支付利息。唯一区别在于，抵押品是现实世界资产而非加密货币。这为大量企业和机构打开了获取加密资产的大门。无抵押类 RWA 私人信贷协议则与加密原生协议有显著不同。Aave 等 DeFi 中主流的加密借贷协议，都要求借款人存入足够的加密抵押品，使其抵押价值超过贷款价值，即所谓“超额抵押”。这在去中心化和无信任的 DeFi 系统中有助于管理交易对手风险，但同时也限制了借款规模，降低了资金利用效率。为提升 DeFi 的资本效率并服务于无法满足超额抵押要求的借款人，无抵押的 RWA 私人信贷协议应运而生。这类协议通过中心化审查流程评估借款人资质，结合白名单钱包机制，使得企业和机构能够以少量或无抵押的方式借入加密资产。无抵押借贷大幅提高了资金利用效率，降低了机构的进入门槛，拓展了 DeFi 的商业边界，代表着 DeFi 正在逐步适应不同客户需求，是 DeFi 不断进化的一个缩影。虽然也有批评者认为无抵押借贷协议依赖中心化的 KYB/KYC 评估，天然与“去中心化金融 DeFi”不兼容，但另一方面，这类协议中其实引入了许多 DeFi 原语，如使用流动性池进行债务发行、利用区块链使借贷流程透明化等。除了私人信贷产品外，基于 RWA 的固定收益市场也提供公共信贷类产品。目前，公共信贷相较于私人信贷活跃程度较低，这可能与股权和实物资产市场中缺乏活跃度的原因相同：要提供公共工具的代币化形式，协议必须满足严格的监管准则并获得许可。尽管与私人信贷产品相比活跃程度较低，但目前仍有一些协议正在该领域发展，如 Ondo Finance、BondBlox 等。典型项目介绍MakerDAOMakerDAO 是一个基于以太坊的抵押借贷平台，在 RWA 领域取得了长足发展。使用 MakerDAO 的借款人可以将资产抵押在智能合约中，并从平台借入等值的 DAI 稳定币。智能合约持有用户的抵押资产，直到借出的所有 DAI 归还为止。只要抵押资产价值高于清算线，借款人就拥有对抵押资产的完全控制权。但若抵押资产价值下降导致抵押不足，合约会通过拍卖自动抵押资产，无需中间人介入，以去中心化的方式偿还贷款。图11 Maker借贷说明  图源：MakerDAO2020年，MakerDAO 正式将 RWA 纳入其战略重点，并发布了将 RWA 引入生态系统的计划。Maker 已经将抵押品范围从 ETH 扩展到包括代币化房地产、发票和应收账款形式的抵押品。Maker 协议的主要收入来源是稳定币 DAI 的贷款利息和清算罚金。现状：就 TVL 而言，Maker 是排名第二的 DeFi 协议，仅次于 Lido，在抵押借贷协议 (CDP) 中位居首位。目前 Maker 仅部署在以太坊上，根据 DefiLlama 数据，其 TVL 为 57.8 亿美元，30 天收入为 921 万美元，储备资金为 1.3 亿美元。治理代币 $MKR 已在 Coinbase、币安、Kucoin、Kraken、OKX、火币、Bybit 等交易所上市，30 天交易量 12.67 亿美元。图12 Maker协议总锁定价值/收入/储备资金  图源：DefiLlama目前，MakerDAO 的 RWA 金库总额已超过 20 亿美元。这些资产令 MakerDAO 能够增加DAI的发行量，也意味着有超 20 亿美元的 RWA 资产帮助维持 DAI 的稳定锚定。图13 Maker协议的RWA金库  数据源：Dune自 2022 年底以来，RWA 在 MakerDAO 的收入份额显著增长，目前占据了该协议收入的 61％，RWA 资产占据了 MakerDAO 总资产的 49％。图14 RWA为Maker协议提供了60%以上的收入  数据源：DuneMakerDAO 于 6 月购买了价值 7 亿美元的美国国债，使其总储备持有量达到 12 亿美元。具有 RWA 敞口的多元化抵押品基础使得 MakerDAO 能够在利用当前的收益环境的同时分散风险。优势:      在 EVM 和 L2 生态的支持下，MakerDAO 有更忠实的用户群以及比其他 L1 链更稳定安全的网络。        MakerDAO 的治理机制已在多轮牛熊周期中经受住了检验，对抵押品设置严格的准入门槛、实行超额抵押，以及有效的拍卖系统，确保了 DAI 与美元的汇率稳定。在极端情况下，也有紧急停机的应急预案。  风险:      治理攻击风险。$MKR 代币的短期大规模聚集可能导致治理权力过度集中，引发一系列治理攻击，如增加劣质抵押品、强制紧急停机、恶意修改风险参数等。随着 $MKR 升值，协议能应对这些风险，多数情况下能有效化解。        市场价格风险。当主流代币价格大幅波动时，大规模拍卖结算会增加市场上的代币供应，加剧流动性问题。这在过去两年主流代币大跌时就曾出现过，但协议本身并未出现大规模损失。  Ondo FinanceOndo Finance 是今年上半年最受关注的 RWA 项目之一，于 4 月份获得了由 Founders Fund 和 Pantera Capital 领投的 2000 万美元 A 轮融资。Ondo Finance 是一个去中心化的投资银行，主要在链下投资美国上市的加密货币基金，并与 Flux Finance 开展稳定币链上借贷业务，协议的收入来自 0.15% 的年度管理费。Ondo Finance 推出了四种代币化债券供投资者选择，包括：      美国货币市场基金 (OMMF)：Ondo 货币市场基金，评级 AAA，投资于信用评级高的美国政府债券、短期债券和其他债务工具，旨在保本；当前收益率 4.7%。        美国国债 (OUSG)：Ondo 短期美国政府债券基金，评级 AAA，投资于美国短期国债 ETF，当前年化收益率 5%，TVL 为 1.38 亿美元。        短期债券 (OSTB)：Ondo 短期投资级债券基金，是一只积极管理的交易所交易基金 (ETF)，评级 BBB-，旨在在确保保本和每日流动性的同时最大化当前收入。该 ETF 主要投资于短期投资级债务证券，平均投资组合期限通常不超过一年，当前年化收益率 5.3%。        高收益债券 (OHYG)：Ondo 高收益企业债券基金，评级 BB-，主要投资于高收益企业债务，当前年化收益率为 7.3%。  Flux Finance 是由 Ondo Finance 团队开发的去中心化借贷协议，是 Compound V2 的分叉版本，现已出售给了 Neptune Foundation。该协议支持 USDC、DAI、USDT 和 FRAX 的借贷，平均借贷利率约为 5%，OUSG 是其中唯一的抵押资产，不可借出。Flux 的主要目标是为 OUSG 资产创造效用，OUSG 持有者可通过 Flux 获得更多收益。图15 Ondo及Flux生态中的投资流程说明  图源：Ondo FinanceOndo Finance 中的投资和赎回流程如下：  投资流程:          投资者完成 KYC 流程，获得白名单资格。      签署基金文件，指定接收代币的钱包。      向基金合约转入 USDC 进行认购。      USDC 被兑换成美元，购买基础资产（例如 ETF）。      投资者获得代表基金份额的代币。        赎回流程:          投资者向基金合约转入代币申请赎回。      Ondo 卖出部分基础资产兑换成 USD。      USD 兑换成 USDC 并被转账到投资者钱包。      销毁投资者的相应代币，完成赎回。      整个流程涉及多方合作，通过代币化实现传统资产投资的链上化。投资者只需持有代币即可进行投资、赎回等操作。与链下资产和法币之间的转换由 Ondo 负责处理。用户在交易基金代币或参与许可的 DeFi 协议之前，必须通过 KYC/AML 流程。现状：在 ETH 上的 TVL 达到 1.38 亿美元，在 DefiLlama 的 RWA 类别中排名第二。根据 rwa.xyz 数据，Ondo Finance 依靠 OUSG 占据代币化国债市场的 27%，仅次于传统金融资产管理公司富兰克林邓普顿。图16 Ondo占据代币化国债市场份额的27%  数据源：rwa.xyz治理代币 $ONDO 的功能包括以下几点：      支付平台费用：用户在平台进行交易、借贷或其他金融活动时可能需要支付一定的费用，可以使用 $ONDO 支付。        投票权和治理：Ondo Finance 代币持有者可以参与平台的治理和决策过程。他们可以对平台升级、参数调整、提案采纳等进行投票，并对整体发展表达意见和建议。        奖励和激励：Ondo Finance 平台可能以 $ONDO 的形式发布代币奖励和激励，以吸引用户参与平台活动和生态建设。        借贷和抵押：在 Ondo Finance 平台上，用户可以使用 $ONDO 作为抵押物来获得借贷服务。持有 $ONDO 的人在使用 $ONDO 作为抵押物时可以获得更高的借款信用额度或更低的利率。  优势：  完全合规。不管是低风险的美国政府相关债务工具，还是高风险的 ETF，都是经过第三方会计披露审计的合法产品。且用户均需通过 KYC/AML 流程。风险：      由于大多数产品是由链下 ETF、美国政府债务工具等支持，尽管可以保证合规，但会承担更多来自加密世界之外的市场风险和信用风险，尤其是在 OHYG 和其他高风险公司债券方面。        背离加密精神。该项目目前采用了中心化+合规的新运营理念，治理代币的效用可能会被边缘化。区块链技术在去中心化研发方面毫无作为，仅仅服务于利润分成、账目记账和股份分发，这有悖于加密世界中多数项目的宗旨。  Maple FinanceMaple Finance 的主要业务是借贷/机构信贷。其在链上业务是提供 USDC、wETH 借贷服务，由独立的中心化资金池管理者运营，该管理者决定借贷对象、金额、利率、策略等。图17 Maple协议结构  图源：Maple FinanceMaple Finance 的收入有以下几个主要来源：  借贷费用：Maple Finance 对贷款收取借贷费用。费用根据借款金额和贷款期限计算，利率由资金池设置。  贷款处理费：作为一个平台，Maple Finance 对与贷款交易相关的处理费用进行收费。处理费用包括贷款申请费、借贷费和贷款结算费。  代币挖矿奖励：Maple Finance 通过代币挖矿机制向参与者分发奖励。Maple 代币持有者可提供流动性或参与借贷池获得奖励。  平台治理费用：作为借贷资金池的管理者，Maple Finance 收取一定比例的平台治理费用。这些费用用于支持和维护平台的运营，包括开发新功能、进行安全审计和维护社区治理。现状：在 TVL 方面，Maple Finance 在 DefiLlama 上排名第 161 位，在无抵押贷款协议中排名第三，总 TVL 为 3300 万美元，未偿还贷款共 29 笔（由于中心化的性质，借款对象是大型机构，数量较少），总额超 3.32 亿美元。Maple 先前专注于无抵押贷款，目前已逐渐涉足了基于 RWA 的贷款。之前，无抵押的加密货币贷款让 Maple 遭受了超过 5000 万美元的坏账损失。图18 RWA私人信贷的未偿债务  数据源：DuneMaple 在四月推出了美国国债池，由美国国债和逆回购协议支持，允许非美国许可的投资者和实体直接投资美国国债。目标净年化收益率为当前美国国债月收益率减去 0.5% 的管理费用。$MPL 代币是 Maple Finance 平台的原生代币，作用如下：  支付交易费用：$MPL 代币可用于偿还在 Maple Finance 上的借贷费用。$MPL 持有者可会获得折扣或其他福利以持续持有。  社区治理：$MPL 持有者可以参与 Maple Finance 平台的治理决策，并拥有一定的投票权利，有资格参与决定协议参数、协议升级和其他重要事项。  接收分红：$MPL 代币持有者有资格分享 Maple Finance 借贷池的利润。这些利润可能来自借款人支付的利息或其他收入来源，按比例分配给 $MPL 持有者。  激励机制：Maple Finance 可通过向 $MPL 持有者提供激励来促进其生态系统的增长。激励包括空投、奖励或其他形式的优惠，以刺激用户参与并支持平台的增长。优势：运营安全，信用风险由资金池的管理者管理，作为回报收取管理费。流动性提供者可同时享受较低的利率和较低的违约风险。风险：  信用风险。借贷池的管理者、借贷对象由中心化机构审批，债务主要依赖信用而非资产抵押（抵押资产来自池子管理者）。如果发生大规模机构违约，可能会导致无法偿付。  门槛较高。为了维护高安全级别，借贷门槛设定较高，阻碍了大多数用户使用该服务，因此社区不太活跃。CentrifugeCentrifuge 于 2017 年上线，是最早将 RWA 整合进 DeFi 的项目之一，也是 MakerDAO 和 Aave 等头部协议的技术解决方案提供者。Centrifuge 同样是一个链上信贷生态系统，旨在为中小企业提供一个在链上质押资产以换取流动性的渠道。Centrifuge 允许任何人创建链上信贷基金和抵押贷款池。借款人可以通过 Tinlake 将实物资产进行代币化，Tinlake 是一个由智能合约驱动的开放资产池。Centrifuge 是最早将分级纳入其合约的协议之一，实物抵押品将根据风险和回报被分为两类代币，$DROP 和 $TIN，分别代表优质贷款的固定利率和次级贷款的浮动利率。投资者可以根据自己的风险偏好和收益预期选择投资 $DROP 或 $TIN。目前，Centrifuge 的借贷是完全免费的。图19 Centrifuge协议的分配瀑布  图源：Centrifuge现状：5 月 23 日，Centrifuge 宣布推出新的Centrifuge App 以取代 Tinlake。新 App 提高了 KYC 和投资流程的速度，增加了 KYB（了解你的企业）自动化功能，并为即将到来的多链支持奠定了基础。Tinlake 之前的数据将自动迁移到新 App。根据官方数据，Centrifuge 的 TVL 为 2.01 亿美元，基础资产总值 3.97 亿美元。$CFG 是 Centrifuge 的原生代币，作为链上治理工具，使 $CFG 持有者能够监督 Centrifuge 协议的发展。$CFG 也可以用作交易手续费。优势：  在降低融资门槛的同时，从现实世界的资产中获得收益。Centrifuge 基本模拟了传统金融中企业的信贷借款过程；  全面致力于合规。Centrifuge 遵循美国资产证券化的法律框架构建。风险：  贷款违约风险。根据 rwa.xyz 的数据，Centrifuge 有 1320 万美元的贷款已经逾期 90 天以上。GoldfinchGoldfinch 与 Maple Finance 类似，是面向链下的债券和金融科技实体的去中心化信贷协议，可提供零抵押的 USDC 信用贷款。Goldfinch 的模式类似传统金融中的银行，但有一批去中心化的审计人、信用分析师和投资者。借款人在申请贷款之前，必须由被协议认可的去中心化审计人批准。审计人是独立实体，须质押治理代币 $GFI 以获得审查借款人的权限并获取奖励。审计人对一个借款人是否能在 Goldfinch 上开立信用额度进行投票。当审计人的投票达成共识时，将被奖励 $GFI 代币。通过审计人的验证后，借款人为信贷额度提议的交易条款将被构建成一个 DeFi 借贷池。随后，投资者可以选择如何配置资本。与 Centrifuge 类似，Goldfinch 的债务产品中存在着优先级结构。Goldfinch 投资者可以将资本分配给单个池（成为“资助人”），或通过高级池自动分配资本到整个协议（成为“流动性提供者”）。将资本分配到单个池，说明资助人在评估后愿意为该池提供风险更高的第一损失资本，以获取更高收益。而通过在整个协议中分配流动性，投资者的资金将得到分散，并被 Goldfinch 协议视为比资助人的第一损失资本更“高级”，当借款人向借贷池还款时，该借贷池会优先偿还高级池中的本金和利息，因此风险更低。图20 Goldfinch协议借贷流程说明  图源：GoldfinchGoldfinch 目前有两个 ERC20 原生代币，$GFI 和 $FIDU。      $GFI 是 Goldfinch 的核心原生代币。可用于治理投票、审计人抵押、投票奖励、支持者质押、协议奖励。        $FIDU 代表流动性提供者在高级池中的存款。当流动性提供者向高级池提供资金时，会收到等额的 $FIDU。$FIDU 可以在 Goldfinch dApp 中按高级池的净资产价值汇率减去 0.5% 的提现费用兑换成 USDC 。$FIDU 的汇率会随着高级池中的已支付利息的增加而上涨。  现状：目前，Goldfinch 下所有贷款的未偿本金总额为 1.02 亿美元，累计损失率为 0%，已偿还本息总额为 2774 万美元。过去 30 天，协议收入 95.97 万美元。目前为止还没有不良债务。图21 Goldfinch协议未偿贷款  数据源：Dune图22 Goldfinch协议总还款额  数据源：Dune优势：  降低了借款门槛，在一定程度上帮助信用较低的用户获得贷款。  与传统金融平台相比，Goldfinch 的流程由智能合约处理，易用性更好。风险：  DeFi 是全球性的，各国法律的不同是 Goldfinch 面临的主要问题，可能导致运营成本过高。  Goldfinch 高级池因缺乏抵押品而面临违约风险。发展趋势RWA Layer 1随着 RWA 生态的发展，RWA 专有的 Layer 1 网络已经出现，这一趋势值得关注。目前，主要的 RWA 协议均部署在以太坊、BNB Chain 等 L1 公链上。虽然部署在公链上有着开发便捷、加密原生网络效应等一系列有点，但同时也存在运营和技术上的问题。公链天然是开放的，不受监管或权限许可逻辑约束。这与许多 RWA 协议的监管和许可要求存在结构性冲突。因此，现有 RWA 协议只能使用各种补丁方案限制平台访问以满足监管要求（如手动设置白名单、限制前端、设置代币权限账户等）。从运营角度看，公链现有的代币标准以及完全透明的特性，在 RWA 协议场景下也可能不太合适。链上数据完全透明无法满足机构用户对于安全性和隐私性的较强需求。公链的代币标准虽使得 DeFi 应用的智能合约得以高效开发和运行，但往往无法完全代表现实世界资产的特殊性。以太坊的 ERC-20 标准为例，发行方无法对代币实现回收、股份管理、身份管理、批处理、链下授权等等行为，这为资产的管理带来了更大的难度。因此，现有的公链无法实现金融市场对资产管理的复杂操作。为应对这些结构和运营方面的局限，一些人将目光投向了联盟链。联盟链几乎与公链同时出现，是指由若干组织或机构共同参与管理的区块链，每个组织或机构控制一个或多个节点，共同记录交易数据，只有经许可的组织或机构能够在联盟链中发起交易或对数据进行读写。联盟链有以下几个特点：      部分去中心化，节点数量有限且对应于实体机构，链上数据不会默认公开，验证交易或发布智能合约需获得联盟许可；        可控性较强，公链的区块信息一旦形成将不可篡改，但联盟链中只要大部分机构达成共识，即可修改区块数据；        交易速度快，节点较少，达成共识容易。  联盟链的这些特点似乎很适合 RWA 领域，但笔者并不认为传统的联盟链能够成为 RWA 赛道的主流：  加密行业的发展离不开去中心化和开放性，联盟链并不带来财富效应，只能作为技术手段，难以吸引投资者和机构入场；  RWA 赛道的发展，尤其在合规化和白名单上，确实具有联盟链的一些特征，但联盟链权力过于集中，无法实现民主化和透明化，难以发挥 DeFi 和区块链技术相较于传统金融系统的优势。当前，已有一些项目在构建针对 RWA 赛道的垂直L1链以满足其特殊需求。如 Polymesh，是一条“公有许可链”，采用 Polkadot 开发的提名权益证明 (NPoS) 共识模型来明确网络的角色、规则和激励措施。链上代币标准受 ERC-1400 启发，并提供了更完善的功能和安全性，以方便链上资产的发行和管理。以及 MANTRA Chain，一个基于 Cosmos SDK 构建的 L1 链，旨在成为企业之间相互协作的网络，吸引企业、开发人员构建 NFT、游戏、元宇宙、DEX 等应用。 MANTRA Chain 可以通过 IBC 跨链实现与 Cosmos 其它生态的互操作，也与 EVM 链兼容。另外，Inatain Markets 也在 Avalanche 上启动了一个子网，专为资产支持证券的链上许可发行和交易设计。未来，随着 RWA 生态的发展，这类 RWA 垂直 L1 链能否真正解决 RWA 协议所面临的问题，还有待进一步观察。监管与执行机制RWA 成功连接起了传统金融和 DeFi，这一创新具有深远意义。但要使这种融合长期可持续，RWA 的监管和执行机制必须跟上创新的步伐。一方面，现有加密行业相关法律不完善，对代币属于商品还是证券仍无定论。从目前来看，除比特币之外的所有加密货币都会被归为证券。这使得资产、发行方、交易市场、甚至链上合约、公链节点都需要受到监管。为了配合现有监管体系， 大部分 RWA 项目不得不对用户进行 KYC/ AML 流程。另一方面，现实资产是受地域限制的。RWA 的核心在于信用机制，促成全球性流通的关键在于设立国际通用、具备强制执行力的法案，这将有助于 RWA 生态的持续发展。但目前大多数国家都缺乏明确规管 RWA 的法规。只有瑞士、法国等极少数国家制定了相关法规来指导 RWA 的运作，明确协议应如何将现实世界资产引入区块链。未来，香港和新加披等加密友好地区或许会成为试点，但总体而言 RWA 合规阻力仍然较大。此外，还需要建立有效的执行机制，以更好地保障 RWA 的价值。在未来几年内，将有大量RWA债务到期，以私人信贷领域为例，根据 rwa.xyz 数据，未来两年内将有近 2 亿美元的债务到期。若借款人违约，RWA 资产清算偿还过程相较于加密抵押品将十分繁琐，建立专门的链下清算流程和法律约束措施是十分必要的。图23 未偿还的RWA私人信贷  数据源：rwa.xyz目前，在 Centrifuge 和 Maple 等协议上已经出现了几起 RWA 的坏账案例。为了防止坏账的进一步蔓延，更好地为贷款人提供服务，建立清算借款人抵押品的监管和执行机制至关重要。用户行为许多 RWA 协议发展的源动力都来自于对 DeFi 长期前景的看好，但短期内，RWA 领域的热潮主要是受宏观利率上升和 DeFi 收益率下降所推动。熊市导致的币价下跌、收益率下降让投资者的目光逐步转向了 DeFi 之外，以寻求更稳定的高收益投资。由此，各类 RWA 协议加快了开发，以满足投资者的需求，在 TradFi 和 DeFi 之间建立起了桥梁。归根到底，当前 RWA 用户真实的投资需求是链上 RWA 项目带来的稳定高收益。但随着经济环境的变化，当加密行业步入牛市时，RWA 项目是否还能继续受到关注、积蓄力量并进一步发展，仍需拭目以待。结论总结而言，RWA 项目正在成为传统金融 (TradFi) 和去中心化金融 (DeFi) 之间的桥梁，现实世界的房地产、债券、碳信用等传统资产首次被引入了区块链。短期内的宏观利率上升结合 DeFi 长期的效率和机会优势，正在共同驱动着 RWA 生态的发展。未来，相信 DeFi 原生协议和传统金融机构将继续建设 RWA 生态。现实世界资产的代币化展现出了区块链技术的强大，为加密领域新一批用户的引入创造了条件。目前，已经有越来越多的传统金融实体认识到了 RWA 的价值，高效、透明、低成本的优势让 RWA 成为了替代现有解决方案的可靠选择。从更宏观的角度来看，RWA 为加密世界和现实世界架起的桥梁具有里程碑意义，这代表着世界正借由技术变得更加互联互通，DeFi 不再孤立于现实世界和传统金融之外，而区块链技术也通过实际的用例彰显了其作为一项革命性技术的巨大潜力。但需要注意的是，这座桥梁仍需要现实资产与加密资产之间完善的法律和可靠的机制作为支撑，确保信息交流畅通无阻、流程定义清晰有效。展望未来，RWA 叙事的潜力远未被完全发掘，持续创新和发展仍然值得期待。相信在投资者、DeFi 协议、传统金融实体和政府等各方的共同建设下，未来这一领域将出现更多样应用场景，继续开拓加密世界的边界。"
  },
  
  {
    "title": "以太坊 PoS 共识协议详解（二）：Casper FFG",
    "url": "/posts/casperffg/",
    "categories": "Blockchain, Ethereum",
    "tags": "ethereum, pos",
    "date": "2023-05-21 12:00:00 +0800",
    





    
    "snippet": "本文是介绍以太坊 PoS 共识协议的第二部分，主要对最终性工具 Casper FFG 进行全面说明。许多文章都介绍过 Casper FFG 的运行机制，但很少解释这些机制为什么有效。希望本文能为读者理解 Casper FFG 的有效性提供一些参考。总体而言，Casper FFG 的机制并不复杂，其有效性主要归功于为两个重要理念：两阶段提交，以及可问责安全性。两阶段提交赋予了 Casper F...",
    "content": "本文是介绍以太坊 PoS 共识协议的第二部分，主要对最终性工具 Casper FFG 进行全面说明。许多文章都介绍过 Casper FFG 的运行机制，但很少解释这些机制为什么有效。希望本文能为读者理解 Casper FFG 的有效性提供一些参考。总体而言，Casper FFG 的机制并不复杂，其有效性主要归功于为两个重要理念：两阶段提交，以及可问责安全性。两阶段提交赋予了 Casper FFG 经典的共识安全性，令其能够声明区块是最终化的，并确信诚实验证者永远不会回滚最终化的区块。但两阶段提交生效的条件是诚实验证者控制至少三分之二的质押权益。因此，针对作恶验证者超过三分之一的情况，Casper FFG 还提供了可问责安全性作为额外的保障。如果链上出现最终性冲突，至少三分之一的总质押权益会经由罚没作恶验证者被销毁。概述Casper FFG 是一种元共识协议。可以作为覆盖层运行在底层共识协议之上，为其添加最终性。回顾一下，最终性是保证链中的一些区块永远不会被回滚的属性。在以太坊的权益证明共识中，底层协议 LMD GHOST 并不提供最终性，验证者构建竞争链不会受到惩罚。Casper FFG 的作用就是作为“最终性小工具 (finality gadget) ”来为 LMD GHOST 添加最终性。Casper FFG 利用了权益证明协议的性质：参与者是已知的（管理质押以太坊的验证者）。因此，可以通过计数来判断是否获得多数诚实验证者的投票（确切地说是管理多数质押权益的验证者的投票，每个验证者的投票都会根据其管理的权益价值被加权，简单起见，这一点后文不再赘述）。协议在异步网络 (Internet) 上运行，这意味着如果想同时实现安全性和活性，最多只能容忍 $\\frac{1}{3}$ 的验证者存在恶意或故障。这是共识理论中的一个著名结论，推理如下：  设共有 $n$ 个验证者，其中 $f$ 个可能以某种方式存在故障或恶意。  为了保持活性，协议需要在只收到 $n-f$ 个验证者的响应后就能做出决定，因为那 $f$ 个验证者可能无法或拒绝进行投票。  但由于是异步环境，所以未收到的 $f$ 个响应可能只是延迟了，并非故障导致。  因此，必须考虑在收到的 $n-f$ 个响应中，最多可能有 $f$ 个来自有故障或恶意验证者。  为了保证在收到 $n-f$ 个验证者响应后诚实验证者的仍占多数，则需要 $\\frac{(n-f)}{2} &gt; f$，即 $f &lt;\\frac{n}{3}$。总之，与所有经典拜占庭容错 (BFT) 协议一样，Casper FFG 在不到三分之一的验证者集存在故障或恶意时，能够提供最终性。而特别之处在于，即使超过三分之一的验证者存在故障或恶意行为，Casper FFG 仍能够提供经济最终性（可问责安全性）。本文将单独考虑 Casper FFG，不会过多花费时间讨论其如何与 LMD GHOST 集成。Casper FFG 论文也几乎没有涉及到底层区块链共识机制。后续文章将继续探讨两者是如何结合为 Gasper 的。命名Casper FFG 协议的名称由两部分组成：Casper协议名中的 Casper 部分似乎出自 Vlad Zamfir 之手。正如他在《History of Casper》Part 5 中解释的：  在本章中，我将讲述 Casper 协议的诞生，它是将 Aviv Zohar 和 Jonatan Sompolinsky 的 GHOST 协议应用于权益证明机制之上的产物。我称之为“友好的幽灵”，因为其激励机制旨在对抗寡头垄断的审查制度：这些激励机制迫使卡特尔对非卡特尔验证者保持友好。这里提到的 GHOST 协议就是在前一篇文章中介绍过的协议。Casper 这个名字其实来自 Casper the Friendly Ghost，是一个自 1940 年代以来就存在的卡通角色。Zamfir 最初将协议命名为 Casper TFG（The Friendly Ghost，友好的幽灵），后来又改名为 Casper CBC（Correct By Construction，通过构建实现正确性）。Vitalik 的 Casper FFG 与 Zamfir 的 Casper TFG/CBC 几乎同时出现，但两者并没有什么共同之处，Casper FFG 也没有使用 GHOST 协议。FFG如 Casper FFG 论文标题所写，FFG 代表“Friendly Finality Gadget（友好的最终性小工具）”。这一命名在借鉴了 Zamfir 的 TFG 的同时，也表明了 Casper FFG 不是一个完全独立的区块链协议，而是一个可以添加到底层共识协议中实现最终性的“小工具”。术语Epoch 和检查点为了就最终性做出决定，Casper FFG 机制需要处理来自至少 $\\frac{2}{3}$ 的验证者集的投票。在以太坊中，验证者集非常庞大，成千上万个验证者的投票同时广播、传播和处理是不现实的。为了解决这个问题，投票会分散在 Epoch 的持续时间内进行1，在 Eth2 中，一个 Epoch 由 32 个间隔 12 秒的 Slot 组成。在每个 Slot 中，总验证者集合的 $\\frac{2}{3}$ 会被安排广播投票，因此每个验证者在每个 Epoch 会投票一次。实现中为了提高效率将每个验证者的 Casper FFG 投票与其 LMD GHOST 投票绑定在一起，但不是必须如此。为了确保在不同时间投票的验证者拥有共同的投票对象，我们令其对检查点 (Checkpoint) 进行投票，检查点是 Epoch 的第一个 Slot。第 $N$ 个 Epoch 的检查点位于第 $32N$ 个 Slot（Slot 和 Epoch 的编号从 0 开始）。一个 Epoch 分为 32 个 Slot，每个 Slot 通常包含一个区块。每个 Epoch 的第一个 Slot 是其检查点  “最终化 Epoch”这种表述是不正确的，Casper FFG 最终化的是检查点。当完成第 $N$ 个 Epoch 的检查点最终化时，也就最终化了含第 $32N$ 个 Slot 在内的所有内容，包括整个第 $N-1$ 个 Epoch 和第 $N$ 个 Epoch 的第一个 Slot。但第 $N$ 个 Epoch 并没有完成最终化，还有 31 个 Slot 未最终化。暂且假设每个 Slot 都包含一个区块，因为原始的 Casper FFG 检查点是基于区块高度而不是 Slot 号的。在协议中，一个检查点对象只包含其 Epoch 编号以及 Epoch 第一个 Slot 中区块的哈希根 (root)：class Checkpoint(Container):    epoch: Epoch    root: Root合理化和最终化与经典的 BFT 共识协议类似，Casper FFG 通过两轮流程（合理化 Justification，最终化 Finalization）来实现最终性。第一轮：我向网络广播对当前 Epoch 的检查点的看法（记为 $X$），并获取其他人的看法。如果绝大多数验证者也支持 $X$，那么我就可以对 $X$ 进行合理化。合理化仅限于我的网络视图：在这一阶段，我相信网络中的大多数验证者都认为 $X$ 对最终化有利。但是我还不知道网络上的其他人是否也得出了同样的结论。在对抗性条件下，可能有足够多的其他验证者无法就 $X$ 做出决定。第二轮：我广播“我接收到的绝大多数验证者支持 $X$” 的消息（即，“我已经合理化了 $X$”），并询问其他验证者是否认为绝大多数验证者支持 $X$（即，他们是否已经合理化 $X$）。如果绝大多数验证者同意 $X$ 是合理的，那么我将最终化 $X$。最终化是全局属性：一旦一个检查点被最终化，任何诚实的验证者都不会将其回滚。即使其他验证者还没有在其视图中将该检查点标记为最终化，我也知道他们至少将其标记为了已合理化，并且没有任何（不可惩罚的）行为能够撤销该合理化。总结一下，我要确定整个网络都同意不会回滚某个区块，需要以下步骤2：      第一轮（理想情况下可合理化）    a. 我告诉网络我支持的检查点。    b. 我从网络中获取其他验证者支持的检查点。    c. 如果 $\\frac{2}{3}$ 的验证者与我意见相同，我将合理化该检查点。        第二轮（理想情况下可最终化）    a. 我将我已合理化的检查点告诉网络。    b. 我从网络中获取其他验证者已合理化的检查点。    c. 如果 $\\frac{2}{3}$ 的验证者与我意见相同，我将最终化该检查点。  简而言之，当我合理化一个检查点时，我承诺永远不回滚它。当我最终化一个检查点时，我知道所有诚实验证者都承诺永远不回滚它。理想情况下，第一轮将合理化检查点第二轮将最终化检查点在理想条件下，每轮持续一个 Epoch，因此合理化检查点需要一个 Epoch，最终化检查点需要再花费一个 Epoch。  在 Epoch N 开始时，我们的目标是合理化检查点$N-1$ 并最终化检查点$N-2$。  具体来说，协议中最终化一个检查点需要 12.8 分钟，即两个 Epoch。在 Casper FFG 中，这两轮是重叠流水线式的，因此尽管最终化一个检查点总共需要 12.8 分钟，但仍然可以做到每隔 6.4 分钟（即每个 Epoch）最终化一个检查点。  注意，在协议外部，如果不存在长链重组，则可以稍早于 12.8 分钟预见到某个检查点可能被最终化。例如可能在第二轮进行到 $\\frac{2}{3}$ 的时候（大约 11 分钟）就收集了足够多的来自 $\\frac{2}{3}$ 验证者的投票。但协议内部的合理化和最终化仅在 Epoch 结束处理期间进行。源和目标 链接和冲突Casper FFG 中的投票包含两个部分：源 (Source) 检查点投票和目标 (Target) 检查点投票，即见证数据中的 source 和 target 字段：class AttestationData(Container):    slot: Slot    index: CommitteeIndex    # LMD GHOST vote    beacon_block_root: Root    # FFG vote    source: Checkpoint    target: Checkpoint源和目标投票同时以链接 (Link) 的形式进行广播：$𝑠→𝑡$，其中 $s$ 是源检查点，$t$ 是目标检查点。目标投票的作用是广播验证者视图中下一个应该被合理化的检查点，是验证者的第一轮投票。目标投票对不会滚该检查点的软（条件性的）承诺，条件是收到 $\\frac{2}{3}$ 的验证者对该检查点的承诺。源投票的作用是广播验证者已经收到网络中 $\\frac{2}{3}$ 的验证者支持检查点$s$，且 $s$ 是已知符合该条件的最新检查点，是验证者的第二轮投票。通过源投票，验证者将之前不回滚检查点的软承诺升级为永远不回滚的硬（无条件）承诺。诚实验证者的源投票始终是其链视图中最高的已合理化检查点。其目标投票将是当前 Epoch 的检查点，该检查点是源检查点的后代。源和目标检查点不必连续，跳过检查点是允许的。但是，当信标链运行平稳时，一个 Epoch 的目标投票将是下一个 Epoch 的源投票。从已合理化检查点到其后代链上检查点的链接是有效的。图中只展示了检查点，中间的区块被省略在有效的链接中，源检查点始终是目标检查点的祖先。否则，验证者会自相矛盾：源投票承诺了永远不会回滚检查点 $s$， 如果目标检查点 $t$ 不是 $s$ 的后代，那么实际上就是在投票回滚 $s$。但发布这样的无效链接并不算可罚没的违规行为。从已合理化检查点到非其后代链上的检查点的链接是无效的。这两个检查点是冲突的，因为它们都不是彼此的祖先评估 Casper FFG 投票时，只会考虑在区块中接收到的投票。与 LMD GHOST 的分叉选择不同，验证者不会考虑通过 Gossip 或任何其他方式接收到的 Casper FFG 投票。因为我们围绕最终性的决策必须有一个共同记录，而区块历史正是这样的共同记录。因此，前文提到“告诉网络”时，实际上是说验证者广播了一个见证，该见证将被区块提议者拾取并包含在了一个区块中。当提到“从网络获取”时，实际上是说验证者处理了包含在区块中的见证。绝对多数链接如上所述，链接是一个 Casper FFG 投票对，用于链接源检查点和目标检查点，$s→t$。当超过 $\\frac{2}{3}$ 的验证者（按权益加权）发布了相同的链接（且其投票被及时包含在区块中）时，链接 $s→t$ 就是一个绝对多数链接 (Supermajority Link)。Casper FFG 的机制在了解了大部分术语和关键概念之后，本节将详细介绍 Casper FFG 的运行方式。合理化当节点看到从已合理化检查点 $c_1$ 到检查点 $c_2$ 的绝对多数链接时，就认为检查点 $c_2$ 是已合理化的。节点已经看到绝对多数链接 $C_N→C_{N+2}$ ，因此将 $C_{N+2}$ 标记为已合理化。已合理化的检查点用阴影线标记并标有“J”合理化意味着我已经看到超过 $\\frac{2}{3}$ 的验证者集做出承诺不会回滚检查点 $c_2$，其条件是收到至少 $\\frac{2}{3}$ 同样承诺不回滚 $c_2$ 的验证者的确认。换句话说，如果一个验证者看到超过 $\\frac{2}{3}$ 的其他验证者承诺不回滚检查点$c_2$，那么该验证者本身也承诺不会回滚 $c_2$。最终化当节点看到从已合理化检查点 $c_1$ 到检查点 $c_2$ 的绝对多数链接，且检查点 $c_2$ 是 $c_1$ 的直接子检查点时，就会认为检查点 $c_1$ 是最终化的。换句话说，当一个已合理化检查点的直接子级也已合理化时，该检查点成为最终化的检查点。节点已经看到绝对多数链接 $C_N → C_{N+1}$，因此将 $C_{N+1}$ 标记为已合理化。由于 $C_{N+1}$ 是检查点树中 $C_N$ 的直接子级，因而将 $C_N$ 标记为最终化。最终化的检查点用交叉阴影线标记并标有“F”最终化意味着我已经看到超过 $\\frac{2}{3}$ 的验证者确认其已收到超过 $\\frac{2}{3}$ 的验证者的承诺，因此不会回滚检查点 $c_1$。此时检查点 $c_1$ 已经不可能被回滚，除非至少有 $\\frac{1}{3}$ 的验证者被证明反悔，并因此受到罚没。Casper 守则在 Casper FFG 论文中，为检查点定义了高度：如果 $c$ 是一个检查点，那么 $h(c)$ 就是该检查点的高度。检查点高度随距创世块的距离单调增加。在 Eth2 的 Casper FFG 实现中，检查点高度是检查点的 Epoch 编号：$h(c) = c.epoch$。前文提到，检查点包含区块哈希和 Epoch 编号这两个部分。如果这两部分中有任何一个不同，那么检查点就是不同的。Casper FFG 的可问责安全证明依赖于任何违反以下两条规定（或“惩罚守则”）的验证者都会受到罚没。禁止双重投票守则 1：验证者不得发布不同的投票 $s_1 → t_1$ 和 $s_2 → t_2$，使得 $h(t_1) = h(t_2)$。简单来说，验证者对任意目标 Epoch 只能进行最多一次投票。违反守则 1 的一种方式：使用不同的源检查点投票给相同的目标检查点：$0 → 3$ 和 $1 → 3$违反守则 1 的另一种方式：在同一个 Epoch 中投票给不同的目标检查点：$0 → 3$ 和 $0 → 3’$禁止包围投票守则 2：验证者不得发布不同的投票 $s_1 → t_1$ 和 $s_2 → t_2$，使得使得 $h(s_1) &lt; h(s_2) &lt; h(t_2) &lt; h(t_1)$。也就是说，验证者不得发布投票，使其链接包围或被其先前投票的链接包围。违反守则 2 的一种方式：链接 $0 → 3$ 包围链接 $1 → 2$违反守则 2 的另一种方式：仍是链接 $0 → 3$ 包围链接 $1 → 2$，但处于不同分支罚没机制任何违反 Casper 守则之一的验证者都将被罚没 (Slash)。这意味着其部分或全部质押权益将被扣除，并会被踢出验证者集。罚没机制让不良行为 （是可能导致冲突检查点最终化的行为）有了代价，是 Casper 可问责安全保证的基础。从协议内部检测违规行为比较困难，尤其检测包围投票可能需要搜索验证者大量的历史投票记录。因此，实现中依赖外部罚没检测服务3 来检测违规行为并将证据提交给区块提议者。验证者会对其发布的每个见证进行签名，因此，给定两个相互冲突的证明，只需验证其签名并证明验证者在发布时违反了守则即可。def process_attester_slashing(state: BeaconState, attester_slashing: AttesterSlashing) -&gt; None:    attestation_1 = attester_slashing.attestation_1    attestation_2 = attester_slashing.attestation_2    assert is_slashable_attestation_data(attestation_1.data, attestation_2.data)    assert is_valid_indexed_attestation(state, attestation_1)    assert is_valid_indexed_attestation(state, attestation_2)    slashed_any = False    indices = set(attestation_1.attesting_indices).intersection(attestation_2.attesting_indices)    for index in sorted(indices):        if is_slashable_validator(state.validators[index], get_current_epoch(state)):            slash_validator(state, index)            slashed_any = True    assert slashed_anyCasper FFG 论文中描述的协议假设，一旦证明违反了罚没条件，“验证者的全部存款将被没收”。Eth2 实现了该假设的一种变体，根据给定周期内被罚没的总权益比例来调整验证者的罚没比例。如果在 36 天的窗口期内，$\\frac{1}{3}$ 的总质押权益违反了罚没条件，那么违规者的所有权益都会被没收，与经典 Casper FFG 协议的假设一致。但如果窗口期内的总罚没很少，那么违规者几乎不会被没收权益。这种细微调整在实践中并不影响 Casper FFG 的保证，至少自信标链 Bellatrix 升级将 PROPORTIONAL_SLASHING_MULTIPLIER 常量设置为最终值以来如此。分叉选择规则Casper FFG 的分叉选择规则是对底层共识机制的分叉选择规则的修改。根据 Casper FFG 论文，底层共识机制必须：  遵循包含最高合理化检查点的链纯粹的 LMD GHOST 协议总会从创世块开始搜索链头区块。当被 Casper FFG 的分叉选择规则修改后，LMD GHOST 将从其已知的最高合理化检查点开始搜索链头，并忽略所有不属于最高合理化检查点子链的潜在链头区块。后续文章在讨论 Gasper 时会更详细地介绍这点。正是对底层共识协议分叉选择规则的这种修改赋予了其最终性。当节点在其本地视图中合理化一个检查点时，就承诺了永远不会回滚它。因此，底层链必须始终包含该检查点；所有不包含该检查点的分支都必须被忽略。需要注意的是，这种分叉选择规则与 Casper FFG 的可信赖活性保证是兼容的。Casper FFG 的保证Casper FFG 共识协议提供了两种与经典共识协议中的安全性和活性相类似但又不同的保证：可问责安全性 (Accountable Safety) 和可信赖活性 (Plausible Liveness)。可问责安全性经典的 PBFT 共识协议只能在不到三分之一的验证者作恶的情况下保证安全性。Casper FFG 在少于三分之一权益的验证者作恶的情况下，本质上提供了相同的安全保证：最终化的检查点将永远不会被回滚。此外，Casper FFG 还提供了额外的保证，即如果冲突的检查点最终化，那么代表至少三分之一质押以太坊的验证者将被罚没。 这被称为“可问责安全性”，因为我们可以准确识别哪些验证者行为不当并直接对其进行惩罚。这一额外的安全性保证并非并非通常意义上共识协议所指的安全性，而是具有加密经济学特性的安全性：不当行为会受到协议极强的反激励。通常被称为“经济最终性”。证明Casper FFG 可问责安全性的证明相当直观。这里将大致按照 Casper FFG 论文中的方式勾勒出证明过程，但会使用 Epoch 代替论文中更抽象的”检查点高度”。我们要证明的是：如果少于 $\\frac{1}{3}$ 的验证者（按权益加权）违反 Casper 守则，则两个冲突的检查点不能同时最终化。证明的思路是：证明最终化冲突检查点的唯一方式，是使一个绝对多数链接被另一个绝对多数链接包围，而这与命题中的假设相矛盾，包围投票是违反第二守则的。显然，在这一假设下，任意 Epoch 中最多只能有一个检查点被合理化。这是由禁止重复投票守则直接得出的。取 Epoch $m$ 和 $n$ 中的两个冲突的最终化检查点 $a_m$ 和 $b_n$。由于相互冲突，所以两检查点彼此都不是对方的后代。由上述观察可知，$m ≠ n$。在不失一般性的条件下，取 $m &lt; n$，则 $b_n$ 是更高的最终化检查点。一定存在一系列连续的已合理化检查点，从根检查点一直连到 $b_n$，且相互之间具有绝对多数链接。也就是存在一组 $k$ 个绝对多数链接 $\\lbrace r → b_{i_1}, b_{i_1} → b_{i_2}, b_{i_2} → b_{i_3}, \\dots, b_{i_k-1} → b_{i_k}\\rbrace$，其中 $i_k = n$，这是由合理化的定义得到的。因此，导向 $b_n$ 的已合理化检查点集是 $B = \\lbrace r, b_{i_1}, b_{i_2}, b_{i_3}, \\dots, b_{i_k-1}, b_{i_k}\\rbrace$。对于任意最终化检查点，例如 $b_{10}$，都存在一条由绝对多数链接组成的连续链，从根检查点 $r$ 一直连到它。这里的链接链合理化了检查点集 $B = \\lbrace r, b_1, b_4, b_5, b_9, b_{10}\\rbrace$ 阴影线表示检查点是合理化的（也可能最终化）；交叉阴影线表示检查点是最终化的（并标记“F”)考虑冲突的最终化检查点 $a_m$。根据最终化的定义，从 $a_m$ 到下一个 Epoch 的 $a_{m+1}$，必然存在绝对多数链接 $a_m → a_{m+1}$ 。显然，$a_m$ 和 $a_{m+1}$ 都不在集合 $B$ 中，且集合 $B$ 不包含检查点 $b_m$ 或 $b_{m+1}$，因为一个 Epoch 中最多只能有一个合理化检查点。因此，检查点对 $(a_m, a_{m+1})$ 必定落在集合 $B$ 的两个连续元素的 Epoch 之间，设为 $b_{i_{j-1}}$ 和 $b_{i_j}$。即存在一个 $j$ 满足 $i_{j-1} &lt; m &lt; m+1 &lt; i_j$。由此可见，必定存在绝对多数链接  $b_{i_{j-1}} → b_{i_j}$ 包围了绝对多数链接 $a_m → a_{m+1}$ 。包围链接和被包围链接都需要至少 $\\frac{1}{3}$ 的验证者违背 Casper 第二守则才可能存在，与假设不符。假设较早的冲突检查点 $a_6$ 最终化，则必然存在绝对多数链接 $a_6 → a_7$。$b$ 链上必然存在一条绝对多数链接（本例中为 $b_5 → b_9$) 跨越 $a_6 → a_7$这样就用反证法证明了，如果少于 $\\frac{1}{3}$ 的验证者（按权益加权）违反 Casper 守则，则两个冲突的检查点不能同时最终化。  为什么是三分之二？  最终化的阈值设定在 $\\frac{2}{3}$ 的原因其实并不像直观感觉上那么简单。注意，这里 $\\frac{2}{3}$ 的含义是总质押权益的 $\\frac{2}{3}$ ，与本文开头介绍的 PBFT 算法中共识生效的阈值含义并不相同。实际上绝对多数链接的定义可以选取任意大于 $\\frac{1}{2}$ 的值作为阈值。设此阈值为 $p$：若验证者集中比例为 $p$ 的验证者投票最终化一个检查点，则该检查点被最终化。  该值的设定实际是要平衡两个因素：      从活性角度看，协议的容错度为 $1-p$（多于 $1-p$ 的验证者不响应就无法达到 $p$）；    从安全性角度看，协议的容错度为 $2p-1$（最终化两个冲突检查点各需要 $p$，两者重复的部分为 $2p-1$）。    令 $1-p = 2p-1$ 得 $p=\\frac{2}{3}$ 。因此，将 $p$ 设置为 $\\frac{2}{3}$ 可以最大限度地提高对活性攻击的容错性（不到三分之一的作恶权益无法妨碍最终化），同时最大限度地提高对安全故障的容错性（如果最终化冲突检查点，至少三分之一的质押权益将被罚没）。  平衡安全性和活性的阈值是 $p=\\frac{2}{3}$经济最终性可问责安全性的证明依赖于以下两个条件：  不存在指向相同高度不同检查点的绝对多数链接。  不存在互相包围的绝对多数链接。这两个条件由两条 Casper 守则强制达成。任何违反守则的验证者都会被罚没，因此，在发生冲突最终化的情况下，可以保证至少 $\\frac{1}{3}$ 的质押以太坊被罚没。罚没机制使对链的攻击需要付出代价，成功攻击的成本巨大4。正如 Vitalik 所说：  如果一个区块被最终化，那么它就成为了链的一部分，改变它会非常昂贵。我们称之为“经济最终性 (Economic Finality) ”，因为最终性不是由软件而是由攻击成本保障的。验证者的质押权益是“正当行为保证金”，如果被证明违反了协议规则，就可以没收其权益。那么为什么需要经济最终性这一概念呢？经典 PBFT 并不依赖这样的结构来实现最终性。协议难道不能直接拒绝最终化冲突的检查点吗？区别在于 PBFT 可以奢侈地假设作恶验证者不到三分之一。在 Casper FFG 中，持有不到三分之一权益的攻击者同样无法最终化冲突的检查点。然而，在无许可区块链中，必须有防御措施来应对作恶权益超过三分之一的情况。而罚没机制就是这一防御措施，为协议提供了经济最终性保证：如果超过三分之一的验证者行为不当，虽然无法阻止其最终化冲突检查点，但可以令其付出巨大的代价。Vitalik 在一篇博文（论结算最终性）中说道：  我们不能保证“X 永远不会被回滚”，但我们可以保证稍弱的声明：“要么 X 永远不会被回滚，要么一大群验证者将自愿销毁自己的数百万美元的资产”。面对超过三分之一的攻击者和异步网络，验证者仅仅在协议内拒绝最终化冲突检查点是没有意义的。能够最终化冲突检查点的攻击依赖于将诚实验证者集分割开来，使其看不到彼此的投票，也不知道对方最终化的对象。经济最终性是应对这一强大攻击威胁的有力安全保证。在发生冲突最终化的情况下，最终的补救措施是人工干预。正如 Vitalik 在同一篇文章中所说：“围绕链上资产的用户社区可以用常识来区分哪个分叉不是攻击，而真正代表了最初被一致同意最终化的交易的结果”。可信赖活性Casper FFG 本身并不提供经典意义上的活性，即确保用户的交易被包含在链上。所有区块生产和链构建都由底层共识机制 (LMD GHOST) 负责。但我们希望 Casper FFG 在某种程度上具备活性：只要至少三分之二的验证者是诚实的，我们就希望能继续合理化并最终化检查点，且不会导致任何诚实验证者被罚没。反过来，我们永远不希望陷入“除非罚没诚实验证者否则就无法最终化新检查点”的僵局。这也符合“终会发生好事”的活性定义。用 Vitalik 的话来说：  可信赖活性基本上意味着“算法不应该被‘卡住’，无法最终化任何东西”。用 Casper 论文中的表述就是：  只要存在扩展最终化链的子链，就始终能通过绝对多数链接来生成新的最终化检查点。证明过程如下：设存在一个最高合理化检查点 $a$，且存在一个位于相同或更高高度的检查点 $b$（不一定是 $a$ 的后代），是所有验证者目标投票中的最高检查点。令 $c$ 为从 $a$ 延伸到 Epoch $h(b)+1$ 的链上的一个检查点。所有验证者都可以对链接 $a → c$ 进行投票，而不必担心被罚没，因为：1) 这不会是双重投票。因为没有验证者以 $h(c)$ 为目标进行过投票。2) 这不会是包围投票。因为诚实验证者不能使用高于 $h(a)$ 的源进行投票，所以 $a → c$ 不会包围别的链接；而因为没有现有链接的目标高于 $h(b)$，所以 $a → c$ 也不会是被包围链接。对链接 $a → c$ 进行投票是安全的，既不会是双重投票，也不会是包围投票因此，可以合理化检查点 $c$。随后，对于所有验证者来说为 $c → d$ 投票就是安全的，其中 $d$ 是 $c$ 的直接孩子。由此即可最终化 $c$ 而不会违反守则。Casper FFG 的分叉选择规则正是基于这种对可信赖活性的需求：底层共识机制必须遵循包含最高合理化检查点的链。根据此证明，只要底层链继续在最高合理化检查点之上构建，我们就可以保证在该链上继续最终化检查点，且不会有人被罚没。其他动态验证者集在以上所有讨论中，我们都假设没有验证者加入或退出协议，从而将 Casper FFG 放在静态验证者集的环境下讨论。但现实中应当允许质押者的假如和退出。Casper FFG 论文讨论了当验证者集逐个 Epoch 变化时如何保持可问责安全性。文章通过前后验证者集进行了分析。以太坊 2.0 的实现忽略了这一机制，而是对验证者的激活和退出进行了严格限制。每个 Epoch，允许激活和停用的验证者数量大约占全部验证者集的 0.0015%（参见 CHURN_LIMIT_QUOTIENT）。Gasper 论文中对这一简化进行了分析。通过限制进入和退出率，不考虑前后验证者集，会稍微降低可问责安全性的水平。也就是说，在最终化冲突检查点时，被罚没的权益可能略少于三分之一。具体而言，如果两个冲突的最终化检查点所属 Epoch 的验证者集在权益上相差 $ε$，那么经济最终性（将被罚没的最低权益量）变为 $\\frac{1}{3}- ε$，而非 $\\frac{1}{3}$。在实践中，验证者集变化率限制非常严格，差异可以忽略不计。k-最终性原始的 Casper 论文要求，要最终化一个检查点，从该检查点到其直接子孙节点间必须有绝对多数链接。事实证明，我们可以推广这一点，而不影响安全证明的有效性。安全证明依赖的关键观察：集合 $B$ 的两个连续成员之间存在绝对多数链接，跨越最终化 $a_m$ 的绝对多数链接 $a_m → a_{m+1}$。而如果存在绝对多数链接 $a_m → a_{m+k}$，且 $a_m$ 和 $a_{m+k}$ 之间的所有检查点都在 $a$ 分支上被合理化，那么 $b$ 分支上的包围链接仍必定存在。因为这种情况下，$B$ 在 Epoch $m$ 到 $m+k$ 中都不能有成员。如果一个绝对多数链接仅跳过合理化检查点，那么可问责安全证明的所有保证都可以保留。这里 $a_5$ 和 $a_6$ 是合理化的，因此可以使用绝对多数链接 $a_4 → a_7$ 安全地最终化 $a_4$因此，推广后的最终性规则是：当有绝对多数链接 $a_m → a_{m+k}$，且检查点 $a_{m+1}, a_{m+2},\\dots, a_{m+k-1}$ 都已合理化时，即可最终化检查点 $a_m$。这就是 k-最终性 (k-Finality)，Gasper 论文的 4.5 节对其进行了讨论。在计算最终性时要考虑多少个检查点 $k$ 取决于想保留多少记录。以太坊 2.0 信标链采用了 2-最终性：记录四个连续 Epoch 的合理化状态，并允许处理两个 Epoch 的目标投票。2-最终性的四种情况。在每种情况下，绝对多数链接都会使其源检查点被最终化，并使其目标检查点被合理化。情况 2 和 4 是经典的 1-最终性。绝大多数时候都只会见到 1-最终性情况，尤其是情况 4。2-最终性仅在大量见证延迟或接近三分之二参与度阈值的时才会发生。此操作的详细机制在 Epoch 处理期间由 weigh_justification_and_finalization() 函数执行：def weigh_justification_and_finalization(state: BeaconState,                                         total_active_balance: Gwei,                                         previous_epoch_target_balance: Gwei,                                         current_epoch_target_balance: Gwei) -&gt; None:    previous_epoch = get_previous_epoch(state)    current_epoch = get_current_epoch(state)    old_previous_justified_checkpoint = state.previous_justified_checkpoint    old_current_justified_checkpoint = state.current_justified_checkpoint    # Process justifications    state.previous_justified_checkpoint = state.current_justified_checkpoint    state.justification_bits[1:] = state.justification_bits[:JUSTIFICATION_BITS_LENGTH - 1]    state.justification_bits[0] = 0b0    if previous_epoch_target_balance * 3 &gt;= total_active_balance * 2:        state.current_justified_checkpoint = Checkpoint(epoch=previous_epoch,                                                        root=get_block_root(state, previous_epoch))        state.justification_bits[1] = 0b1    if current_epoch_target_balance * 3 &gt;= total_active_balance * 2:        state.current_justified_checkpoint = Checkpoint(epoch=current_epoch,                                                        root=get_block_root(state, current_epoch))        state.justification_bits[0] = 0b1    # Process finalizations    bits = state.justification_bits    # The 2nd/3rd/4th most recent epochs are justified, the 2nd using the 4th as source    if all(bits[1:4]) and old_previous_justified_checkpoint.epoch + 3 == current_epoch:        state.finalized_checkpoint = old_previous_justified_checkpoint    # The 2nd/3rd most recent epochs are justified, the 2nd using the 3rd as source    if all(bits[1:3]) and old_previous_justified_checkpoint.epoch + 2 == current_epoch:        state.finalized_checkpoint = old_previous_justified_checkpoint    # The 1st/2nd/3rd most recent epochs are justified, the 1st using the 3rd as source    if all(bits[0:3]) and old_current_justified_checkpoint.epoch + 2 == current_epoch:        state.finalized_checkpoint = old_current_justified_checkpoint    # The 1st/2nd most recent epochs are justified, the 1st using the 2nd as source    if all(bits[0:2]) and old_current_justified_checkpoint.epoch + 1 == current_epoch:        state.finalized_checkpoint = old_current_justified_checkpoint总结本文介绍了 Casper FFG 中的术语及核心概念，并对其运行机制、可问责安全性、可信赖活性进行了讲解。后续文章将对结合 LMD GHOST 与 Casper FFG 的 Gasper 协议进行探讨。            Casper FFG 的论文中常用 “dynasty” 一词表示 Epoch，只有少数例外，这两个词其实是一回事。 &#8617;              这两轮与经典 PBFT 共识中的 PREPARE 和 COMMIT 阶段相对应，而 PRE-PREPARE 阶段大致相当于 Casper FFG 中将检查点块进行广播。 &#8617;              Lighthouse 团队和 Prysm 团队都开发了罚没检测软件。↩ &#8617;              截止目前（2023年6月），信标链上质押了 2160 万枚 ETH，因此最终性回滚将导致至少 720 万枚 ETH 被罚没。按当前价格计算，相当于有 137 亿美元的安全预算。↩ &#8617;      "
  },
  
  {
    "title": "以太坊 PoS 共识协议详解（一）：核心概念与 LMD GHOST",
    "url": "/posts/lmdghost/",
    "categories": "Blockchain, Ethereum",
    "tags": "ethereum, pos",
    "date": "2023-04-15 12:00:00 +0800",
    





    
    "snippet": "本文是介绍以太坊 PoS 共识协议的第一部分。主要包括对术语和核心概念的讲解，以及对 LMD GHOST 分叉选择规则的详细介绍。术语及重要概念节点和验证者以太坊网络的主要参与者是节点 (Node)。节点的作用是验证共识并与其他节点形成通信骨干网。共识由验证者 (Validator) 形成。  “验证者”这一术语具有一定的误导性，验证者其实并不会验证任何东西，验证是由节点完成的。每个验证者代...",
    "content": "本文是介绍以太坊 PoS 共识协议的第一部分。主要包括对术语和核心概念的讲解，以及对 LMD GHOST 分叉选择规则的详细介绍。术语及重要概念节点和验证者以太坊网络的主要参与者是节点 (Node)。节点的作用是验证共识并与其他节点形成通信骨干网。共识由验证者 (Validator) 形成。  “验证者”这一术语具有一定的误导性，验证者其实并不会验证任何东西，验证是由节点完成的。每个验证者代表一笔初始的 32 ETH 的质押。验证者有自己的私钥，以及与其对应的公钥，代表其在协议中的身份。验证器附属于节点，单个节点可以托管零到数百甚至数千个验证者，附属于同一节点的验证者共享相同的全局视图。 1权益见证与工作量见证的一个重要区别在于，在权益见证下验证者集是已知的，系统拥有在任意时刻期望处于活跃状态的公钥的完整列表。由此我们可以确定何时获得了参与者的多数票，并达成最终性。Slot 和 Epoch以太坊的权益见证共识中，时间是被精确控制的，这与工作量见证截然不同，后者只是尝试在平均水平上保持出块间隔不变，仅此而已。图1 前 32 个 Slot 属于第 0 个 Epoch，创世块位于第 0 个 SlotSlot（常译作“时隙”、“区块槽”）和 Epoch（常译作“时段”）是划分时间的两个主要概念，每个 Slot 为 12 秒，每个 Epoch 为 32 个 Slot，即 6.4 分钟。无论网络上情况如何，Slot 和 Epoch 都会保持节奏持续向前推进。区块和见证每个 Slot 会选出一名验证者提议一个区块 (Block)。区块包含对信标状态的更新，其中包括提议者知道的见证 (Attestation) 以及带有以太坊用户交易的执行负载。提议者通过 Gossip 协议与整个网络共享其区块。Slot 可以为空：因为区块提议者可能处于离线状态，或者提议了一个无效的区块，或者其区块随后被链重组 (Reorg) 剔除出链。每个 Epoch 中，每个验证者都能以见证的形式分享一次自己的视图。见证包含用于 LMD GHOST 协议的链头投票以及用于 Casper FFG 协议的检查点投票。见证也会广播到整个网络。见证和区块一样，也可能由于各种原因丢失，并且协议可以在一定程度上容忍这种情况。粗略地说，随着见证者参与率的下降，共识的质量将会下降。 2Epoch 的功能是分散处理所有这些见证的负载。通过见证，每个验证者都会将自己的全局视图告知其他每个验证者，如果所有操作都同时进行，可能带来巨量的网络流量和处理负载。将见证负载分散到一个 Epoch 的 32 个 Slot 中可以保持较低的资源使用率。在每个 Slot 中，仅由$ \\frac{1}{32} $的验证者组成的委员会进行见证。协议通过奖励和罚没系统激励验证者进行区块和见证的生产并确保其准确性，这一部分本文暂不深入探讨。安全性和活性在讨论共识机制时，经常会用到两个重要概念：安全性 (Safety) 和活性 (Liveness)。用一句话概括安全性要求就是“永不发生坏事”。在区块链语境下，“坏事”可能是币的双花，或冲突检查点的最终化等。分布式系统中安全性的一个重要方面是“一致性 (Consistency)”。也就是说，如果我们向不同的节点查询链的状态（如特定区块高度的账户余额），则无论询问哪个节点，都应该始终得到相同的答案。在一个安全的系统中，每个节点对链的历史都具有完全相同的视图，并且该视图永远不会改变。安全性意味着我们的分布式系统“像中心化实现一样原子地执行每一步操作”。根据 Vitalik 的中心化分类法，安全的系统在逻辑上是中心化的。而同样用一句话概括活性就是“终会发生好事”。在区块链语境中，通常可以理解为链可以始终添加新块，永远不会陷入死锁状态。也可以从“可用性 (Availability)”的角度出发理解活性：链是可用的，意味着如果我向诚实节点发送有效交易，最终交易一定会被包含在一个扩展链的区块中。值得注意的是，CAP 定理证明了没有分布式系统可以同时满足：一致性，可用性和分区容错性。现实中，我们只能基于不可靠的互联网构建区块链，即分区容错性是必须成立的，这也就说明链的安全性和活性是难以两全的。以太坊共识协议在网络状况良好时能同时提供安全性和活性，但在运行不顺利的情况下优先考虑活性。在网络分区的情况下，分区两侧的节点都将继续产生区块，但最终性（安全属性）不再有保障。具体取决于各分区中的质押权益比例，可能只有一侧继续最终化，也可能双方都无法最终化。最终，除非分区问题得到解决，否则消极惩罚机制 (Inactivity Leak) 会使双方都将重新获得最终性，但同时也将导致共识的撕裂，两条链将最终化不同的历史记录，并且这两条链将永远保持独立，无法调和。罚没在工作量证明中，产生区块代价很高。这会激励矿工按照协议的目标正当行事，以确保其区块被包含。而在权益见证中，创建区块和见证的成本近乎为零3，我们需要一种方法来防止攻击者利用这一点扰乱网络。这就是罚没 (Slashing) 的作用。对区块或见证有分歧行为的验证者会受到罚没，即被驱逐出验证者集并被没收部分质押金。这里的分歧行为的意思简单来说就是自相矛盾，例如为同一个 Slot 提议两个不同的区块，或做出两个相互冲突的见证等遵循协议的诚实验证者不会做出的行为。Ghost in the Shell了解术语和核心概念后，本节将概述以太坊的实际共识机制。以太坊的权益证明共识协议实际上是两个独立的共识协议的组合，分别称为 LMD GHOST 和 Casper FFG，二者组合的协议有时被称为“Gasper”。Gasper 中将两者结合，试图在活性和安全性方面获得两全其美的效果。从本质上讲，LMD GHOST 提供了逐区块的活性（保持链的运行），而 Casper FFG 提供了安全性（保护链免于长回滚）。LMD GHOST 允许我们不断地生成区块，但它存在分叉，因此在形式上并不安全。Casper FFG 修改了 LMD GHOST 的分叉选择规则，以期定期为链提供最终性。然而，如前所述，以太坊优先考虑活性。因此，在 Casper FFG 无法提供最终性的情况下，链仍然会通过 LMD GHOST 机制继续增长。这种“拼凑”在一起的共识机制并不完美，但秉承以太坊的精神来看，这是一种可行的工程解决方案，在实践中能很好地达成目的。历史Gasper 的详细历史与其组件（LMD GHOST 和 Casper FFG）的发展紧密相关，我们将在后续文章中进行回顾。在此要指出的是，Casper FFG 从来就不是设计成独立的共识机制。正如 Casper FFG 论文中所述：  “Casper the Friendly Finality Gadget” 是提案机制（提出区块的机制）之上的一个覆盖层。因此，Casper FFG 之下还存在着一个底层的区块提议机制（说明存在底层共识机制）提供了某种“元共识”，赋予区块链最终性。最初的计划是将 Casper FFG 作为权益证明覆盖层应用于以太坊的工作量证明共识之上。Casper FFG 将周期性地为链提供最终性（PoW 链缺少的属性）。旨在成为以太坊逐步淘汰工作量证明铺路，有了最终性保证，就可以减少工作量证明区块奖励，从而降低总哈希算力，为用权益证明取代挖矿做准备。到 2017 年底，EIP-1011（混合 Casper FFG）详细描述了架构，甚至还有一个测试网在 2017 年 12 月 31 日上线。然而以太坊虚拟机的有限带宽限制了 EIP-1011 可以支持的验证者集大小，进而导致最低质押额高达 1500 ETH，被认为是不可取的，因此该计划在 2018 年初被取代了。大约在同一时间，开发者们开始着手设计以太坊 2.0 协议。Casper FFG 的通用性使其在重新设计中得以被保留，作为新权益证明协议 LMD GHOST 的覆盖层，被采纳为以太坊 2.0 的一部分。最终性小工具前文提到 Casper FFG “覆盖”现有区块提议机制时，其含义是 Casper FFG 使用现有的区块树并以特定方式进行剪枝，通过使某些分支不可访问来修改底层区块树的分叉选择。考虑下面这个由某种底层共识机制（无论是工作量证明还是权益证明中的 LMD GHOST）产生的区块树：由三个分叉组成的任意区块树 区块 I、E 或 M 都可以是链头（块标签仅为方便起见，并不表示特定顺序）在这种情况下，有三个候选头块，I、E 和 M。在工作量证明最长链规则下，必须选择块 M 作为链头，因为它具有最大的区块高度，或者说该分支积累了最大的工作量。在 LMD GHOST 下，仅凭此信息无法选择头块，我们需要查看其他验证者的投票才能做出选择。问题在于从区块 J 到 M 的链可能来自攻击者。攻击者可能秘密挖矿了该链随后在 51% 攻击中揭示。PoW 节点别无选择，只能重组链以使 M 成为头块，这有利于攻击者的链并可能受到双花攻击。Casper FFG 带来的价值在于赋予最终性。假设 Casper FFG 将区块 D 标记为最终化（这也会自动将区块 A、B 和 C 标记为最终化）。最终化会修改底层协议的分叉选择规则，因此任何包含与区块 D 竞争的区块（即不是 D 的子孙的区块）的分支都会被排除在外。相当于修剪分支来确保最终化的区块之前没有分叉。与上面相同的区块树，但区块 D 已被最终化 Casper FFG 分叉选择规则指出，任何不包含区块 D 的链都会被忽略，因此头块明确是 E本质上，Casper FFG 提供的最终性可以防止长重新（回滚）。任何最终化区块及最终化区块的祖先区块都将永远不会被回滚。在以太坊的 Casper FFG 实现中，“永远”被限定在“不销毁质押以太坊总量的三分之一”的条件下。这就是 PoS 链所提供的经济最终性。LMD-GHOST在本节中，我们将单独探讨 LMD GHOST，忽略 Casper FFG 最终性叠加层。 与 Nakamoto 共识中的最长链规则一样，LMD GHOST 也是一种分叉选择规则，是共识机制的核心，有着自己的一套属性和权衡取舍。本文会先介绍其原理，后续文章将深入探讨可能存在的问题。命名LMD GHOST 这个名称由两个缩写词组成，”Latest Message Driven”（最新消息驱动）和 “Greedy Heaviest-Observed Sub-Tree”（贪婪最重观测子树）。GHOSTGHOST 协议来自 Sompolinsky 和 Zohar 在 2013 年的一篇论文，该论文讨论了如何在比特币上安全地提高交易吞吐量。增加区块大小或缩短区块间隔会使区块链在延迟不受控的网络中（例如互联网）更容易分叉。分叉的链会发生更多重组，而重组会损害交易安全性。用 GHOST 分叉选择规则替换比特币最长链分叉选择规则被证明在存在延迟的条件下更加稳定，能使出块更加频繁。GHOST 代表了 “Greedy Heaviest-Observed Sub-Tree”（贪婪最重观测子树），这也是对算法工作原理的描述，后文将详细解释。简而言之，GHOST 的分叉选择并不遵循最长链，而是遵循最重的子树。算法认识到对区块的投票不单是投给该区块的，而是隐含了对其所有祖先区块的投票，因此整个子树都应具有相关的权重。LMD以太坊权益证明中使用的 GHOST 协议已经扩展到能够处理见证。在工作量证明中，投票者是区块提议者。他们通过在其之上构建自己的区块来对分支进行投票。而在权益证明协议中，所有验证者都是投票者，每个验证者平均每 6.4 分钟通过发布见证来对其网络视图进行一次投票。因此，在 PoS 下，我们会拥有更多关于参与者网络视图的信息。这就是“消息驱动”的含义，也就是 LMD 中的 MD。分叉选择不是由提议者添加的区块驱动的，而是由所有验证者发布的消息（见证、投票）驱动的。“L” 代表 “latest”（最新）：LMD GHOST 只考虑来自每个验证者的最新消息，即从该验证者收到的最近的见证。验证者所有早期的消息都会被丢弃，但其最新投票会被保留并无限期地具有权重。原理LMD GHOST 首先是一个分叉选择规则。给定一个区块树和一系列投票，LMD GHOST 会告诉节点应该将哪个区块视为链头，节点可以从该块一直回溯到创世块，获得线性的历史视图。这一选择基于节点接收到的消息（区块和见证）所形成的对链的本地视图。没有“上帝视图”，节点只能依赖其本地视图，该视图可能与其他节点的本地视图大不相同。重点在于是诚实的验证者会基于他们看到的链头来构建区块，并且会根据看到的链头投票。对 LMD GHOST 原理的介绍将分两部分进行，首先说明 LMD 部分（最新消息），随后是 GHOST 部分（确定链头）。最新消息在此语境下，消息是指见证中的链头投票。LMD GHOST 中的投票在见证的 data 中，链头投票是 beacon_block_root 字段：class AttestationData(Container):    slot: Slot    index: CommitteeIndex    # LMD GHOST 投票    beacon_block_root: Root    # FFG 投票    source: Checkpoint    target: Checkpoint每个诚实的验证者在每个 Epoch 恰好进行一次见证，其中包含对当时其视图中的最佳链头的投票。在每个 Epoch 内，验证者集会被分割，这样每个 Slot 只有$ \\frac{1}{32} $的验证者进行见证。节点通过两种方式接收见证：直接通过证明 Gossip 协议接收，以及间接地从区块内接收。存储最新消息无论通过何种方式接收见证，节点都会调用分叉选择规则的 on_attestation() handler。在继续之前，on_attestation() handler 会对见证执行一些基本的有效性检查：      是否太旧？          见证必须来自当前或前一个 Epoch。            是否太新？          见证必须不迟于上一个 Slot。            是否知道该见证投票的区块 beacon_block_root？          必须收到过该区块。如果没有，尝试从对等点处获取。            见证的签名是否正确？          验证者对见证进行签名并对其负责。            见证是否可罚没？          必须忽略与其他见证冲突的见证。      通过检查后，见证将被放入节点的 Store 中（Store 是节点对链视图信息的存储库），这一步由 update_latest_messages() 完成。def update_latest_messages(store: Store, attesting_indices: Sequence[ValidatorIndex], attestation: Attestation) -&gt; None:    target = attestation.data.target    beacon_block_root = attestation.data.beacon_block_root    non_equivocating_attesting_indices = [i for i in attesting_indices if i not in store.equivocating_indices]    for i in non_equivocating_attesting_indices:        if i not in store.latest_messages or target.epoch &gt; store.latest_messages[i].epoch:            store.latest_messages[i] = LatestMessage(epoch=target.epoch, root=beacon_block_root)如果本地没有存储该验证者的链头投票，则存储该见证和验证者信息。如果本地已存有该验证者的链头投票，若该见证更新，则进行替换。随着时间的推移，节点的 Store 会建立起一个列表，其中包含所有接收过的每个验证者的最新投票。注意，节点只能存储在当前或前一个 Epoch 创建的投票，一旦投票进入 Store，就会无限期地被保留，并继续影响分叉选择，直到被替换为更新的投票为止。确定链头本质上，LMD GHOST 分叉选择规则即函数 GetHead(Store) → HeadBlock。如前所述，节点的 Store 就是其全局视图：节点接收到的所有可能影响分叉选择的相关信息。对于 LMD GHOST 算法而言， Store 中的相关部分如下：  区块树，实质是一个区块列表。区块的父链接将其在逻辑上连成了一个树。  来自验证者的最新消息（投票）列表。  验证者的有效余额，为算法提供了所需的权重。GHOST 算法的目标是从给定的区块树中选择单个叶块（即没有子孙块的区块）作为链头区块。我们假设区块树中的所有区块都来自单个根区块。在纯 GHOST 算法中，根区块就是创世区块：根据定义，所有区块都来自创世区块。而在完整的共识实现中，根区块是最后一个已确认 (Justified) 的检查点区块。重点在于 GHOST 算法会从给定的区块开始，并会忽略所有不从该区块派生的区块。获取权重第一步要计算树中每个分支的“权重”。投票的权重是进行投票的验证者的有效余额，通常会是 32 ETH（最大有效余额），但也有可能更少。$B_N$表示最新链头投票目标为区块$N$的验证者的有效余额之和，$W_N$表示以区块$N$为根的分支的权重分支权重$W_x$ 和区块投票权重$B_x$之间有一些显而易见的联系：  对于仅包含叶块的分支 $L$，$W_L = B_L$。  分支的权重等于其根块的投票权重与所有子分支权重之和。因此，在上图中，$W_1 = B_1 + W_2 + W_3$。  分支的权重是形成该分支的子树中所有区块投票权重之和。由于投票的权重总是正的，因此根区块的权重最大，且等于树中所有区块投票权重之和。每个验证者最多只有一个最新消息（即一个投票），因此该权重上限为所有活跃验证者的总有效余额。获取链头计算出每个分支或子树的权重后，算法开始递归进行。给定一个区块，选择从中派生的最重分支。然后用该分支的根块重复该过程。如果两个或更多分支权重相等，选择根块哈希值最高的分支。最终算法输出一个叶块。从 GHOST 这个名称可见，该算法：  是贪婪的，即立刻获取观测到的最重分支，不会进一步查看；  处理子树，分支的权重是子树中所有区块投票权重之和。举例如下，图中包括：特定区块的投票权重（附加到每个区块的数字）和分支权重（块连线上的数字）。  根据存储器中的最新消息，我们计算树中每个区块的投票权重。get_head()从区块树的根块 A 开始  根据每个区块的投票权重，计算每个分支或子树的权重。get_weight() 函数应用于块时返回该块的子树及其所有后代的总权重  递归地遍历子树根块，始终选择权重最高的分支或子树。最终到达一个叶块，即算法选择的链头区块。在块 A 时，分支 AC 最重 在块 C 时，分支 CE 最重 块 E 是叶块，因此是 GHOST 算法选出的链头区块 链为[A←C←E]假如以 B 和 C 为根的子树权重相等，则选择 B 和 C 中哈希值更大的分支，这是完全随机的平局打破机制。规范中实现以上功能的是 get_head()，它从根部向上遍历树，在每个分叉处取最重的分支。def get_head(store: Store) -&gt; Root:    # Get filtered block tree that only includes viable branches    blocks = get_filtered_block_tree(store)    # Execute the LMD-GHOST fork choice    head = store.justified_checkpoint.root    while True:        children = [            root for root in blocks.keys()            if blocks[root].parent_root == head        ]        if len(children) == 0:            return head        # Sort by latest attesting balance with ties broken lexicographically        # Ties broken by favoring block with lexicographically higher root        head = max(children, key=lambda root: (get_weight(store, root), root))计算子树权重的是 get_weight()。实际的 get_weight()实现比上面介绍的更复杂一些，主要是因为 “proposer boost” 机制，这里暂不展开，后续文章将深入探讨。def get_weight(store: Store, root: Root) -&gt; Gwei:    state = store.checkpoint_states[store.justified_checkpoint]    unslashed_and_active_indices = [        i for i in get_active_validator_indices(state, get_current_epoch(state))        if not state.validators[i].slashed    ]    attestation_score = Gwei(sum(        state.validators[i].effective_balance for i in unslashed_and_active_indices        if (i in store.latest_messages            and i not in store.equivocating_indices            and get_ancestor(store, store.latest_messages[i].root, store.blocks[root].slot) == root)    ))    if store.proposer_boost_root == Root():        # Return only attestation score if ``proposer_boost_root`` is not set        return attestation_score    # Calculate proposer score if ``proposer_boost_root`` is set    proposer_score = Gwei(0)    # Boost is applied if ``root`` is an ancestor of ``proposer_boost_root``    if get_ancestor(store, store.proposer_boost_root, store.blocks[root].slot) == root:        committee_weight = get_total_active_balance(state) // SLOTS_PER_EPOCH        proposer_score = (committee_weight * PROPOSER_SCORE_BOOST) // 100    return attestation_score + proposer_score小结了解了 GHOST 协议的工作原理后，可以更直观地看出其相较于最长链规则的优越性。分叉的出现表明块传播时间已经接近甚至超过了出块时间（Slot），因此并非所有验证者都能及时看到所有区块并进行见证。在这种情况下，需要利用可用的大量信息：对同一个父区块两个不同子区块的投票应该被理解为所有这些验证者都支持父区块的分支，仅对子区块存有分歧。GHOST 通过让子区块的投票为其所有祖先区块增加权重实现了这一点，选择时总会优先考虑验证者总支持最多的分支。而最长链规则会丢弃这些信息，一个分支即使只有少数验证者在处理也有可能胜出。激励机制加密经济系统保障自身安全的一种方式是“惩恶扬善”，即奖励正当行为并惩罚不良行为。在对 LMD GHOST 的实现中，提议者和验证者都会因正确找到链头而以不同的方式获得奖励。区块提议者隐式地获得激励。如果不在最佳链头区块上构建，那么其区块很可能不会被包含在最终的规范链中，成为孤块，那么提议者将不会收到区块奖励。工作量证明中的矿工情况与之类似。而当验证者进行了准确的链头投票，并且其见证在下个 Slot 的区块中被包含时，将直接获得奖励。理论上，完美运行的验证者能通过准确的链头投票获得其总协议激励的大约 22%。同时，提议者则被激励在区块中包含这样的证明。这里投票“准确”的含义是投票与最终规范链中的内容一致。如果验证者的链头投票不准确，并不会受到惩罚。因为当链上压力较大，存在延迟和丢失的区块时，准确进行投票很困难，在这种情况下惩罚验证者是不公平的。罚没机制权益证明设计的一大突破之一是采用罚没机制来解决 “nothing at stake” 问题。提议者罚没当轮到某验证者在一个特定 Slot 中生成区块时，验证者应该运行分叉选择规则来决定基于哪个现有区块来构建自己的区块。其目标是根据自身拥有的信息，识别最有可能最终成为规范链的分叉，即正确验证者集将收敛到的那个分叉。但与工作量证明不同，在权益证明下，验证者生成区块几乎没有成本。因此，最好的策略是为每个可能的链头都生成区块，其中有一个区块一定会成为最终规范链的一部分。但这样的行为会延长分叉并阻止网络收敛到线性历史。用户可能无法确定哪个分叉是正确的，容易受到双花攻击。这就是 “nothing at stake” 问题。解决方案如前所述，是检测两个相互矛盾的区块并惩罚提议者。提议者分歧行为不是在协议内部检测到的，而是依赖于第三方用 ProposerSlashing 对象的形式构建证明。class ProposerSlashing(Container):    signed_header_1: SignedBeaconBlockHeader    signed_header_2: SignedBeaconBlockHeader该证明仅包含两个签名区块头，足以证明提议者在同一 Slot 中签署了两个区块。后续的区块提议者会将该证明包含在区块中（并获得丰厚的奖励），协议会罚没违规验证者的质押金并将其从活动验证者集中踢出。见证者罚没当轮到某验证者发布见证时，验证者应该运行其分叉选择规则并投票给其视图中正确的链头区块。问题在于攻击者可以做出多个相互矛盾的见证来引发或延长分叉并阻止网络收敛到单个链。如果不罚没，那么这就会成为对投票错误错过奖励的对冲方式。措施与上述相同：检测并罚没相互矛盾的见证（同一个验证者在同一个 Slot 中针对不同链头做出的见证）。总结本文首先介绍了以太坊 PoS 共识协议中重要的术语和概念，并深入讲解了分叉选择规则 LMD GHOST 的机制和原理。后续文章将继续介绍 Casper FFG，以及两者结合的 Gasper，并将探讨以太坊共识协议实现中存在问题及解决方法。            注意这一点有助于我们评估以太坊的去中心化程度。比如网络上有 500000 个活跃的验证者远不能代表网络有 500000 个独立的参与者。节点数量以及验证者在节点间分布情况，是更能衡量以太坊去中心化程度的指标。 &#8617;              Beaconcha.in 显示了每个 Epoch 的见证参与率（也叫投票参与率），这是衡量网络运行状况的一个很好的指标。参与率通常都超过了 99%，对大规模分布式共识协议来说是十分出色的水平。 &#8617;              即常说的 “nothing at stake”（利益无关）问题。 &#8617;      "
  },
  
  {
    "title": "文件同步系统 Floo 基本功能演示",
    "url": "/posts/floo/",
    "categories": "Project",
    "tags": "p2p",
    "date": "2023-03-11 12:00:00 +0800",
    





    
    "snippet": "Floo 是基于 IPFS 构建的一个点对点文件同步系统，旨在提供安全、分布式、版本化的文件同步解决方案。相较于现有工具，Floo 具有可扩展性高、资源开销小、数据隐私性强、灵活易用等优点，能够适应不同场景下的需求，如企业文件共享、个人多终端文件同步等。Floo 系统核心流程以下视频演示了 Floo 的基本功能，主要包括：  初始化仓库          floo init        添...",
    "content": "Floo 是基于 IPFS 构建的一个点对点文件同步系统，旨在提供安全、分布式、版本化的文件同步解决方案。相较于现有工具，Floo 具有可扩展性高、资源开销小、数据隐私性强、灵活易用等优点，能够适应不同场景下的需求，如企业文件共享、个人多终端文件同步等。Floo 系统核心流程以下视频演示了 Floo 的基本功能，主要包括：  初始化仓库          floo init        添加文件          floo add         常用文件操作          floo rm,  floo cat, floo edit, floo cp, floo mv, floo mkdir, floo tree, floo ls, floo info        挂载          floo mount        版本控制          floo commit, floo reset, floo diff, floo log        远程同步          floo remote, floo whoami, floo sync            "
  },
  
  {
    "title": "zkEVM Rollup：从理论到现实",
    "url": "/posts/zkevmrollup/",
    "categories": "Blockchain, Layer 2",
    "tags": "rollup, zkEVM",
    "date": "2023-02-20 12:00:00 +0800",
    





    
    "snippet": "为了解决区块链Layer 1网络的扩容问题，Rollup方案应运而生。结合零知识（ZK）技术，ZK Rollup成为Layer 2赛道中备受瞩目的解决方案。然而，对于大多数人而言，ZK、Rollup和EVM等相关概念可能有一定的理解难度。因此，本份研报的目标是用简洁易懂的语言，对zkEVM Rollup的一系列概念进行系统梳理，深入分析zkEVM Rollup技术的发展和现状，并详细解读其中...",
    "content": "为了解决区块链Layer 1网络的扩容问题，Rollup方案应运而生。结合零知识（ZK）技术，ZK Rollup成为Layer 2赛道中备受瞩目的解决方案。然而，对于大多数人而言，ZK、Rollup和EVM等相关概念可能有一定的理解难度。因此，本份研报的目标是用简洁易懂的语言，对zkEVM Rollup的一系列概念进行系统梳理，深入分析zkEVM Rollup技术的发展和现状，并详细解读其中的主要生态项目。通过这样的方式，旨在帮助您全面深入地了解和评估zkEVM Rollup赛道的发展趋势。1 理解ZK零知识证明（Zero-Knowledge Proof，简称ZK或ZKP）是一种密码学方法，它允许一方（证明者）向另一方（验证者）证明某个事实的正确性，而无需泄露任何与该事实相关的具体细节。因此，ZK技术在隐私保护领域提供了广阔的应用前景。除了隐私保护方面的优势，ZK技术在ZK Rollup中还起到解决“验证难”问题的关键作用。在区块链中，“验证”过程至关重要，在以太坊这样的平台中，大部分计算过程都是为了满足验证需求。ZK Rollup通过利用ZK技术，极大地减少了节点网络在验证过程中所需的时间。举例来说，如果验证一个区块是否符合整个网络规则的过程耗时很长，那么可以由一个验证者首先验证并生成与该区块计算结果相关的“证明”，其他节点只需快速验证这个“证明”，而不需要进行复杂的原始区块计算即可达成区块验证的效果。1.1 生成密钥、证明及验证在ZK中，我们需要首先将待验证的问题转化为数学表达式，进而转化为多项式，并将其表示为算数电路的形式。当程序转换为算术电路时，它被分解为由加法、减法等基本算术运算组成的单个步骤。图 1 是一个电路图的示例，可以注意到在电路中所有的运算被拆分为最简单的基本运算。图1 电路示意图 | 图源：https://cs251.stanford.edu/lectures/lecture14.pdf使用电路和一些随机数作为输入，可以生成证明密钥（pk，proving key）和验证密钥（vk，verification key），用于后续的验证过程，如图 2 所示。图2 “公共参数”的生成我们的证明系统还需要两种类型的输入——私有输入和公开输入，与证明密钥一起生成证明。其中，私有输入（witness）是我们想要隐藏的秘密，而公开输入是可以公开的信息。当验证方接收到证明后，使用公开输入、证明和验证密钥来验证该证明，并返回验证结果（即是否验证成功）。图 3 是ZK-SNARK的证明和验证过程。实现零知识证明的协议和方式有很多，SNARK是比较容易理解的一种，也是现阶段多数项目的选择。后文也将阐述SNARK的优势和不足。图3 ZK-SNARK的证明过程和验证过程以一个记录账户状态的 Merkle Tree 的零知识证明为例。该 Merkle Tree 用于记录账户的地址和余额。当有新的交易需要更新 Merkle Tree 时，需要执行以下操作：      验证交易的发送方和接收方是否在树的叶子节点上。        验证发送方的签名。        更新发送方和接收方的余额。        更新 Merkle Tree 的根节点（即 Merkle Root），并将其作为最终输出。  在没有零知识证明的情况下，验证者需要重复这些步骤以确保计算的准确性。而使用零知识证明，则大有不同。首先，需要确定私有输入和公开输入：      在上述过程中，只有新的交易信息、原 Merkle Root 和更新后的 Merkle Root 是公开输入。        Merkle Tree 本身作为 witness（见证），不需要被验证者读取或处理。  其次，需要生成密钥和计算电路。我们将 Merkle Tree 的更新、输入输出地址的验证等计算过程构建成计算电路，以获得证明密钥和验证密钥。注意：该电路与输入的交易信息无关，也与现有的 Merkle Root 无关，因为 Merkle Tree 的计算方式是固定的。在生成证明的阶段，我们将前后两个 Merkle Tree 和交易信息作为输入。在验证阶段，验证者可以在不获取 Merkle Tree 的情况下，仅凭输入来确保更新过程的可靠性。该电路就像一个稳固的黑盒，只要输入正确，就能得到正确的输出。使用零知识证明，其他验证者可以快速验证 Merkle Tree 的生成过程是可信的，从而减少了网络上重复验证的时间，同时Merkle Tree的信息无需向验证者披露。1.2 从ZK-SNARK到ZK-STARK上述提到的证明协议是ZK-SNARK。“SNARK”代表“Succinct Non-interactive ARguments of Knowledge”，其中“Succinct”表示这种方式的简洁性，“Non-interactive”表示相对于其他证明方式，SNARK是非交互性质的证明，即验证者只需使用由证明者生成的证明即可获得验证结果。然而，ZK-SNARK存在一些问题。在密钥生成阶段，电路和随机数相当于一组初始的公共参数，这个公共参数的生成过程存在不可避免的中心化问题。ZK-STARK在SNARK的基础上另辟蹊径，“STARK”代表“Scalable Transparent ARguments of Knowledge”，“Scalable”说明其具有可扩展性。在ZK-STARK中，证明生成时间与原始计算的耗时呈拟线性关系，而验证耗时与原始计算呈对数关系。这意味着在处理大量数据集的情况下，验证者所需的验证时间将大大缩短。作为ZK-SNARK的升级版，ZK-STARK不仅提高了验证效率（理论效率为10倍），且不依赖椭圆曲线或可信设置，且无需生成初始公共参数的过程（即“Transparent”代表的透明性），从而消除了对可信设置的中心化需求。一些开发者认为，ZK-STARK中的哈希函数有助于抵御量子攻击。然而，ZK-STARK的推出较晚，目前技术仍不够成熟，并且依赖哈希函数，这限制了其通用性。相比之下，ZK-SNARK仍是通用的证明算法。基于STARK的算法示例包括Fractal、SuperSonic等，而与之相关的项目方包括StarkWare、Polygon Miden等。2 理解Rollup除了零知识证明，我们还需要了解另一个概念，即Rollup。Rollup的意义在于解决底层网络的拥堵问题。例如以太坊目前仍存在交易拥堵的问题。为了解决此问题，有两种方法可供选择：一种是增加区块链本身的交易处理能力，例如通过分片等技术扩展区块链的吞吐量。另一种方法是改变区块链的使用方式，即将大部分复杂计算由链上直接执行转移到在二层（Layer 2，简称L2）上执行，仅在底层链进行验证结算。在这种情况下，底层链上通常会部署一个智能合约负责处理存款和取款，并使用各种方法来读取链下数据，以保证链下的计算和执行符合规则。此方式可以形象地理解为在原有的小路上搭建高架桥，通过“二层”的扩展来解决拥堵问题。当前，L2扩容主要有三种类型或方案，即状态通道（State Channel）、Plasma和Rollup。它们分别代表了三种不同的范式，各有其优点和缺点。所有L2扩容方案大致都可以归为这三个类别（尽管有些命名存在争议，例如Validium），其中Rollup具有一些独特的优势。2.1 Rollup和数据可用性相比于其他扩容方案，Rollup具有一定的优越性，其中一个比较直观的优势是解决了数据可用性的问题。第二层扩展解决方案，例如Rollup，通过在链下处理交易来降低交易成本并提高以太坊的吞吐量。Rollup将大量的链下交易压缩成批次，并一次性发布到以太坊网络上。这样一来，基础层的拥堵问题得到缓解，用户也能够享受更低的手续费。然而，要想信任发布到以太坊的“汇总”交易，我们需要能够独立验证并确认这些交易所引起的状态变更确实是执行所有链下交易而得出的结果。如果Rollup运营者不提供交易数据供验证，则可能向以太坊发送错误的数据。这就是数据可用性问题。当前，Rollup方案将交易数据作为calldata永久存储在底层链上，而随着 Danksharding 的引入，区块中存放L2数据的空间将更大，并会使用临时的、成本更低的“blob”进行存储。但无论使用何种数据可用性方案，数据在链上这一点非常重要。需要注意的是，将数据存储在IPFS上是行不通的，因为IPFS并不提供共识层面的验证，无法保证给定数据是否可用。在Plasma以及Channel这两种扩容方案中，数据和计算完全存放在L2网络中，当L2和以太坊进行交互时，L2链上的交易数据并不包含在内，从状态机视角来看，也就是没有包含L2链每一次状态变更的信息。这会导致以太坊如果脱离了Plasma等L2网络就无法复原之前状态变更的情况。如果恶意运营方在Plasma链上进行了无效转换，用户将不能对其发起挑战，底层链也无法独立验证并解决分歧，因为运营方可以扣留所需的数据。 Rollup通过迫使运营方在以太坊上发布交易数据来解决这个问题，使任何人都可以验证链的状态。2.2 Rollup的机制为了保证数据可用性，因此市场选择了Rollup，那么Rollup具体是如何工作的？图4 L1合约中的State Root | 图源：https://vitalik.ca/general/2021/01/05/rollup.html在Rollup的设计中，主链上有一个Rollup合约，其中保存了一个状态根（state root）。可以把这个状态根看作是Merkle Tree的Merkle根的升级版，它存储了账户余额、合约代码等信息，图 4展现了Rollup合约中存储的状态根。L2节点主要有三个功能：首先确定哪些交易应该优先被打包（通常该类节点或客户端被称为定序器Sequencer），其次需要对每个打包的数据给出证明，最后提交给L1上的Rollup contract由该合约进行验证。图 5展现了L2中定序器的作用。图5 定序、证明和提交Batch是L2节点的主要功能具体而言，L2可以将一批数据（batch）传递给L1。这些数据可以是高度压缩的交易集合或合约运行后的状态变化。同时，这批数据还包括L1合约中存储的状态根（state root）以及经过L2处理后得到的新的后状态根（post-state root）。合约利用这些数据来验证新的后状态根的正确性。一旦验证通过，这批数据就会在L1层得到确认，完成了从L2到L1的上链过程。为了确保提交的数据中的新后状态根是正确的，最直接的方法是让L1重新计算一次。然而，这种方法会给L1带来巨大的压力。为了解决这个关键问题，存在两种完全不同的优化方案，即Optimistic Rollup和ZK Rollup。2.3 ZK Rollup和Optimistic Rollup图6 Optimistic Rollup和ZK Rollup的对比图6展示了ZK Rollup和Optimistic Rollup运行机制的对比。ZK Rollup采用有效性证明机制，通过诸如ZK-SNARK或ZK-STARK等加密协议来验证执行交易批次后状态根的正确性。无论L2中的计算量有多大，ZK Rollup都能够快速在L1上进行链上验证。Optimistic Rollup使用欺诈证明的机制，在这种机制下，Rollup合约跟踪状态根的完整历史和每个批次的哈希值，合约会“乐观地”默认所有交易都是合法的，除非其被证明无效（无罪推定）。如果有验证者发现某个批次的后状态根不正确，他们可以发布一个错误性证明，证明该批次计算不正确。其他节点将一起验证该证明，并恢复该批次及其后续的所有批次。图7总结了Optimistic Rollup和ZK Rollup的优劣势对比。值得注意的是，ZK Rollup在TPS（每秒交易处理量）方面表现出色，并且在提款周期方面具有显著优势。其劣势在于EVM兼容性和L2层的计算消耗：  Optimistic Rollup项目，如Optimism和Arbitrum，分别使用OVM和AVM，它们的虚拟环境与EVM基本相同，因此可以直接将L1层的合约迁移到L2上进行部署。然而，在ZK Rollup中，将ZK-SNARK用于证明通用的EVM执行是相当困难的，因为EVM并不是按照ZK证明计算的数学需求来开发的，所以需要对某一类的EVM客户端进行改造，以利用ZK技术来验证交易和合约运行。  同时，即使经过相应的转化，ZK运算仍然需要大量的计算能力，因此在L2层的效率上ZK Rollup不及Optimistic Rollup。  ZK Rollup提供了比Optimistic Rollup更好的数据压缩功能，因此能够在L1上提交更小的数据。  由于ZK中的证明验证过程更快捷，且具有更高的批处理密度，因此在L1层的计算消耗上ZK Rollup较低。可以理解为L2上的节点付出大大减轻了对L1节点的要求，从而显著提升了L1层的可扩展性。图7 两种Rollup方案的对比2.4 ZK Rollup 还是 zkEVM Rollup？虽然ZK Rollup看起来很有吸引力，但在实际部署中存在诸多困难。目前，ZK Rollup仍然具有相当大的局限性，而Optimistic Rollup仍然是主流方案。大多数已实现的ZK Rollup也都是为某些特定应用程序定制的。如何理解定制化的ZK Rollup？开发者为不同DApp构建专用电路（“ASIC”），如Loopring、StarkEx rollup和 zkSync 1.0，它们支持特定类型的支付、Token交换或者是NFT铸造，然而，它们的电路设计需要高度的技术知识，这导致了开发者体验的不佳。以特定类型的支付数据为例子，节点将交易数据提交给定序器，由定序器打包给提验证（proof）的节点作为公开的输入，证明过程和虚拟机上的合约执行过程无关，ZK只是负责将某个特定的执行结果的Rollup计算、压缩过程进行进行证明。而zkEVM Rollup又代表了将虚拟机运行结果Rollup的能力。当在L2层运行通用的智能合约，需要证明合约运行前后状态转换的有效性时，便需要有一个虚拟环境能够支撑ZK算法的运行。因此，运行合约，输出最终状态，证明合约执行过程的有效性，并将交易记录，账户记录，状态变化记录数据一同rollup提交，这便是zkEVM的意义。而L1层只需要快速验证证明，开销较小，无需再次运行合约，图8说明了zkVM的作用。需要注意的是，zkEVM其实是运行在L2层的类EVM虚拟机，因此更为精确的说法是Zero Knowledge Virtual Machine，zkVM，只不过大家强调其兼容以太坊而称之为zkEVM。图8 zkVM说明现有项目也在考虑逐渐放弃了为特定应用程序做优化，而升级转向支持运行通用合约即zkEVM Rollup。因此，zkEVM Rollup虽然作为ZK Rollup的下位概念，在大部分情况下，提起ZK Rollup时便指zkEVM rollup。3 理解zkEVM要理解zkEVM，首先需要对EVM（Ethereum Virtual Machine）的概念有一个基本的认知，才能更好地认识到当前zkEVM的局限性。3.1 从EVM到zkEVM在以太坊社区中关于EVM的经典表述：以太坊虚拟机由成千上万台运行着以太坊客户端、相互连接的计算机节点共同维护。以太坊协议的存在主要是为了保持这个独一无二的状态机连续、不间断、不可变地运行。EVM定义了如何在不断更迭的区块中定义标准的状态。它的行为类似于一个数学函数：给定一个旧的有效状态和一组新的有效交易T，EVM状态转换函数将生成一个新的有效状态。在这个图灵完备的状态机上，可以运行任何代码。EVM按照一定的规则执行交易。首先，人类可读的编程语言（如Solidity和Move）将被编译为面向机器的16进制“低级”语言，称为EVM字节码（bytecode），最终按照操作码（Opcode）解析为顺序指令列表。因此，zkEVM是一个至少在编程语言层面兼容EVM的zkVM。然而，EVM的设计存在一定的局限性，其中与本文最相关的是EVM无法与ZK相应协议兼容。因此需要对原有的EVM进行一定的改造，这种改造是相当具有挑战性的：  EVM对椭圆曲线的支持有限。  EVM基于256位整数运行，而零知识证明则“天然”基于素域运行。  EVM与传统虚拟机不同，具有许多特殊的操作码。  EVM是基于堆栈的虚拟机，而不是更高级别的零知识证明友好型中间表示（IR，Intermediate Representation，即计算机程序编译过程中源代码和目标代码之间的中间状态）。[1]3.2 zkEVM兼容性：三种类型在Vitalik的文章中对于改造程度和EVM兼容程度做了四个层面的划分，本文对其做更为明晰的理解。[2]      类型一：以太坊等效。zkEVM和EVM在共识层面兼容，这意味着在一定程度上，zkEVM可以完全融入L1作为等效的P2P节点进行协议通信，zkVM和EVM可以等效运行。在这种情况下，zkEVM可以在L1层面证明多个交易，加快其他节点对区块的验证。这意味着L1层本身具备扩容能力，无需建立围绕zkEVM的L2。这种类型的EVM效率非常低，仍处于“研究”阶段，并且大规模部署可被视为以太坊的升级。类似的情况还有另一种极端情况，即公链运行zkVM，L1证明本身数据而放弃以太坊原有架构，以减少生成证明的时间。        类型二：EVM等效。zkEVM和EVM在字节码层面兼容，但在共识层面不兼容，将大部分EVM操作码转化为电路。这种类型涵盖了目前项目范围最广的情况，根据Vitalik的博客，可细分为三种类型。这些类型在gas费用（满足ZK协议复杂操作的费用）、部分操作码兼容性、哈希函数类型、区块、交易树和状态树结构、证明时间等方面存在一定的差异。这些不同方面的不兼容性可能导致较高的证明/验证开销、较低的效率，或对开发者在合约部署和运行方面产生影响。然而，与类型一和类型三相比，类型二有着本质的区别。由于zkEVM项目目前正处于高速迭代阶段，技术细节的差异评估变得困难，因此笔者认为只需要关注项目之间的技术差异，而不必强求对类型的具体界定。        类型三：高级语言编译等效。这是最低层级的“兼容”方式，将流行的高级编程语言编译为ZK-SNARK友好的语言。在这种情况下，底层环境和虚拟机特性与EVM完全不同，最直接的体现是在字节码层面与EVM不兼容。开发者无法直接复制和修改EVM字节码，并且可能与以太坊技术支持脱节，缺乏一定的ERC标准，无法保持与EVM同步的安全性。  所谓兼容性，是在有限的ZK协议证明能力，网络协议设计难度，网络算力下，对运行效率、投入计算量和开发适配性的一个取舍。      运行效率针对链上用户而言，最为直观体现是交易速度，有关L1上验证者效率，L2上证明者效率，zkEVM硬件加速程度等；        投入计算量体现L1、L2层网络的gas费；        开发适配性从合约开发者角度，考虑合约兼容程度和基础设施通用性等，和特殊操作码、部分ERC协议是否适配等有关。  类型三的zkVM完全实现一个ZK友好型VM，在性能方面具有优势，但DApp开发者有相当高的迁移成本，同时无法同步以太坊的最新特性和安全性。而在类型二中，往往证明者证明速度（满足电路转化的要求）、计算量和基础设施通用程度（满足以太坊的要求）呈负相关。3.3 当前ZK赛道梳理围绕zkEVM的不同等效程度的优劣势，ZK赛道在以下几个方面逐渐兴起：  ZK以太坊兼容和电路编译：解决EVM兼容性的核心问题，是赛道焦点，各类其他生态项目均围绕其展开。  ZK公链，如Manta Network等：放弃Ethereum，建立新的zkVM，注重L1层的扩容问题，同时关注数据隐私问题。  ZK跨链桥和预言机：作为ZK赛道的基础设施，用于在多个zkEVM Rollup的L2之间进行资产迁移或获取链下数据。  ZK硬件加速：作为ZK赛道的基础设施，解决生存证明计算量大的问题。目前主要依赖GPU进行加速，预计在2023年年底之后，将出现支持zkEVM证明的专用硬件（ASIC），以及相对成熟可用的产品。  ZK工具类：根据兼容程度提供不同的开发工具，以支持不同类型的ZK应用。  ZK应用：虽然与本文关联较小，但更多关注于ZK隐私性的优势，探索在各个领域中应用ZK技术的潜力。4 zkEVM Rollup项目概览在关注2023年上半年涌现的各类zkEVM项目时，以下几个方面是关注重点：  项目进展：关注项目的当前阶段，测试网和主网的预计上线时间，以及与其发展路线图的一致性。  实际交互情况：通过与测试网或主网的交互，获取网络的TPS（每秒交易数）以及单笔交易确认时间等指标，以对网络性能有直观的了解。  zkEVM兼容性：这是最核心且最具挑战性的技术点，需要关注项目中的ZK协议、VM安全性和兼容程度等方面的表现，尤其是对于开源的项目，深入研究其VM层面的技术细节。  zkEVM Rollup架构：大多数项目都会公开其Rollup架构，关注其整体的去中心化程度，尽管这些项目在总体上相似，但也需要关注细微的差异。  生态运营：考察项目的用户数量、活跃程度，链上应用生态的运营和孵化情况，以及开发者社区的维护等软性指标，以了解项目的运营状况。  投融资情况：关注项目的融资状况和投资者的参与情况。本文将主要聚焦在前四个方面，着重从技术角度对zkEVM Rollup项目的整体架构进行评估和考量。4.1 ScrollScroll团队成立于2021年，专注于开发EVM等效的ZK Rollup以扩展以太坊。在过去的两年中，Scroll与Privacy and Scaling Explorations团队以及其他开源贡献者合作，共同努力构建与字节码兼容的zkEVM。在2月底，Scroll宣布他们的Alpha测试网已在Goerli上线，任何用户都可以参与技术测试，无需许可。测试网的平均出块时间为3秒，已经处理了超过2千万笔交易、150多万个区块和400万多个交互地址。同时，Scroll于4月11日推出了网站生态系统界面。根据最近的信息披露，Scroll在实现类型二EVM等效的目标上取得了持续进展。他们已经完成了所有EVM操作码的兼容开发工作，并正在进行审计。下一个目标是兼容EIP2718交易。从技术架构上看，Scroll的架构相对传统，这里将进行详细介绍。如图9所示，主要分为两个部分：核心部分是zkEVM，用于证明在L2上执行的EVM的正确性；但要将zkEVM变成完整的以太坊ZK Rollup，还需要构建一个完整的L2架构。目前的Scroll Alpha测试网络由Scroll Node、Bridge Contract和Rollup Contract组成。[3]图9 Scroll rollup整体架构 | 图源：https://scroll.io/blog/architecture  Scroll Node：由Sequencer、Relayer和Coordinator组成。          Sequencer，即定序器，向用户和应用开放JSON-RPC，读取交易池中的交易并生成L2的区块和状态根。目前的Scroll定序器节点是中心化的，但在未来的升级中将逐渐实现去中心化。      Coordinator，负责在Roller和Scroll Node之间进行通信。当Sequencer生成新的区块时，Coordinator会在池中随机选择一个Roller进行证明生成。      Relayer，监测以太坊和Scroll链上的Bridge Contract和Rollup Contract。Rollup Contract保证L2数据在L1层面的数据可用性，确保在L1层可以恢复L2区块，L2层提交的区块经过L1层上Rollup Contract的有效性验证，该区块在L2才成为不可更改的最终状态。Bridge Contract在跨链时负责双链合约之间的通信，可实现双向消息传递，以及跨链资产的质押和提取操作。      图10 Roller Network | 图源：https://scroll.io/blog/architecture  Roller Network：Roller内置zkEVM，在网络中充当证明者，负责为ZK Rollup生成有效性证明，见图10。          Roller首先从Coordinator接收execution trace（即合约执行的具体操作和涉及的地址），然后将其转换为电路的witnesses（证明所需的信息）。      Roller为每个zkEVM电路生成证明，并最终聚合来自多个zkEVM电路的证明。      4.2 StarkWareStarkWare是一家提供基于STARK的扩展解决方案的公司，旨在确保L2具备安全性、快速性和无缝用户体验。StarkWare支持多种数据可用性模式，并通过StarkNet作为其L2网络，以及StarkEx作为面向企业用户的Rollup验证服务，让DApp可以构建在StarkEx服务之上。然而，目前的实现仍然是针对特定DApp进行定制化的电路编写，而不是通用的zkEVM Rollup。StarkEx支持各种即插即用的服务，例如NFT铸造和交易、衍生品交易等。在生态方面，去中心化期货合约交易平台DYDX是StarkWare的忠实用户。StarkNet本质上是zkVM，没有针对以太坊操作码进行ZK电路的设计，而是采用了一套更加ZK友好的汇编语言、AIR（代数中间表示）和高级语言Cairo。尽管StarkNet本身不兼容EVM，但可以通过其他方式（如Kakarot）与以太坊进行兼容。需要注意的是，StarkNet相对来说还是一个中心化的项目，无法与以太坊的安全性升级同步，因此需要集中的研发人员来弥补安全性方面的不足，并开发适配以太坊新协议。StarkNet采用STARK作为其证明系统，相对于SNARK，STARK具有更多的创新。它不需要依赖“可信设置”，并且具有更简化的密码学假设，避免了对椭圆曲线、配对和指数知识的需求，仅依赖哈希和信息论，从而更具抗量子攻击能力。总体而言，STARK比SNARK更安全。在可扩展性方面，STARK具有显著的边际效应，证明规模越大，总体成本越低。然而，在架构方面，目前系统中只有一个Sequencer（定序器），由StarkWare控制，并且只有一个Prover（生成ZK Proof的证明者），不仅为StarkNet生成证明，还为运行在StarkEx Rollup上的其他所有应用程序生成证明。4.3 Validium和VolitionValidium是一种L2级别的扩展解决方案，类似于ZK Rollup，它利用计算证明（如ZK Rollup）来确保交易过程的完整性。然而，与ZK Rollup不同的是，Validium并不在以太坊主网上存储交易数据。虽然这种权衡会导致链上数据可用性的牺牲，但也带来了巨大的可扩展性改进，使Validium每秒可以处理约9000笔交易。尽管Validium在处理交易效率上表现出色，但在笔者看来，它并不能被严格视为ZK Rollup。这个方案和Plasma类似，都没有实现L1层的数据可用性，因此也无法称为真正的Rollup。不过，Validium与Plasma的区别在于其采用了类似于OP Rollup的“七天退出机制”，而利用了ZK技术来缩短L2层对数据的验证时间，并避免将数据同步到L1层。StarkWare率先推出了名为Volition的方案，使用户能够轻松在ZK Rollup和Validium之间切换。例如，对于一些应用程序，如去中心化衍生品交易所，可能更适合采用Validium，同时仍希望与ZK Rollup上的应用程序进行互操作，这时Volition提供了这种可切换性。这使得用户可以根据自己的需求和优势选择最适合的方案。4.4 zkSync与StarkNet类似，zkSync始终坚持选择上文提到的高级语言等效的zkVM，并且备受关注，其在市场上拥有相当高的热度和用户锁仓量。zkSync 1.0（zkSync Lite）于2020年6月15日在以太坊主网上启动，实现了约300 TPS的交易吞吐量，但不兼容EVM。而zkSync 2.0（zkSync Era）于2023年3月24日启动。zkSync Era的目标是通过使用他们自定义的虚拟机（VM）进行优化，而不是追求EVM等效性，以实现更快的证明生成。它通过强大的LLVM编译器支持Solidity、Vyper、Yul和Zinc（rollup的内部编程语言），以此来实现大部分智能合约功能。由于采用了自研的虚拟机，zkSync Era支持原生账号抽象，使得任何账户都可以使用任何代币支付费用。此外，通过应用zkPorter协议，结合ZK Rollups和分片技术，zkSync网络的吞吐量得到了指数级的增长，达到了20,000+ TPS（类似于Volition中的数据可用性切换）。总体而言，zkSync是一个生态丰富的L2项目，备受开发者和投资者的关注。但目前仍存在一个问题，即开发者能否在高级语言等效的zkVM上获得良好的开发和迁移体验。目前仍然缺乏开发者角度的使用报告，如果开发者能够有良好的体验，那么其他类型努力兼容EVM的zkVM又有何意义呢？在这方面还需要更多时间来观察和评估。4.5 Polygon zkEVMPolygon于3月27日启动了zkEVM Rollup主网络的Beta版，该网络是以太坊等效的虚拟机，并开源了所有zkEVM代码。与zkSync相比，Polygon zkEVM的锁仓量较小，但生态系统中仍存在有趣且活跃的项目。在Rollup的设计方面，Polygon与Scroll有所不同，他们采用了效率证明（PoE）模型来激励排序器（Sequencer）和聚合器（Aggregator），以应对去中心化和无许可验证器的挑战。在无需许可的排序器-聚合器两步模型中，任何排序器都可以申请打包批次并获得打包费用，但需要支付L1层的Gas费用并存入一定数量的代币；同时，聚合器需要设定自己的目标，以最大化每次证明生成的利润。此外，Polygon还与Volition（ZK Rollup和Validium）模式具有深度兼容的数据可用性模型，以为用户提供不同层次的服务。[4]此外，Polygon在ZK协议方面也投入了相当的工作量，并取得了显著的成果。在他们的文档中，他们总结了自己的技术优势，主要包括以下几点：  更高的兼容性：Polygon始终坚持采用EVM等效的zkVM，以降低开发者迁移dApp的成本。同时，尽管Polygon Miden采用了ZK-STARK协议，但仍支持运行Solidity合约。  更容易的验证：ZK Rollup经常受到批评，因为生成有效性证明需要昂贵的专用硬件，而这些硬件的运营成本转嫁给了用户。Polygon ZK Rollup（如Polygon Zero）旨在简化证明方案，使得更低级别的设备可以参与其中，例如，在消费级PC上进行的Plonky2证明生成测试。  更快的证明生成和验证过程：Polygon Zero可以在170毫秒内生成一个45kb的证明。[5]5 理论与现实本份研报主要介绍了ZK技术和Rollup机制，并着重强调了数据可用性的重要性。在区分ZK和zkEVM Rollup的问题上，对两者进行了界定。此外，详细梳理了zkEVM的三种类型以及相关的ZK赛道。最后，结合几个优势项目，回顾了其技术框架和现有生态。在具体项目方面，高级语言等效EVM虽然在理论研究中有很大的局限性，却占据了当前的主流地位，StarkWare这类中心化较为严重的产品也博得了市场的青睐。相较之下，zkEVM的“通用性”似乎成为了一种累赘，我们无法分辨出“高效拓展”突破了哪些问题并实现了超越理论的效果。当然，很多参与者、投资人实际上并不关心项目背后的技术特点，这似乎不太Web3，却又很Web3。Rollup技术的目的是进一步挖掘区块链的价值，但往往因为迫切需要成为市场上的“创新性概念”，而产生“开倒车”的现象，回归到中心化。这是当前市场存在的问题。区块链的价值很容易被看到，谁不想要一台永恒的计算机呢？但核心问题是，当这台计算机的运行能力远远低于我们身边任何一台服务器，并需要大量资源投入，甚至使用价值远低于投入成本，那么作为一个“公共产品”，它还能吸引更多人加入使用吗？当我们已经拥有了相当多国家、社会甚至个人的产品时，在什么情况下，我们愿意忽视高昂的使用成本，追求“永远在线，永远正确”的结果呢？我认为这是当今区块链行业需要思考的问题。Rollup技术在技术上可以改善这个问题，但还有一大部分问题需要留给浮躁的市场去解决。参考：[1] zkEVM - HackMD[2] The different types of ZK-EVMs (vitalik.ca)[3] Scroll’s Architecture Overview - Scroll[4] Polygon zkEVM Documentation - Hermez 1.0 Documentation[5] Polygon ZK Rollups: Everything You Need to Know (alchemy.com)"
  },
  
  {
    "title": "Avalanche 共识协议详解",
    "url": "/posts/avalanche/",
    "categories": "Consensus",
    "tags": "avalanche, consensus",
    "date": "2022-12-24 12:00:00 +0800",
    





    
    "snippet": "本文对雪崩共识协议簇进行介绍。该协议簇是一系列无主拜占庭容错协议，使用基于网络重复随机采样的亚稳态机制构建。雪崩协议能够在存在拜占庭节点的情况下提供强概率安全保证，且并发和无主性质使其能够实现高吞吐量和高可扩展。协议主要目标：      绿色 (green)：能源消耗很少。        静止 (quiescent)：没有交易时不需要工作（出块）。        高效 (efficient)...",
    "content": "本文对雪崩共识协议簇进行介绍。该协议簇是一系列无主拜占庭容错协议，使用基于网络重复随机采样的亚稳态机制构建。雪崩协议能够在存在拜占庭节点的情况下提供强概率安全保证，且并发和无主性质使其能够实现高吞吐量和高可扩展。协议主要目标：      绿色 (green)：能源消耗很少。        静止 (quiescent)：没有交易时不需要工作（出块）。        高效 (efficient)：节点交互复杂度 O(knlogn) ~ O(kn)。  以上目标的实现主要依赖以下几种措施：      并行共识模型：不使用单一复制状态机 (RSM) 模型，每个节点维护自己的 RSM（可以互相转移所有权），系统对有关联的交易只维护偏序。        重复随机采样：引导诚实节点产生相同输出。        亚稳态决策：亚稳态决策可以使得一个大网络快速推进到一个不可逆的状态。  该算法可以提供强概率安全性保证，并且保证诚实节点的活性。所谓“强概率安全保证”，是指共识被逆转的可能性小到可以被忽略（甚至小于哈希碰撞的概率），实际上 PoW 提供的也是强概率安全性保证。1 基础首先介绍下论文中的一些基本概念。任何基于复制状态机 (Replicated State Machine, RSM) 的分布式系统，都需要满足两个特性，即安全性和活性，文中定义如下：  P1. Safety. No two correct nodes will accept conflicting transactions.  P2. Liveness. Any transaction issued by a correct client (aka virtuous transaction) will eventually be accepted by every correct node.2 Slush（雪泥）论文首先提出了名为 Slush 的共识算法，也是整个协议簇中最简单最基础的部分。Slush 是非拜占庭协议，意味着其不能容忍作恶节点的存在。作者抽象出一个染色问题：任何时刻所有节点可能处于 $\\lbrace red,blue,⊥\\rbrace$ 三种状态之一，共识就是如何通过节点间的通信使得所有节点最终的颜色一致。Slush 算法描述如下：详细分析：  在起始时刻，所有节点都是未着色状态；  节点 $u$ 循环发起查询，总共 $m$ 轮，每轮随机选择 $k$ 个样本发送包含自身颜色的查询；  当其他节点 $v$ 收到一个查询时，则返回一个包括自己颜色的响应，如果此时节点 $v$ 还是未着色状态，则先将查询中的颜色更新为自己的颜色再响应；  一旦收集到 $k$ 个响应，节点 $u$ 判断是否存在某个颜色数大于等于 $αk$，$α &gt; 0.5$ 表示一个协议参数。如果存在某个颜色满足该阈值则将自身颜色更新为该颜色；  如果没有在限定时间收集到 $k$ 个响应，则 $u$ 重新采样发送查询，直至收集到 $k$ 个响应Slush 有如下优点：  无记忆，每轮之间节点除了自身颜色不记录额外信息；  小样本采样，不同于其他算法需要对所有节点发送请求，Slush 只需要发送 $k$ 个请求；  抗亚稳态，即便是 50/50 的初始状态，也可以通过采样的随机扰动打破平衡，然后反复采样放大优势；  如果 $m$ 足够大，算法可以保证所有节点都有同等机会被染色；但是 Slush 并不能提供足够强的拜占庭安全保证，如果存在拜占庭节点故意将自身颜色变成和主流颜色不一样，则可能打乱平衡。因此 Slush 并不是 BFT，但为后续机制提供了基础。3 Snowflake（雪花）相比 Slush，第二个算法 Snowflake 做了进一步改进：详细分析：  每个节点引入了一个计数器 $cnt$ 变量，初始 $cnt=0$，每当一轮查询返回的 $k$ 个响应某个颜色 $col ≥ αk$，则将其 $cnt+1$ ；  如果满足条件的颜色和自身颜色不同，则将自身颜色设定为该颜色，并且重置 $cnt$；  引入另一个安全系数 $β$ ，当 $cnt$ 大于 $β$ 时，则最终确定该节点颜色，而不需要执行 $m$ 次查询。当给定一个 $ε$-guarantee 的拜占庭环境，Snowflake 可以保证安全性和活性。4 Snowball（雪球）Snowflake 记忆的状态是短暂的，每次颜色变化都会将计数器清零。为了使得系统更加难以被攻击，Snowball 增加了一个置信度计数器 (confidence counter) 。算法具体如下：详细分析：  对每个颜色都增加一个置信度计数器，例如 $d[R]$、$d[B]$；  每当一轮查询返回的 $k$ 个响应某个颜色 $col’ ≥ αk$，将该颜色的 $d[col’] + 1$，如果 $d[col’]$ 最大，则将自身 $col$ 更变为该 $col’$；  进一步地，如果 $col’$ 和上次响应 通过阈值的 lastcol 不一样，则更新 lastcol 为 $col’$，并且重置 $cnt$；  如果 $col’$ 和上次响应通过阈值的 $lastcol$ 一样，则 $cnt+1$ ，当 $cnt$ 大于 $β$ 时，则最终确定该节点颜色。总的来说，Snowball 中仍然是连续 $β$ 满足阈值条件的颜色才可能成为最终颜色，但置信度计数器的引入保证了最终确定的颜色同样具有高信任度。5 Avalanche DAG（雪崩）作为论文中协议簇的最终版本，Avalanche 将 Snowball 拓展为 multi-decree protocol。Avalanche 在 Snowball 的基础上引入了有向无环图 (DAG) 的存储结构，即在冲突集中使用 Snowball 协议共识出一笔交易。这会带来两点好处：      更高效：对 DAG 某个顶点的投票隐含了对“从创世顶点到该顶点”这整条链的认可。        更安全：DAG 把各个交易的命运交织在了一起，因此共识更加难以被逆转（需要更多诚实节点的认可）。  首先解释一下几个术语：      祖先集 (ancestor set)：每个交易可能有一到多个父交易，从它们的父交易一直追溯到创世顶点过程中的所有交易组成该交易的祖先集。        子孙集 (progeny set)：该交易的所有子孙后代交易的集合。        冲突集 (conflict set)：如果两笔或多笔交易发生了冲突（例如使用了同样的 UXTO 输入，即双花），那么它们组成一个冲突集。        凭证 (chit)：如果某笔交易查询成功（超过阈值），则称该交易收到了一个凭证，否则凭证值为 0 。  为了适用于资产交易这样的真实场景，Avalanche 相比前三个算法引入了很多新概念，具体如下：      $\\mathcal{T}_u$ 表示节点 $u$ 内所记录的所有交易集合；        $\\mathcal{P}_T$ 表示所有和交易 $T$ 冲突的交易集合（$P_T$ 应该包括 $T$ 本身）    比如两个冲突的交易 $T_i$ 和 $T_j$ ，由于冲突具有传递性，有 $\\mathcal{P_{T_i}}=\\mathcal{P_{T_j}}$ ，每个冲突交易集都有 $pref$ 和 $cnt$ 两个属性，简单地说这些都跟交易 $T$ 的置信度相关，具体后文会详细介绍。        $T’\\leftarrow T$ 表示交易 $T$ 的父交易是 $T’$ ；        $T’\\leftarrow^* T$ 表示从在至少一条路径从交易 $T$ 到交易 $T’$ 。  图中，每个方块表示一笔交易，具有一对 &lt;chit, confidence&gt; ，可以看到颜色更深的方块置信度更高。而每个交易都有一个冲突交易集，例如 $T_9$、$T_6$ 和 $T_7$ 属于同一个冲突交易集，即 $P_{T_9} = P_{T_6} = P_{T_7}$，但由于 $T_9$ 具有更高的置信度，因此该交易集的优先交易 (preferred tx) 为 $T_9$。Avalanche 会将每笔交易存储在一个 DAG 中，DAG 中每个元素可能有多个父交易，并且需要注意的是父子关系不代表在应用层面相互依赖。为了避免共识结果中包含冲突交易，在 Avalanche 中定义了冲突集，Avalanche 中的每笔交易都属于一个冲突集，冲突集中也只能有一笔交易，每个冲突集就是一个 Snowball 实例，但是置信度的计算方式略有不同，某笔交易的置信度是所有子孙交易的凭证值之和（$c$ 表示凭证，$d$ 表示置信度）。每个节点需使用 Snowball 协议从冲突集中共识出一笔交易，当这笔交易的置信度达到一个阈值时，即可认为这笔交易被最终确定了。具体流程：初始化上面提到每个 $P_T$ 包括了 $pref$ 和 $cnt$ 两个属性，初始化的 $P_T$ 只有 $T$ 本身，因此 $P_T.pref = T，P_T.cnt = 0$。当后续收到更多冲突交易后， $P_T$ 集合增加，$pref$ 和 $cnt$ 的更新则发生在收到响应 后。查询和响应详细分析：  节点 $u$ 找到新交易 $T$（还未确定的交易，即 $T ∈ \\mathcal{T} ∧ T ∉ \\mathcal{Q}$），启动一轮查询，即随机采样 $k$ 个节点发送包括该 $T$ 的查询，注意此处查询中实际上包含了 $T$ 和以及在 DAG 中所有 $T$ 可达的其他交易；  对收到查询的节点而言，如果查询中的 $T$ 以及其祖先交易在其冲突集中是优先选项 (preferred option)，则返回 yes-vote，反之则返回 no-vote；  节点 $u$ 收集返回的 $k$ 个响应，如果该 $T$ 满足阈值（即有超过 $αk$ 个节点返回认可该交易的响应），则该交易的 $cT$ 赋值为 1（获得一个凭证），并且更新 $T$ 交易所有祖先交易 $T’$  对应 $P_{T’}$ 的 $pref$ 和 $cnt$；  类似 Snowball，节点 $u$ 对 $T$ 有一个置信度值，其计算方法如下：\\[d_u(T)=\\sum_{T'\\in\\mathcal{T}_u, T\\leftarrow^*T'} c_{uT'}\\]简单地说就是交易 $T$ 的 $d(T)$ 等于其所有后代交易的 $cT’$ 之和，凭证的获取方式见第三步。需要指出的是，任何交易的凭证只可能为 0 或 1，但随着 DAG 规模的不断增长，$dT$ 也会随着其后代的增加而表现出单调增特性。这里再详细介绍下第二步中，查询返回的机制。当其他节点收到 $u$ 发送的查询后，在 DAG 中找到 $T$ 的所有祖先交易 $T’$ ，如果每个 $T’$  在其冲突交易集合 $P_{T’}$ 中都处于优先状态，则返回一个 yes-vote，反之返回 no-vote。具体描述如下：那么，什么时候应用可以接受 (accept) 或者提交 (commit) 一笔交易呢？Avalanche 把对最终交易的接受点和决定权交给了应用层。应用层通过自己定义的谓词 (predicate)，把接受交易的风险加入考虑。      如果冲突集中只有一笔交易（即没有冲突），置信度超过 $β_1$ 即可提交，称为“安全早期提交 (safe early commit) ”        否则，如果冲突集中有多笔交易，根据 Snowball 协议，需要收到超过 $β_2$ 个连续查询回复才可以提交  至此，Avalanche 整个流程介绍完毕。6 分析通信复杂度      Snowflake &amp; Snowball: O(knlogn)        Avalanche: 每个节点O(k), 总复杂度O(kn)  两大创新Avalanche 中的两大创新分别是：      子采样 (subsampling)：通信成本低。无论是 20 个节点还是 2000 个节点，某个节点发送的共识信息数量是恒定的。        传递性投票 (transitive voting)：即给某个顶点投票等同于给该顶点的所有祖先顶点投票，这样有助于提升交易吞吐量。每个节点实际上集合了很多投票。  可能存在的问题      随机采样达到的是非确定性共识    随机数对于区块链技术来说很关键。本质上，分布式账本的核心问题就是随机选择出块人的问题，这个随机性要能被全网确认，并且不能被操控，也不能被预测，否则恶意节点可以通过操控这个随机数从而达到操控整个链。在 Avalanche 中，随机采样是非常关键的，但是对怎么随机采样却没有像 Algorand 那样详细地描述，随机抽到样本的整体代表性也没有详细的理论论证，因此其所达成的共识只是一种概率性的共识，并非确定性共识。        冲突交易不受保护    如果用户不小心将一笔交易发送了两次，Avalanche 是无法在这两种交易之间做出选择的，会直接导致这笔钱丢失，这点被 Avalanche 当成能抵御双花攻击来做宣传，但是实际应用中，用户无任何主观恶意下，不小心将一笔交易点击两次发送的情况还是会经常发生，如果直接将用户资金丢失的话，每次发送交易都得非常小心并等待系统回复才行，这将大大降低交易的速度。        需要大量参与者的支持    随机采样所达到的共识必须依赖大量的节点支持才能算是有效，并且这些节点还得时刻保持在线，以便被随机抽取到，这在现实的自由网络下是不太现实的，如果采用云服务器的方式，整个网络运作成本将会非常高。  "
  },
  
  {
    "title": "Web3 组成架构纵览",
    "url": "/posts/web3/",
    "categories": "Blockchain",
    "tags": "blockchain, web3, defi",
    "date": "2022-12-14 12:00:00 +0800",
    





    
    "snippet": "Web3 发展至今，其生态已日趋完善，按照分层的方式对Web3 的组成架构进行抽象，可以分为四个主要层级，自下而上分别是：区块链网络层、中间件层、应用层、访问层 。本文将逐一介绍四个层级的作用及概况，以帮助读者更加全面、清晰地理解Web3。            Web3 组成架构                  访问层              应用层              中间件层...",
    "content": "Web3 发展至今，其生态已日趋完善，按照分层的方式对Web3 的组成架构进行抽象，可以分为四个主要层级，自下而上分别是：区块链网络层、中间件层、应用层、访问层 。本文将逐一介绍四个层级的作用及概况，以帮助读者更加全面、清晰地理解Web3。            Web3 组成架构                  访问层              应用层              中间件层              区块链网络层      1 区块链网络层架构最底层是 ⌈ 区块链网络层 ⌋，也是Web3 的基石，主要由各区块链网络所组成。该层级的区块链网络数量众多，如：Bitcoin、Ethereum、BNB Chain(BSC)、Polygon、Arbitrum、Polkadot、Cosmos、Celestia、Avalanche、Aptos、Sui 等。根据 Blockchain-Comparison 的统计，截止撰文之日的区块链至少有 150 条。此处主要指公链，联盟链不包括在内。因为区块链种类繁多，有必要分门别类进行介绍。  首先，不同区块链之间存在着分层结构，有 Layer0、Layer1、Layer2 之分。  其次，Web3 的繁荣发展，依赖于智能合约技术，而智能合约的运行环境为虚拟机。从不同的虚拟机维度上划分区块链，可分为两大类：EVM 链和 Non-EVM 链。EVM 是 Ethereum Virtual Machine，即以太坊虚拟机的简称。EVM 链即为兼容 EVM 的区块链，而 Non-EVM 顾名思义是不兼容 EVM 的区块链。  最后，还可以根据存储的数据大小进行分类，可以分为计算型区块链和存储型区块链。先从分层结构说起。最好理解的是 Layer1，我们所熟知的比特币、以太坊、EOS、BSC 都属于 Layer1，也称为主链。在分布式系统中，存在 CAP 定理，即一个分布式系统不可能同时满足三个特性：一致性、可用性、分区容错性。一个分布式系统只能同时满足三项中的两项。Layer1 的区块链本质上也是分布式系统，也同样存在不可能三角问题，只是三个特性与 CAP 不同，分别为：可扩展性、安全性、去中心化，每个区块链也只能满足三项中的两项。比特币和以太坊偏向于安全性和去中心化，所以可扩展性比较弱，TPS 比较低。EOS 和 BSC 则只依赖于少数节点来维护共识，相比于比特币和以太坊，减低了去中心化特性，但提高了可扩展性，从而能达到很高的 TPS。为了解决比特币和以太坊的可扩展性问题，衍生出了 Layer2。Layer2 是作为依附于主链的子链而存在，主要用于承载 Layer1 的交易量，承担执行层的角色，而 Layer1 则变为结算层，可大大减少交易压力。目前主流的 Layer2 都是扩展以太坊的子链，包括 Arbitrum、Optimism、zkSync、StarkNet、Polygon 等。比特币也有 Layer2，主要包括闪电网络、Stacks、RSK 和 Liquid，但目前都比较小众。Layer0 一般被定义为区块链基础设施服务层，主要由模块化区块链所构成，包括 Celestia、Polkadot、Cosmos 等。模块化区块链这个概念主要由 Celestia 提出，其核心设计思路就是把区块链的共识、执行、数据可用性这几个核心模块拆分开来，每个模块由一条单独的链来完成，再将几个模块组合到一起完成全部工作。这和软件架构设计中所提倡的模块化设计思想是一样的，可实现高内聚低耦合。实现跨链通信的跨链桥或跨链协议也可以划入 Layer0。跨链桥的数量也非常多，撰写此文时，debridges.com 上统计的跨链桥多达 113 条，其中 TVL 排名最高的三个分别为 Polygon、Arbitrum、Optimism 的官方跨链桥，这几个桥分别实现了各自的 Layer2 和以太坊之间的资产跨链。TVL 排名第四位的则是 Multichain，其前身为 Anyswap，是连接了最多条区块链的第三方跨链桥，截至今年 1 月份时，其连接的区块链多达 81 条。聊完分层结构的划分，我们再从 EVM 的维度来梳理下不同的区块链。前面说过，从 EVM 维度上可划分为 EVM 链和 Non-EVM 链两大类。EVM 链是目前最主流的方向，基于 EVM 链的 DApp 和用户群体是目前整个 Web3 生态里规模最大的。有些链原生兼容 EVM ，比如 BSC、Heco、Arbitrum、Optimism 等；有些则是后期扩展兼容 EVM 的，比如 zkSync 1.0 并不兼容 EVM，而 zkSync 2.0 则是兼容 EVM 的。很多区块链就算早期并不兼容 EVM，但也逐渐在拥抱 EVM。比如，Polkadot 推出了 Moonbeam 平行链来兼容 EVM，Cosmos 则有 Evmos。目前来看，排名靠前的区块链中，大部分都已经兼容 EVM，不过依然还有少部分 Non-EVM 链存在，比如 Solana、Terra、NEAR、Aptos、Sui。另外，EVM 链的智能合约主要使用 Solidity 作为开发语言，而 Non-EVM 链则主要使用 Rust 或 Move 语言开发智能合约。前面提到的区块链，主要是偏向于解决去中心化计算的区块链，这些区块链普遍不支持大数据的存储，比如文件存储。而存储型的区块链则聚焦于解决大数据存储的问题，这类区块链目前不太多，主要有 Filecoin、Arweave、Storj、Siacoin 和 EthStorage。以上就是当前组成「区块链网络层」的主要区块链网络。2 中间件层在区块链网络层之上的是「中间件层」，主要为上层应用提供各种通用服务和功能。所提供的通用服务和功能包括但不限于：安全审计、预言机、索引查询服务、API 服务、数据分析、数据存储、基本的金融服务、数字身份、DAO 治理等。提供通用服务和功能的组件则可称为「中间件」，这些中间件也存在多种形式，可以是链上协议，也可以是链下平台，或链下组织，包括中心化企业或去中心化自治组织 DAO。下面对这一层包含中间件进行分类介绍。      安全审计。这是非常核心的中间件，因为 Web3 里的区块链和应用大多都是开源的，且很多都是跟金融强相关，因此，安全性就成为了重中之重，安全审计自然也变成了刚需。安全审计的服务大多由一些安全审计公司所提供，比较知名的审计公司包括：CertiK、OpenZeppelin、ConsenSys、Hacken、Quantstamp，以及国内主要有慢雾、链安、派盾等。另外，还有不少知名度不高的小审计公司。    除了审计公司，还有一些提供 Bug Bounty 的平台，项目方在这些平台上发布任务，由白帽黑客们来找 Bug，找到的 Bug 安全漏洞等级越高则可获得的赏金越高。目前，全球最大的 Bug Bounty 平台是 Immunefi。        预言机。是区块链系统与外部数据源之间沟通的桥梁，主要实现智能合约与链下真实世界的数据互通。因为区块链网络本身对状态一致性的限制，需要保证每个节点在给定相同输入的情况下必须获得相同的结果，所以区块链被设计成一个封闭系统，只能获取到链内的数据，而无法主动获取外部系统的数据。但很多应用场景中是需要用到外部数据的，这些外部数据就由预言机来提供，这也是目前区块链与外部数据实现互通的唯一途径。    根据预言机所提供的具体功能，目前对预言机的分类大致有：DeFi 预言机、NFT 预言机、SocialFi 预言机、跨链预言机、隐私预言机、信用预言机、去中心化预言机网络。具体的预言机项目有 CreDA、Privy、UMA、Banksea、DOS、NEST、Chainlink 等，其中，Chainlink 作为预言机的龙头，其定位为去中心化预言机网络，推出了 Data Feeds、VRF、Keepers、Proof of Reserve、CCIP 等一系列产品和服务。        索引查询。解决链上数据的复杂查询问题。如：查询 Uniswap 上某天的总交易量，如果直接在链上查询较为繁琐。因此就诞生了提供索引查询的服务，其主要代表为 The Graph 和 Covalent。The Graph 的实现方案主要是可定制化监听链上数据并映射成自定义的数据进行存储，从而方便查询。而 Covalent 则是将很多通用、广泛使用的数据封装成统一的 API 服务，供用户查询。    提到 API 服务，除了 Covalent，还存在解决其他不同需求的 API 提供商，比如：NFTScan，是聚焦于提供 NFT API 数据服务的；Infura 和 Alchemy，则主要提供区块链网络节点服务；API3，旨在打造去中心化 API 服务。    不管是索引查询服务还是 API 服务，都是链上数据相关的服务，数据分析也是数据相关的服务，这一版块的成员主要有 Dune Analytics、Flipside Crypto、DeBank、Chainalysis 等。        数据存储。数据存储中间件和底层几个专门做存储的区块链容易混淆，有人将底层的 Filecoin、Arweave、Storj 等划分到这一层，但笔者认为这些本质上还是底层区块链，所以将其划入到区块链网络层。而中间件层的数据存储，目前主要是 IPFS。IPFS 全称为 InterPlanetary File System，中文名为星际文件系统，是一个基于内容寻址、分布式、点对点的新型超媒体传输协议，其旨在取代 HTTP 协议。IPFS 与区块链网络很相似，但并不属于区块链网络，基于 IPFS 的 Filecoin 才是区块链网络。        基础金融服务。主要包括 Uniswap、Curve、Compound、Aave 等，Uniswap 和 Curve 提供了链上交易功能，而 Compound 和 Aave 则是链上借贷平台。这几个协议本质上是应用层的链上协议，但因为这些协议都逐渐被越来越多其他应用所依赖，可以用来组合搭建出不同的应用，于是演变成了通用性的应用协议，即下沉为了中间件的角色。  其实，任何具有可组合性的组件，不管是链上应用协议，还是链下提供不同服务的中心化实体，或是 DAO，只要其提供的服务和功能是大部分应用都需要的，就可以划入「中间件层」。不同的中间件就像乐高积木一样，通过组合可以创建出不同的应用。包括数字身份、DAO 治理的工具等，其实也都是同样道理。3 应用层应用层是 Web3 生态里最繁荣的一层，这一层里，充斥着各种不同的 DApps，可谓是百花齐放、百家争鸣。下面我们主要介绍几个发展得相对比较繁荣的板块。3.1 NFTNFT 全称为 Non-Fungible Token，表示「非同质化代币」，国内也称为数字藏品，用于代表艺术品等独一无二的数字资产。第一个真正意义上的 NFT 项目叫 CryptoPunks，于 2017 年 6 月发布，由 10,000 个 24x24 像素的头像所组成。每个头像都是由算法生成的，独一无二且所有头像都上传到了以太坊上，也是目前为止唯一一个将所有头像数据全部上链的 NFT 项目。下图为 CryptoPunks 官网展示的部分头像：截止撰文之日，CryptoPunks 的地板价（即最低价）为 66.88 ETH，按 ETH 的价格换算成美元，大概为 $84,397.21 美元。最贵的一个 CryptoPunk，成交价达到了 8000 ETH，成交于 2022 年 2 月 12 日。一个 NFT 头像为何会这么贵，这对于很多人都是很难理解的。其中最主要的一个原因，就是它是第一个 NFT 项目，就像比特币是第一条区块链一样，其开创性的地位所带来了巨大的价值潜力。受 CryptoPunks 的启发，一家名为 Axiom Zen（Dapper Labs 的前身）的公司于 2017 年 11 月底发行了 CryptoKitties，国内也称为加密猫、以太猫、谜恋猫。CryptoKitties 上线后便病毒式地传播开来，甚至造成了以太坊的拥堵，暴露出以太坊的性能问题。CryptoKitties 发行之前，Axiom Zen 的技术总监 Dieter Shirley 以 CryptoKitties 为案例，还提出了 ERC721 Token 协议作为 NFT 的通用技术标准，而随着 CryptoKitties 爆火后，以 ERC721 为主要技术标准的 NFT 被进一步采用，如今 ERC721 已经成为了所有 NFT 的基础标准之一。继 CryptoPunks 和 CryptoKitties 之后，NFT 开始逐渐遍地开花，NFT 生态逐渐蓬勃发展。NFT 发展至今，已经涉足到了多个领域，如果对 NFT 生态的所有组成部分做详细分类的话，可以多达几十种。如果只聚焦于 NFT 本身，即 NFT 的不同用例，那大致可以做出以下分类：收藏品、艺术品、音乐、影视、游戏、体育运动、虚拟土地、金融、品牌、DID。下面主要介绍每个分类的一些代表性的 NFT 项目。  收藏品其实很难单独定义为一个类别，宽泛地讲，几乎任何东西都可以归为收藏品，包括艺术品、游戏道具、虚拟土地等。能被定义为收藏品的 NFT 主要需具备一个特性：稀缺性。比如，10000 个 CryptoPunks 中，外星人的数量最少，所以有很高的稀缺性，而男性最多，稀缺性就很低了。最知名的收藏品 NFT，除了 CryptoPunks，还有 BAYC，全称为 Bored Ape Yacht Club，也称为无聊猿。无聊猿不只是一套单独的 NFT，其实只是「无聊猿宇宙」的开端，基于无聊猿之后，背后的团队 Yuga Labs 又相继发行了无聊猿犬舍俱乐部（Bored Ape Kennel Club，BAKC)、变异猿游艇俱乐部（Mutant Ape Yacht Club，MAYC），也发行了 ApeCoin（APE）代币，还推出了 Otherside，专为元宇宙打造的虚拟土地。这些，都已经形成了「无聊猿宇宙」系列 IP，而且无聊猿不只是在加密圈内流行，在圈外的周边产品也在不断增加，比如有无聊猿的帽子、衣服、雕像、餐厅等。无聊猿的成功已超越了 CryptoPunks，Yuga Labs 之后还直接收购了 CryptoPunks。  NFT 的特性能有效保护版权的所有权，所以在艺术品领域流行开来也是理所当然。艺术品 NFT 有几个代表性的作品值得介绍一番，首先是艺术家 Beeple 的作品，名为“每一天：第一个 5000 天（EVERYDAYS: THE FIRST 5000 DAYS）”，是将他过去 5000 天内每天创作一幅的所有作品（共 5000 幅）合成一个 NFT 图像，在 2021 年 3 月以 69,346,250 美元售出。第二个值得介绍的是生成艺术，也称为衍生艺术。生成艺术中的艺术品不是由人创作出来的，而是由编程算法自动生成的，最知名的 NFT 生成艺术平台叫 Art Blocks，是一个基于以太坊的随机生成艺术平台。艺术家们可以把自己设计的独特算法上传到 Art Blocks 平台，并设定特定数量 NFT 进行发行，NFT 会根据算法自动生成。最后再介绍目前最贵的 NFT 艺术品，叫 ”The Merge“，2021 年 12 月以 9180 万美元天价成交。与其他 NFT 不同，”The Merge“ 其实不是一个单独的作品，而是由多个「mass」代币动态组合而成的。销售的其实也是 mass 代币，当初共售出 312,686 个 mass 代币，共有 28,983 个买家，即是说，”The Merge“ 是由这 28,983 个买家共同拥有其所有权，每个买家所拥有的 mass 代币数量就代表了占有多少份额的所有权。”The Merge“ 也可以理解为是一个碎片化 NFT 作品。  音乐 NFT 的兴起和艺术品类似，主要也是因为版权。下面介绍几个具有代表性的音乐 NFT 相关人物，第一个要介绍的是 Justin David Blau，是美国 DJ 和电子舞曲制作人，以艺名 3LAU 而闻名。他是最早采用音乐 NFT 的人之一，在 2020 年秋天卖出了他的第一张 NFT。而在 2021 年 2 月底，凭借 Ultraviolet NFT 专辑为他带来了 1168 万美元的收入。2021 年 5 月又成立了 NFT 音乐平台 Royal，8 月份完成了种子轮融资 1600 万美元，有 a16z、Coinbase 等顶级机构参与。第二个要介绍的是 Don Diablo，荷兰 DJ、数字艺术家、唱片制作人、音乐家和电子舞曲创作者，他在 2021 年卖出第一部完整的音乐会电影 NFT，名为 “Destination Hexagonia”，成交价 600 ETH（当时为 126 万美元）。最后再介绍一个叫 Kingship 的摇滚乐队，这是一支由无聊猿组成的虚拟乐队，由环球音乐集团所组建。  NFT 也席卷到了影视圈，有几个知名的影视剧都陆续发行了 NFT，国外有《权力的游戏》《蝙蝠侠》《指环王》《黑客帝国》《行尸走肉》等，国内有《大话西游》《流浪地球》《我不是药神》《封神三部曲》等。  NFT 用在游戏里主要作为游戏资产的载体，相比于传统游戏内的资产，NFT 的形式对游戏玩家来说可以真正拥有游戏资产的所有权，且 NFT 可以在游戏外流通交易。第一个游戏 NFT 项目就是 CryptoKitties，每一只猫都是一个独立的 NFT。后面讲到 GameFi 小节再继续深入介绍游戏版块。  体育运动领域也同样涉足了 NFT，目前最知名的两大体育 NFT 平台是 NBA Top Shot 和 Sorare。NBA Top Shot 顾名思义主要以 NBA 为主，而 Sorare 则服务于足球领域。除了 NBA 和足球，橄榄球、棒球、拳击、摔跤也都纷纷推出了各自的 NFT 纪念品。  虚拟土地类 NFT 主要由一些主打「元宇宙」概念的项目所推行，比较知名的有 Decentraland、The Sandbox、Roblox、Axie Infinity Land、Otherdeed 等。  金融和 NFT 的结合，主要就是将 NFT 应用到 DeFi 中，比如 UniswapV3 中的流动性仓位就是 NFT。另外，还有一个思路则是先将 NFT 碎片化，接着将这些碎片后的 NFT 再赋予 DeFi 功能，比如可以赋予交易、借贷、质押挖矿等功能。  品牌和 NFT 的结合，主要是作为一种新的营销方式。这两三年陆续有各种品牌加入这个阵营，比如，奢侈品品牌有 GUCCI、LV、爱马仕等，餐饮品牌有 Taco Bell、星巴克、必胜客、可口可乐等，汽车品牌有迈凯伦、雪佛兰等，运动品牌有阿迪达斯、李宁、耐克等，还有很多其他品牌。  最后是 DID，全称为 Decentralized Identity，即去中心化身份。所有人都知道 DID 非常重要，但其发展还比较缓慢，目前除了细分领域 ENS 域名之后，还没有成熟的 DID 体系形成网络效应。目前，应用最广泛的只有域名，基于以太坊的 ENS 是龙头，ENS 之于 Web3，就相当于 DNS 之于 Web2。不同的是，ENS 解析的域名，映射的不是网站 IP，而是用户的以太坊地址。比如，以太坊创始人 V 神的 ENS 为 “vitalik.eth”，映射的地址为 0xd8da6bf26964af9d7eed9e03e53415d37aa96045。NFT 的可应用场景纷繁复杂，以上列出的分类还未覆盖到全部。 NFT 的特性使得任何具有所有权的东西都可以被其所指代，因而坊间也有“万物皆可 NFT”的说法。3.2 DeFiDeFi 即去中心化金融，崛起于 2020 年夏天，因此那段时间也被称为 DeFi Summer。根据 TradingView 的统计数据，2020 年夏天刚崛起时，DeFi 总市值仅 50 亿美元，随后一路飙升，在 2021 年底达到了最高峰，将近 1800 亿美元。DeFi 有很多细分板块，主要包括：稳定币、交易所、衍生品、借贷、聚合器、保险、预测市场、指数等。稳定币主要可分为三类：中心化稳定币、超额抵押稳定币、算法稳定币。其中，超额抵押稳定币和算法稳定币为去中心化稳定币。中心化稳定币直接与法定货币挂钩，由中心化机构所发行，要求每单位稳定币需要有 1:1 的法币储备。目前交易量最大的两个稳定币 USDT 和 USDC，都是法币抵押稳定币，与美元 1:1 挂钩，分别由 Tether 和 Circle 两家中心化机构所发行。另外，币安，全球第一大中心化数字货币交易所，联合 Paxos 发行了自己的法币抵押稳定币 BUSD，目前也是全球交易量排名第三的稳定币，仅次于 USDT 和 USDC。超额抵押稳定币通过超额抵押其他加密货币而锻造，抵押品会被锁定在智能合约里，智能合约会根据抵押品的价值锻造出对应数量的稳定币，智能合约依靠价格预言机来维持与法币的锚定。此类型的稳定币主要以 DAI 为代表，由 MakerDAO 推出，和美元保持 1:1 锚定，目前交易量排名第四。算法稳定币则比较新颖，顾名思义，主要是通过算法来控制稳定币的供应。此赛道的选手也不少，包括 UST、FEI、AMPL、ESD、BAC、FRAX、CUSD、USDD、USDN 等，但目前还没有一个真正实现稳定的算法稳定币出现。接着，来聊聊交易所，DeFi 里的交易所是指去中心化交易所，简称 DEX。DEX 是 DeFi 所有板块里市值占比最高的板块，也是 DeFi 的基石板块。如果对 DEX 再进一步细分，还可以分为现货 DEX 和衍生品 DEX，衍生品 DEX 主要交易永续合约或期权。如果从交易模式上划分，那 DEX 主要可分为两种：Orderbook 模式和 AMM 模式。Orderbook 模式的 DEX，主要包括 dYdX、apeX、0x、Loopring 等。AMM 模式的 DEX 则比较多了，主要包括 Uniswap、SushiSwap、PancakeSwap、Curve、Balancer、Bancor、GMX、Perpetual 等。Orderbook 模式是最早出现的交易类型，交易方式和股票盘口的买卖方式一样，交易用户可选择成为挂单者（maker）或吃单者（taker），交易会根据价格优先和时间优先的规则撮合成交。采用 Orderbook 的 DEX，根据其发展历程主要还可以再分为三种模式：纯链上撮合+结算模式、链下撮合+链上结算模式、Layer2 模式。纯链上撮合结算模式，用户提交的挂单和吃单都是直接在链上，吃单会直接和链上的挂单成交。该模式的代表为 EtherDelta，其优点是完全链上，去中心化程度高，但缺点是交易性能很低且交易成本很贵，用户挂单、撤单都需要支付燃料费。链下撮合+链上结算模式的代表则是 0x 协议，相比于第一种模式，主要多了链下的「中继器」角色，用户通过链下签名的方式生成委托单并提交给中继器，由中继器来维护 Orderbook，撮合成功的委托单再由中继器提交到链上进行结算。因为将撮合移到了链下处理，大大提高了交易性能，但结算是一笔笔单独结算的，所以结算的性能成为了瓶颈。Layer2 模式的代表为 dYdX，背后所使用的技术主要由 StarkWare 所提供的产品 StarkEx 所支持。其基本原理就是部署一个单独的、专用的 Layer2，用户的撮合交易和结算都发生在这个 Layer2 上，然后定时将所有交易记录（包括结算记录）全部打包生成证明并发送到 Layer1 上进行验证。与 Layer2 公链不同，Layer2 公链提供的是通用交易，而 dYdX 背后所使用的这个 Layer2 只能用于专用的交易场景，这其实算是个私有链，也可称为应用链，这也是一种新的应用模式。这种模式的交易体验和中心化交易所已经相差无几了，但中心化程度比较高。完全去中心化且交易体验也较好的交易模式，目前主流的就是 AMM 模式了，AMM 为 Automated Market Maker 的简称，也称为自动做市商模式。引爆 AMM 模式的是 Uniswap，于 2018 年 11 月上线，之后的 SushiSwap、PancakeSwap、Curve 等都是基于 Uniswap 的模式进行改造。该模式需要流动性池作支撑，流动性提供者（简称 LP）往交易池里注入资产作为流动性，其实就是资金池，然后用户直接和流动性池进行交易，而 LP 则从中赚取用户的交易手续费。关于交易所暂时就先聊这么多，接着来看看衍生品。DeFi 衍生品板块主要包括几个方向：永续合约、期权、合成资产、利率衍生品。永续合约也是期货合约，加了杠杆的交易产品，前面提到的 dYdX、apeX、GMX、Perpetual 就是知名的几个永续合约 DEX。期权比期货复杂，DeFi 期权领域的玩家主要包括 Hegic、Charm、Opium、Primitive、Opyn 等，但目前期权市场还很小，被关注的不多。合成资产是由一种或多种资产/衍生品组合并进行代币化的加密资产，早期主要合成 DAI、WBTC 等数字资产，后面基于现实世界中的股票、货币、贵金属等的合成资产也越来越多，目前该赛道的龙头项目是 Synthetix，另外还有 Mirror、UMA、Linear、Duet、Coinversation 等项目。DeFi 的利率衍生品主要是基于加密资产利率开发不同类型的衍生产品，以满足 DeFi 用户对确定性收益的不同需求，主要玩家有 BarnBridge、Swivel Finance、Element Finance 等。接着来看看借贷，这也是 TVL 很高的一个版块，和 DEX 一样也是 DeFi 的基石。这块的借贷协议主要有 Compound、Aave、Maker、Cream、Liquity、Venus、Euler、Fuse 等。目前，大部分借贷协议都是采用超额抵押的借贷模型，所谓超额抵押，举个例子，比如，要借出 80 美元的资产，那至少需要存入价值 100 美元的抵押资产，即抵押资产价值要高于借贷资产价值。虽然超额抵押模型是主流，但也存在几个创新方向：无息贷款、资产隔离池、跨链借贷、信用贷。无息贷款的代表为 Liquity，用户在 Liquity 借出其稳定币 LUSD 的时候，用户一次性支付借款和赎回费用，借出后无需支付利息。资产隔离池就是将不同的借贷资产分开为不同的池子，每个借贷池都是独立的，避免一个不良资产或者一个池子受损导致整个平台都被连累。目前，资产隔离池差不多已经成为了标配，很多借贷协议都引入了这种模式，除了一开始就使用这种模式的 Fuse，包括 Compound、Aave、Euler 等协议也都加入了阵营。跨链借贷也是一个新趋势，Flux、Compound、Aave 等都在这个方向上进行拓展。信用贷在传统金融非常普遍，但在 DeFi 领域还比较少，主要是还缺乏有效的链上信用体系，目前的代表项目是 Wing Finance。下一个是聚合器，DeFi 聚合器也分为好几种类型：DEX 聚合器、收益聚合器、资产管理聚合器、信息聚合器。DEX 聚合器，主要就是将多个 DEX 聚合到一起，通过算法从中寻找出最优的交易路径，主流的 DEX 聚合器包括 1inch、Matcha、ParaSwap，以及 MetaMask 钱包内置的 MetaMask Swap 等。收益聚合器主要有 Yearn Finance、Alpha Finance、Harvest Finance、Convex Finance 等，主要就是聚合各种流动性挖矿，让参与多平台的 Yield Farming（收益耕作）实现自动化。资产管理聚合器主要就是监控、跟踪和管理 DeFi 用户的资产和负债，主要以 Zapper 和 Zerion 为代表。最后是信息聚合器，主要包括 CoinMarketCap、DeFiPulse、DeBank、DeFiPrime 等平台。另外，这些其实都是中心化数据平台，但其在 DeFi 生态里依然扮演了重要角色，DeFi 生态里并非全都是去中心化的应用。然后，再简单聊聊保险。我们知道，保险在传统金融中是非常大的一块市场，但 DeFi 里的保险发展至今，却是非常缓慢。整个 Web3 行业里，各种风险很多，协议漏洞风险、项目跑路风险、监管风险等，所以实际上对 DeFi 保险的需求市场本身很大，但因为开发设计门槛高，且流动性比较低，所以才导致整个保险赛道发展缓慢，目前依然处于非常早期的阶段，Nexus Mutual、Cover、Unslashed、Opium 等项目是该领域主要的玩家。然后，再看看预测市场。预测市场是依托数据的市场，可用于押注和预测未来的所有事件，也是以太坊生态最早出现的应用场景之一，并在 2020 年美国大选中迎来爆发式增长，主要项目有 PolyMarket、Augur、Omen 等。最后就是指数板块，提供一揽子资产敞口的指数基金在 DeFi 领域逐渐兴起。但广为人知的指数其实并不多，主要有：DPI、sDEFI、PIPT、DEFI++。DPI 全称为 DeFi Pulse Index，是由 DeFi Pulse 和 Set Protocol 合作创建的，是一种市值加权指数，包含了一些主流 DeFi 协议代币作为基础资产，包括 Uniswap、Aave、Maker、Synthetix、Loopring、Compound、Sushi 等。DPI 可以赎回为一揽子基础资产。sDEFI 则是由 Synthetix 所推出的指数代币，是该领域历史最悠久的指数。sDEFI 是一种合成资产，它不持有任何基础代币，而是使用预言机喂价来跟踪代币价值。PIPT 全称为 Power Index Pool Token，是由 PowerPool 所发行，由 8 种代币资产所组成。PowerPool 发行的指数除了 PIPT，另外还有 Yearn Lazy Ape Index、Yearn Ecosystem Token Index 和 ASSY Index 三个指数。DEFI++ 则是由 PieDAO 所发行，其组成有 14 种资产。PieDAO 还发行了 BCP 和 PLAY，BCP 由 WBTC、WETH、DEFI++ 三种代币组成，PLAY 则由一些元宇宙项目的代币所组成。3.3 GameFiGameFi 从字面上理解就是 Game Finance，是游戏和金融的融合体，也是目前 Web3 游戏的代名词。GameFi 这个词语诞生之前，Web3 游戏则通常被称为区块链游戏，或简称链游。CryptoKitties 是第一款广为人知的区块链游戏，这是一款虚拟养猫的养成类游戏，每一只猫咪都是一个独立的 NFT。初代猫咪总共有 50000 只，每只猫咪都有不同的属性。玩家购买猫咪 NFT 后，就可以开始玩繁殖小猫的游戏。生出来的小猫咪，有部分基因属性会遗传自上一代，而有些基因则随机生成。生出来的猫咪本质上就是新的 NFT，可以卖出变现。如果生成的新猫咪产生了稀有的基因属性，还可以卖到不错的价格。截止撰文之日（2023 年 1 月底），已经产生了 2,021,774 只猫咪，持有的钱包地址有 136,283。继 CryptoKitties 之后，越来越多养成类游戏陆续出现，如加密狗、加密兔、加密青蛙等等。打破这种局面的是一款叫 Fomo3D 的游戏，这是一款公开、透明、去中心化的博彩资金盘游戏。游戏规则也简单，用户通过支付 ETH 购买 Key 参与游戏，用户支付的 ETH 会分配到奖池、分红池、空投池、官方池等。拥有 Key 则可以得到持续的分红，拥有的 Key 越多，则得到的分红会越多。且每轮游戏存在一个倒计时（24 小时），倒计时结束时，最后一个购买 Key 的玩家可以获得奖池里大部分的 ETH。但每次有用户购买 Key，则倒计时剩余时间会增加 30 秒。第一轮游戏持续了很久时间，最后被人用技术手段赢走了奖池。Fomo3D 爆火之后，也是各种优化升级版的同类游戏不断出现，但事实证明，这类游戏还是无法持久。而之后，再次引爆市场的游戏则是 Axie Infinity，国内则被称为“阿蟹”（与 Axie 谐音）。这是一款结合了宝可梦和加密猫玩法的游戏，游戏里的 Axies 可以升级、繁殖、对战、交易等。与加密猫等游戏不同的是，Axie Infinity 的经济系统里还引入了 SLP 和 AXS 代币，玩家可通过战斗赢取 SLP 代币，而通过消耗 SLP 和 AXS 可以繁殖新的 Axies，赢取的 SLP 代币和繁殖出来的 Axies 都可以在市场上出售来赚取收入。不过，Axie Infinity 其实在 2018 年就已经问世，但直到 2021 年才开始爆红，让其爆红的主要原因在于它的 Play-To-Earn 模式被推广开来了，即边玩边赚的特性呈病毒式传播了。其赚钱路径主要是先投入成本购买 Axies，然后通过玩游戏赚 SLP 代币和繁殖新的 Axies，再把 SLP 代币和 Axies 出售换成 ETH 或稳定币，最终将 ETH 或稳定币换成法币。这种赚钱模式一开始是从菲律宾逐渐流行起来的，当时，新冠疫情爆发，菲律宾当地许多人陷入了无收入的困境，而 Axie Infinity 的边玩边赚特性让这些人看到了希望。而且，这种赚钱模式也吸引了众多打金工作室，且逐渐从菲律宾扩展到了印度、印度尼西亚、巴西、中国等。截止撰文之日，日活用户已达 280 万。而现在，边玩边赚模式几乎成为了 Web3 游戏的标配。其他比较知名的游戏还有 Decentraland、The Sandbox、Illuvium、Star Atlas、Alien Worlds 等。这些就不展开说了，感兴趣的可以自行去搜索了解。3.4 SocialFiSocialFi 顾名思义就是 Social Finance，是社交和金融在 Web3 领域的有机结合，其实就是去中心化社交，是近两年才开始流行的概念。目前，在这个赛道的知名项目还比较少，目前的龙头是 Lens Protocol。Lens Protocol 是由 Aave 团队所开发的，在 2022 年 5 月上线。它不是一个独立的社交应用，也不是一个带有前端的完整社交产品，而是提供了一系列模块化组件的社交图谱平台，而具体的应用产品可以采用这些组件去构建。所以，Lens 的定义其实是 Web3 社交应用的基础设施。上线之初就已经拥有了 50 多个生态项目，比较热门的有 Lenster、Lenstube、ORB、Phaver、re:meme、Lensport、Lensta 等。Lenster 是去中心化社交媒体应用，可以通过连接 Web3 钱包并使用 Lens 来登录。登录用户就可以在 Lenster 发布内容，和在微博或推特发布内容类似，不同的是，在 Lenster 上发布内容时可选择收费。也可以评论其他用户的内容，不过目前还不支持层级式的评论。Lenstube 则是去中心化视频平台，可以理解为就是去中心化的 Youtube。ORB 是去中心化职业社交媒体应用，具有端到端链上信誉系统。具体来说，ORB 可以通过将各种 NFT 和 POAP 与用户经验、教育、技能和项目联系起来，从而创建个人去中心化专业档案并建立链上可信度，以及探索工作机会和申请链上身份，还可以用在链上分享自己的想法，与 Web3 人士建立联系并构建社区。此外，ORB 还允许用户利用碎片化时间通过学习 Web3 知识来获取 NFT，即 Learn-to-Earn。Phaver 是一款适用于 iOS 和 Android 的 Share-to-Earn 社交应用，用户可以发布帖子，内容可以是图片、链接、产品应用等。用户还可以浏览 Lens 内的所有内容。Lens Profile 用户连接钱包后，还可以通过 Phaver 直接发布帖子到 Lens 中。re:meme 是一个链上 meme 生成器，允许用户上传 meme 模版，也能选择是否收费，然后其他人可以用图像编辑器添加文本、绘图和补充图像等。:meme 还可以扩展到音乐、视频和学术论文等媒体格式。Lensport 是一个只聚焦于 Lens 协议的社交 NFT 市场，用户可以发现、发布和出售帖子，也可以投资支持创作者。Lensta 是一个聚焦于 Lens 协议的图片流应用，可以浏览 Lens 中带有图片的最新、最热门以及 Lenster、Lensport 等上收集费用最多的帖子。4 访问层访问层是 Web3 组成架构里的最上层，也是直接面向终端用户的入口层。这一层里主要包括钱包、浏览器、聚合器等，另外，有一些 Web2 的社交媒体平台也成为了 Web3 的入口。先来看看钱包，这也是最主要的入口。目前的钱包有多种分类，有浏览器钱包、手机钱包、硬件钱包、多签钱包、MPC 钱包、智能合约钱包等。浏览器钱包就是通过网络浏览器使用的加密钱包，是大部分用户使用最广泛的钱包，最常用的就是 MetaMask、Coinbase Wallet、WalletConnect 等。MetaMask 是最被广泛支持的钱包之一，支持所有的 EVM 链，也已经成为了所有 DApps 的标准，目前支持的浏览器包括 Chrome、Brave、Firefox、Edge，以浏览器插件的方式存在。Coinbase Wallet 顾名思义是由交易所 Coinbase 所发行的钱包，于 2021 年 11 月推出后迅速发展，成为了与 MetaMask 旗鼓相当的对手，但浏览器还只支持 Chrome。WalletConnect 则比较特殊，它并不是一款具体的钱包应用，而是连接 DApps 和钱包的开源协议。最常用的就是用于连接手机钱包，在浏览器上的 DApp 选择连接 WalletConnect，会展示一个二维码，用你的手机钱包扫这个二维码就可以授权你的手机钱包连接上浏览器上的 DApp。而且，WalletConnect 支持所有区块链，不只是 EVM 链，也支持接入所有钱包。另外，不像 MetaMask 和 Coinbase Wallet 需要安装其浏览器插件，WalletConnect 不需要安装浏览器插件，所以可以支持所有浏览器，比如也支持 Safari，而 MetaMask 和 Coinbase Wallet 是不支持 Safari 的。因此，WalletConnect 成为了最受欢迎的钱包，也成为了所有 DApp 接入钱包的标配。手机钱包，即移动端数字资产钱包，很多钱包都支持。MetaMask 和 Coinbase Wallet 也有手机端的钱包 App。另外，比较知名的手机钱包还有 TokenPocket、BitKeep、Rainbow、imToken、Crypto.com 等。大部分流行的手机钱包都支持多链，包括 EVM 链，也包括 Non-EVM 链，比如 TokenPocket 目前支持了 Bitcoin、Ethereum、BSC、TRON、Polygon、Arbitrum、Avalanche、Solana、Cosmos、Polkadot、Aptos 等。硬件钱包则是把数字资产私钥存储在安全的硬件设备中，与互联网隔离，可通过 USB 即插即用。现在使用最广泛的硬件钱包是 Ledger 和 Trezor。Ledger 目前有三款不同型号的硬件钱包：Ledger Stax、Ledger Nano X、Ledger Nano S Plus。Ledger Stax 是在 2023 年才推出的新型号，支持触摸屏，而另外两款则不支持。Trezor 则有两款型号：Trezor Model T 和 Trezor Model One。Model T 支持触摸屏。除了 Ledger 和 Trezor，市面上的硬件钱包还有 SafePal、OneKey、imKey、KeepKey、ColdLar 等。多签钱包，顾名思义，是指需要多人签名才能执行操作的钱包。最知名的多签钱包就是 Gnosis Safe，其本质上是一套链上智能合约，最常用的就是 2/3 签名，即总共有三个用户共同管理钱包，每次执行操作时，需要这三人中至少两个人的签名才能触发链上执行。MPC 全称为 Multi-Party Computation，MPC 钱包也称为多方计算钱包，是新一代钱包类型，通过对私钥进行多方计算在链下实现多签和跨链等复杂的验证方式。简单来说，就是将私钥拆分成多个分片，然后由多方各自存储管理每个分片，签名的时候，再联合多方将分片重新拼接成完整的私钥。MPC 钱包与多签签名很类似，也可以实现 2/3 签名，不同的是，多签钱包是在智能合约层面实现签名校验，而 MPC 钱包则是通过链下计算实现的。目前已提供 MPC 钱包服务的还不多，主要有 ZenGo、Safeheron、Fordefi、OpenBlock、web3auth 等。智能合约钱包就是使用智能合约账户作为地址的钱包，多签钱包 Gnosis Safe 也属于智能合约钱包。而近一两年对智能合约钱包最新的尝试则是结合「账户抽象（Account Abstraction）」的新一代钱包。账户抽象主要是要将签名者和账户分离开来，钱包地址不再与唯一的私钥强绑定，可以实现更换签名者，也可以实现多签，还可以实现更换签名算法。目前在这个赛道的选手除了 Gnosis Safe 还有 UniPass、Argent、Blocto 等。钱包暂时就聊这么多，接着来说说浏览器。很多 DApp 都还是只提供了网页版本的前端，所以浏览器就成为了重要的访问入口。但因为不是所有浏览器都支持钱包扩展插件，所以也不是所有浏览器都能成为很好的 Web3 入口。最常用的浏览器是 Chrome，所有浏览器钱包都会开发 Chrome 的钱包插件。而 Safari 则很少用做 Web3 DApp 入口，因为除了 WalletConnect，没有其他浏览器钱包能够支持。还有一个值得介绍的浏览器是 Brave，这是一款内置了钱包的浏览器，其内置钱包叫 Brave Wallet。有一些聚合器也是 Web3 的访问入口，比如 DappRadar 收集了各种 DApps，用户可以通过它浏览并连接到这些 DApps。还有 Zapper、DeBank、Zerion 之类的聚合器，可以帮助用户追踪他们在各种 Web3 应用的所有资产和操作记录。最后，像 Twitter 和 Reddit 这类 Web3 的社交媒体平台，因为聚集了很多 Web3 社群，也逐渐变成了 Web3 的访问入口。"
  },
  
  {
    "title": "Layer 2 概览",
    "url": "/posts/layer2/",
    "categories": "Blockchain, Layer 2",
    "tags": "rollup, channel",
    "date": "2022-11-28 12:00:00 +0800",
    





    
    "snippet": "区块链技术自诞生以来就始终面临着可扩展性问题的挑战。根据以太坊创始人Vitalik Buterin提出的“区块链不可能三角”，区块链无法同时兼顾去中心化、安全性和可扩展性三个维度。其中，去中心化是区块链技术的基本性质，同时，区块链天然的强金融属性也使得安全性必不可少。因此，研究人员和开发者一直致力于解决区块链的可扩展性问题，这一瓶颈导致了链上交易成本居高不下、执行速度慢，难以满足当前日益增长...",
    "content": "区块链技术自诞生以来就始终面临着可扩展性问题的挑战。根据以太坊创始人Vitalik Buterin提出的“区块链不可能三角”，区块链无法同时兼顾去中心化、安全性和可扩展性三个维度。其中，去中心化是区块链技术的基本性质，同时，区块链天然的强金融属性也使得安全性必不可少。因此，研究人员和开发者一直致力于解决区块链的可扩展性问题，这一瓶颈导致了链上交易成本居高不下、执行速度慢，难以满足当前日益增长的应用需求，已成为了区块链技术主流化的绊脚石。在众多的可扩展性解决方案中，Layer 2 扩展方案获得了较多的关注。Layer 2 是指以扩展底层区块链网络为目的，基于特定区块链（又称“Layer 1”）所构建的链下网络或系统。Layer 2 可以大幅提升底层区块链的吞吐量以及其他性能。Layer 2 方案的核心思想在于层级分离的概念。区块链之所以在可扩展性方面受限，本质上是由于区块链主网承担的任务过多。当前区块链网络主要有三个核心功能：交易执行、数据可用以及共识生成。  交易执行：交易的处理和吞吐。由区块链每秒可完成的计算次数衡量。  数据可用：满足网络中的节点和验证者对交易、状态及其他数据的存储需求。由标准存储单位衡量，如MB、GB等。  共识生成：保证节点和验证者就网络状态和交易排序达成共识。由去中心化水平和最终确定时间（所有节点就特定状态变更达成一致所需的时间）衡量。层级分离的目标是将 Layer 1 作为基础层，通过特定机制继承主链共识所保证的安全性和去中心化特性，同时将复杂的快速迭代和计算操作从中剥离出来，转移到 Layer 2 上执行，从而在无需牺牲去中心化水平和安全性的同时快速执行交易并实现可扩展性。依此标准，侧链方案不属于 Layer 2，因为侧链会部署自身专有的共识算法及验证节点，其安全机制独立于底层区块链，未实现安全性的继承。1 Layer 2 原理概述Layer 2 解决方案大致可以分为两个主要部分：  链下网络：负责实际的交易处理；  链上合约：负责解决分歧，并将 Layer 2 网络达成的共识传输到底层区块链进行验证。Layer 2 网络可以快速进行计算、执行交易。不同网络会通过完全不同的方式提升交易吞吐量，其共同点在于交易结算时，会向底层区块链提交某种可验证的密码学证明，以证明状态变更的有效性，根据具体方案的不同，该证明可以是先行性的，也可以是追溯性的。此外，不同 Layer 2 网络在底层区块链上的智能合约实现也有所不同，但其核心功能一致：  保存和释放转账至 Layer 2 的资金；  收到 Layer 2 提交的证明并进行验证，解决分歧，并最终确认交易。以下将探讨两种主流 Layer 2 实现方案：支付通道和Rollup（卷叠）。2 支付通道两个或以上用户可以预先向通道中锁定资金，然后在链下进行通证转账。如图所示，A 和 B 分别在智能合约中锁定一笔资金，创建支付通道，并通过加密签名技术约定双方可以使用的资金量。支付通道建立后，A 和 B 都可以通过签名消息在链下进行交易。A 和 B 可以相互转账，转账无延时、无成本。在双向支付通道中，A、B 之间的交易不会发送到底层区块链，只有当双方都决定关闭通道时，最终交易结果才会被提交至链上进行结算。因此，A 和 B 最终仅需支付创建和关闭支付通道时的两笔链上交易费即可。使用支付通道，可以在极短的时间内处理大量、频繁的转账交易，且无需任何成本，是解决区块链扩容问题的经典案例。3 RollupRollup 在链下执行执行智能合约状态变更，然后将数据发布到 Layer 1 进行验证，以此提升区块链的吞吐量并降低成本。由于交易数据包含在 Layer 1 的区块中，因而 Rollup 可被以太坊原生的安全机制保护。Rollup 使用部署在 Layer 1 上的智能合约来管理 Layer 2 与 Layer 1 之间的交互。在进入 Rollup 前，用户必须将资金存入智能合约中，以在 Rollup 上解锁等额的资金。作为第三方的 sequencer 收到用户在 Rollup 合约中存款的证明后，将相应资金存入用户的 Rollup 账户，随后用户即可在 Rollup 上自由进行交易。3.1 扩容方式Rollup 主要通过以下三种方式实现扩容：  Rollup 在链下执行交易，底层区块链只需计算轻量化的交易证明，验证网络活动并储存原始交易数据。  Rollup 将交易数据打包提交至区块链，因此链上gas费可以分摊到多个交易中。  Rollup 最少仅需要一个诚实的验证者，就可以向底层区块链证明交易的有效性，因此较少的验证节点数和较高的硬件要求不会对安全性有很大影响。3.2 链下执行交易Rollup 的一个核心特性就是在链下执行交易，这使得 Layer 2 网络可以代替底层区块链处理与其他用户或智能合约的交易。由于验证节点数量较少且硬件更强大，因此 Layer 2 网络的交易吞吐量相较于底层链可得到大幅提升。底层区块链只需要计算提交至智能合约的证明，并将未执行的原始交易数据作为calldata储存起来，即可验证 Layer 2 网络中的活动。简而言之，区块链需要进行的计算任务大幅减少，同时链上数据的存储成本也大幅降低，因此交易成本变得极低。3.3 打包交易Rollup 将原始交易数据打包成calldata。与链上交易不同，Rollup 交易不需要以同样的方式进行验证；交易数据打包仅仅是一种在底层区块链上存储数据的方式，以便验证者或 Rollup 参与者在需要时可以随时重建 Layer 2 网络的状态。然而，核心逻辑是相同的，即：主链上的一个打包交易中存储了多笔相关的 Rollup 交易数据。此方法可以有效降低影响区块链可扩展性的交易费用，而结合数据压缩还能进一步降低交易执行成本。3.4 减少验证者如前所述，Layer 2 可以继承底层区块链的安全性和去中心化水平（后文将详细阐述这一点，其关键在于 Layer 2 必须向底层区块链提交某种证明，以保证其提出的状态变更的有效性），这一特性使得 Rollup 可以减少网络中负责处理交易的验证者数量——因为 Layer 2 中的验证者不需要生成共识，共识是在 Rollup 提交的证明经验证后由 Layer 1 达成的。验证者数量变少，同时验证者通常拥有性能更强的硬件，因而可以用更快的速度和更低的成本来计算交易。3.5 结构及运行机制在 Rollup 上，用户签署交易并将其提交给负责排序和执行交易的 sequencer 。交易被排序后，将按顺序逐个通过状态转换函数进行处理。状态转换函数将当前链的状态（账户余额、合约代码等）与下一个交易作为输入，更新状态。状态转换函数是确定性的，即其行为仅取决于当前状态和下一个交易的内容，与其他因素无关。这种确定性，使得交易T的结果仅取决于链的初始状态、序列中T之前的交易以及T本身。因此，任何获取了交易序列的参与方都可以自行计算状态转换函数，而所有诚实参与方的计算都将得到相同的结果。这就是 L2 节点正常运行的过程：获取交易序列，并在本地运行状态转换函数。该过程无需共识机制的参与。那么节点如何获取序列呢？sequencer 会定期将序列中的一组交易打包并进行压缩，将结果作为calldata发布到 L1 链上（calldata 是智能合约中不可修改、非持久的区域，其行为与内存非常相似。 而 calldata 作为区块链的历史日志部分，不会涉及以太坊的状态，因此在链上存储数据的费用较低）。一旦交易在以太坊上最终确定，则记录在其中的 Layer 2 交易也最终确定（即交易的顺序和结果是确定性的，且对任意一方都可知）。Rollup合约中存储着“状态根（state root）”，用于验证Rollup在不同时间点的状态。当向 Rollup 合约提交一批交易时，必须包括一个“前状态根（pre-state root）”和一个“后状态根（post-state root）”。  前状态根：旧的状态根，描述在执行提交的交易之前 Rollup 的状态。  后状态根：新的状态根，描述执行了提交的交易之后 Rollup 的状态。一旦 sequencer 提交了打包的交易，合约将验证前状态根是否与现有状态根匹配。如果两者匹配，合约会丢弃旧的状态根，并存储 sequencer 提议的新状态根。这意味着后状态根中引用的交易已经最终化，无法被撤销。4 Layer 2 安全性的关键——证明“证明”对于Layer 2 来说至关重要，因为 Layer 2 需要通过证明来继承底层区块链的安全性。所有 Layer 2 网络都需要提供某种形式的加密证明，以解决底层区块链上的分歧。上文提到的支付通道，对交易附上加密签名就可以为智能合约提供确定性的权威事实，以解决分歧。Rollup 中主流的两种证明方式是错误性证明（Fault Proof）以及有效性证明（Validity Proof，即“零知识证明”），这两种证明分别是 Optimistic Rollup 和 ZK-Rollup 的底层技术。4.1 支付通道签名消息支付通道的一个关键设计要素是交易必须由加密签名签署。这就提供了一种与底层智能合约兼容的证据形式，可用于解决分歧。例如，在关闭支付通道时，如果 Alice 通过向智能合约提交过时的支付通道“账本”，试图占用不属于自己的资金，则 Bob 可以对结算交易提出异议，并提交最新的账本副本。智能合约就可以确定正确的金额。Alice还会因恶意行为被罚款。底层智能合约的裁决过程是使支付通道成为 Layer 2 的关键，最简单的评判方法就是看是否需要提供“证明”。一旦支付通道打开，Bob 和 Alice 必须对自己进行的每笔交易进行加密签名，并存储对方的签名副本，这是支付通道内活动的证明。但仅有证明是不够的，还需要对事实进行执行，这与现实生活中的审判过程很相似，在证据充分且陪审团给出审议结果后，法官将作出裁决。在上述例子中，智能合约会执行裁定结果，将正确的金额返还到各方在底层链上的钱包中进行结算。这就是前面提到的“继承底层区块链的去中心化水平和安全性保证”的含义。支付通道将大部分计算和执行都放到链下处理，但在结算时，还是会利用底层区块链强大的共识层来作出最终裁决。4.2 Optimistic Rollup 错误性证明错误性证明（Fault Proof），又称欺诈证明（Fraud Proof）是一种主张，声称某个状态转换（交易）是无效的，并且整个交易批次应该因此被撤销。这也是目前 Optimistic Rollup 所采用的机制。“Optimistic”即“乐观的”，代表了其证明思想的核心特点：智能合约会“乐观地”默认所有交易都是有效的，除非其被证明无效（无罪推定），网络会设置一个挑战期，期间任何参与者都可以对交易发起挑战，并向智能合约提交证明，表明交易数据或状态变更是错误的，从而撤销错误交易。具体而言，欺诈证明主要依赖于先前提到的状态根来发挥作用，整个过程始于验证者检测到 L1 链上状态根中引用的 Rollup 状态与 Rollup 链的实际状态不匹配，例如，sequencer 提交了一个后状态根，在 Alice 未进行过转账的情况下，将Alice的余额减少了 5 ETH，同时将Bob的余额增加了 5 ETH。由于验证者会下载所有交易数据，将其应用到本地的 Rollup 状态副本中，并计算后状态根，因而验证者有能力对状态转换提出挑战，并证明其无效性——若 sequencer 的后状态根与验证者的相匹配，说明一切正常；若 sequencer 的后状态根不同（很可能是因为其中包含了虚假交易，如上例所示），则验证者可以触发欺诈证明计算。欺诈证明计算的过程如下：  验证者发起挑战并提供以下信息：          有争议的状态转换      前状态根      Rollup 状态数据      验证者计算出的后状态根            在 L1 链上的一个沙盒环境中使用了挑战者所提供的信息和其他相关数据重新执行交易。此“沙盒环境”是在 L1 上运行的一个智能合约，兼作虚拟机。    如果计算结果得到的后状态根与挑战者的匹配，说明 sequencer 确实发布了带有无效状态转换的交易批次。随后发布的无效交易批次将被撤销，将 Rollup 恢复到先前的状态。4.3 ZK-Rollup 有效性证明有效性证明是一种交易验证方法，涉及到一个证明者（prover）和一个验证者（verifier）。证明者负责为某些信息创建证明，验证者则在双方没有共享信息的情况下验证该证明的有效性。证明者和验证者之间共享的信息称为见证（witness），通常是一个复杂数学方程的解。简而言之，证明者需要证明其准确地知道见证，而验证者需要可靠地确定证明者是否具有该见证的知识。有效性证明方案的核心思想与错误性证明恰恰相反，默认所有计算结果都是存疑的，必须先被证明才能接受。有效性证明允许在不揭示某事件本身的情况下证明该事件的正确性，因此也被称为零知识证明。对 ZK-Rollup 来说，要证明的是 Layer 2 网络中离链状态转换的正确性，得益于零知识证明的特性，ZK-Rollup 发送有效性证明后，Layer 1 无需重新执行交易即可验证交易执行的正确性。对于验证者来说，功能完备的 ZK-Rollup 将不允许任何错误的交易在底层链上结算，因为每一批交易都必须附上相应的有效性证明。当前的有效性证明可以采用ZK-SNARK（零知识精简非交互式知识证明）或ZK-STARK（零知识可扩展透明知识证明）的形式。4.4 有效性证明 vs. 错误性证明有效性证明和欺诈证明有许多不同之处，包括所需的计算量、验证交易所需的时间、安全性及实现难度等。      有效性证明比欺诈证明的计算复杂度更高    零知识证明在数学上具有挑战性，真正的零知识验证过程通常需要大量计算。相较之下，欺诈证明每交易批次成本更低，约为 40,000 gas，而 ZK-SNARK证明需要 500,000 gas。但就链上来说，ZK-SNARK 更便宜，因为 Optimistic Rollup 需要将所有数据发布到底层链，以便欺诈证明来进行验证。        有效性证明是即时的，而欺诈证明有争议时间延迟    零知识证明和欺诈证明之间的一个主要区别在于，零知识证明可以立即验证交易的有效性，而欺诈证明则需要一个固有的争议时间延迟（DTD），从而降低了交易的速度。只有在没有验证者对某个交易批次提交欺诈证明时，该批次才会被真正发布到 L1 网络上。        有效性证明更加去中心化和数据安全    由于零知识证明只需要最小限度的交互（例如，仅将证明共享给验证者），并且可以安全地完成此过程，因此零知识证明具有很高的去中心化性质。此外，由于证明是零知识且完全准确的，有效性证明可以保护用户隐私，并将严重威胁（如对L1链的51%攻击）的风险降到最低。    而 Optimistic Rollup 需要验证者提交欺诈证明，而这些证明可能会被截取或遭到分布式拒绝服务（DDoS）攻击，从而阻止对交易状态进行挑战，并使其无法传递到批次处理过程中。        有效性证明更难实现    Optimistic Rollup 和相应的欺诈证明可以在任何EVM或 Optimistic 虚拟机（OVM）上运行，而零知识证明及 ZK-Rollup 需要专门的虚拟机，即零知识以太坊虚拟机（zkEVM）。这类虚拟机能以与 ZK 计算和密码学有效性证明相适应的方式执行智能合约。    直到最近，zkEVM 还被认为仅在理论上可行。即使到现在，也很难实现支持零知识证明的 EVM 封装。不过 ZK 领域的新发展为零知识证明的吞吐量和可扩展性提供了改进的可能。    最值得注意的是，Polygon 正在创建自己的 Rollup 技术，该技术使用递归 ZK-SNARK 在返回主链之前更有效地执行链下交易。递归证明的工作原理是一次证明多个交易块，然后再证明一个聚合块。从本质上讲，是 SNARK 证明了其他 SNARK ，最后生成一笔最终交易，甚至比多个打包交易更加高效。Polygon 的 Polygon Zero 使用 Plonky2 来创建递归证明，这是目前可用的最快的证明方案。  5 区块链扩容方案支付通道、Rollup和Layer 2都是区块链扩容方案，具有长期发展的潜力，可以为Web3应用的发展提供支持，并提升用户体验。目前，Layer 1和Layer 2等大多数Web3技术还处于起步阶段，仍需要不断发展。这些技术尚未达到爆发点，仍需要经过一系列实践验证才能证明自身的价值。开发者和研究人员也正在推动DAG和Layer 2解决方案，为扩展区块链生态系统，实现Web3主流应用的目标而不断努力。"
  },
  
  {
    "title": "Raft 共识算法详解",
    "url": "/posts/raft/",
    "categories": "Consensus",
    "tags": "raft, consensus",
    "date": "2022-11-12 12:00:00 +0800",
    





    
    "snippet": "本文主要结合 Raft 论文讲解 Raft 算法思路，并遵循 Raft 的模块化思想对难理解及容易误解的内容抽丝剥茧。算法方面包括：选主机制、基于日志实现状态机机制、安全正确维护状态机机制；工程实现方面包括：集群成员变更防脑裂策略、解决数据膨胀及快速恢复状态机策略、线性一致读性能优化策略等。1 概述1.1 Raft 是什么？  Raft is a consensus algorithm fo...",
    "content": "本文主要结合 Raft 论文讲解 Raft 算法思路，并遵循 Raft 的模块化思想对难理解及容易误解的内容抽丝剥茧。算法方面包括：选主机制、基于日志实现状态机机制、安全正确维护状态机机制；工程实现方面包括：集群成员变更防脑裂策略、解决数据膨胀及快速恢复状态机策略、线性一致读性能优化策略等。1 概述1.1 Raft 是什么？  Raft is a consensus algorithm for managing a replicated log. It produces a result equivalent to (multi-)Paxos, and it is as efficient as Paxos, but its structure is different from Paxos; this makes Raft more understandable than Paxos and also provides a better foundation for building practical systems.  –《In Search of an Understandable Consensus Algorithm》在分布式系统中，为了消除单点故障，提高系统可用性，通常会使用副本来进行容错，但这会带来另一个问题，即如何保证多个副本之间的一致性？  这里我们只讨论强一致性，即线性一致性。弱一致性涵盖的范围较广，涉及根据实际场景进行诸多取舍，不在 Raft 系列的讨论目标范围内。所谓的强一致性（线性一致性）并不是指集群中所有节点在任一时刻的状态必须完全一致，而是指一个目标，即让一个分布式系统看起来只有一个数据副本，并且读写操作都是原子的，这样应用层就可以忽略系统底层多个数据副本间的同步问题。也就是说，我们可以将一个强一致性分布式系统当成一个整体，一旦某个客户端成功的执行了写操作，那么所有客户端都一定能读出刚刚写入的值。即使发生网络分区故障，或者少部分节点发生异常，整个集群依然能够像单机一样提供服务。共识算法（Consensus Algorithm）就是用来做这个事情的，它保证即使在小部分（≤ (N-1)/2）节点故障的情况下，系统仍然能正常对外提供服务。共识算法通常基于状态复制机（Replicated State Machine）模型，也就是所有节点从同一个 state 出发，经过同样的操作 log，最终达到一致的 state。图：Replicated State Machine共识算法是构建强一致性分布式系统的基石，Paxos 是共识算法的代表，而 Raft 则是其作者在博士期间研究 Paxos 时提出的一个变种，主要优点是容易理解、易于实现，甚至关键的部分都在论文中给出了伪代码实现。1.2 谁在使用 Raft采用 Raft 的系统最著名的当属 etcd 了，可以认为 etcd 的核心就是 Raft 算法的实现。作为一个分布式 kv 系统，etcd 使用 Raft 在多节点间进行数据同步，每个节点都拥有全量的状态机数据。我们在学习了 Raft 以后将会深刻理解为什么 etcd 不适合大数据量的存储（for the most critical data）、为什么集群节点数不是越多越好、为什么集群适合部署奇数个节点等问题。作为一个微服务基础设施，consul 底层使用 Raft 来保证 consul server 之间的数据一致性。在阅读完第六章后，我们会理解为什么 consul 提供了 default、consistent、stale 三种一致性模式（Consistency Modes）、它们各自适用的场景，以及 consul 底层是如何通过改变 Raft 读模型来支撑这些不同的一致性模式的。TiKV 同样在底层使用了 Raft 算法。虽然都自称是“分布式 kv 存储”，但 TiKV 的使用场景与 etcd 存在区别。其目标是支持 100TB+ 的数据，类似 etcd 的单 Raft 集群肯定无法支撑这个数据量。因此 TiKV 底层使用 Multi Raft，将数据划分为多个 region，每个 region 其实还是一个标准的 Raft 集群，对每个分区的数据实现了多副本高可用。目前 Raft 在工业界已经开始大放异彩，对于其各类应用场景这里不再赘述，感兴趣的读者可以参考 这里，下方有列出各种语言的大量 Raft 实现。1.3 Raft 基本概念Raft 使用 Quorum 机制来实现共识和容错，我们将对 Raft 集群的操作称为提案，每当发起一个提案，必须得到大多数（&gt; N/2）节点的同意才能提交。  这里的“提案”我们可以先狭义地理解为对集群的读写操作，“提交”理解为操作成功。那么当我们向 Raft 集群发起一系列读写操作时，集群内部究竟发生了什么呢？我们先来概览式地做一个整体了解，接下来再分章节详细介绍每个部分。首先，Raft 集群必须存在一个主节点（leader），我们作为客户端向集群发起的所有操作都必须经由主节点处理。所以 Raft 核心算法中的第一部分就是选主（Leader election）——没有主节点集群就无法工作，先票选出一个主节点，再考虑其它事情。其次，主节点需要承载什么工作呢？它会负责接收客户端发过来的操作请求，将操作包装为日志同步给其它节点，在保证大部分节点都同步了本次操作后，就可以安全地给客户端回应响应了。这一部分工作在 Raft 核心算法中叫日志复制（Log replication）。然后，因为主节点的责任是如此之大，所以节点们在选主的时候一定要谨慎，只有符合条件的节点才可以当选主节点。此外主节点在处理操作日志的时候也一定要谨慎，为了保证集群对外展现的一致性，不可以覆盖或删除前任主节点已经处理成功的操作日志。所谓的“谨慎处理”，其实就是在选主和提交日志的时候进行一些限制，这一部分在 Raft 核心算法中叫安全性（Safety）。Raft 核心算法其实就是由这三个子问题组成的：选主（Leader election）、日志复制（Log replication）、安全性（Safety）。这三部分共同实现了 Raft 核心的共识和容错机制。除了核心算法外，Raft 也提供了几个工程实践中必须面对问题的解决方案。第一个是关于日志无限增长的问题。Raft 将操作包装成为了日志，集群每个节点都维护了一个不断增长的日志序列，状态机只有通过重放日志序列来得到。但由于这个日志序列可能会随着时间流逝不断增长，因此我们必须有一些办法来避免无休止的磁盘占用和过久的日志重放。这一部分叫日志压缩（Log compaction）。第二个是关于集群成员变更的问题。一个 Raft 集群不太可能永远是固定几个节点，总有扩缩容的需求，或是节点宕机需要替换的时候。直接更换集群成员可能会导致严重的脑裂问题。Raft 给出了一种安全变更集群成员的方式。这一部分叫集群成员变更（Cluster membership change）。此外，我们还会额外讨论线性一致性的定义、为什么 Raft 不能与线性一致划等号、如何基于 Raft 实现线性一致，以及在如何保证线性一致的前提下进行读性能优化。以上便是理论篇内将会讨论到的大部分内容的概要介绍，这里我们对 Raft 已经有了一个宏观上的认识，知道了各个部分大概是什么内容，以及它们之间的关系。接下来我们将会详细讨论 Raft 算法的每个部分。让我们先从第一部分选主开始。2 选主2.1 什么是选主选主（Leader election）就是在分布式系统内抉择出一个主节点来负责一些特定的工作。在执行了选主过程后，集群中每个节点都会识别出一个特定的、唯一的节点作为 leader。我们开发的系统如果遇到选主的需求，通常会直接基于 zookeeper 或 etcd 来做，把这部分的复杂性收敛到第三方系统。然而作为 etcd 基础的 Raft 自身也存在“选主”的概念，这是两个层面的事情：基于 etcd 的选主指的是利用第三方 etcd 让集群对谁做主节点的决策达成一致，技术上来说利用的是 etcd 的一致性状态机、lease 以及 watch 机制，这个事情也可以改用单节点的 MySQL/Redis 来做，只是无法获得高可用性；而 Raft 本身的选主则指的是在 Raft 集群自身内部通过票选、心跳等机制来协调出一个大多数节点认可的主节点作为集群的 leader 去协调所有决策。当你的系统利用 etcd 来写入谁是主节点的时候，这个决策也在 etcd 内部被它自己集群选出的主节点处理并同步给其它节点。2.2 Raft 为什么要进行选主？按照论文所述，原生的 Paxos 算法使用了一种点对点（peer-to-peer）的方式，所有节点地位是平等的。在理想情况下，算法的目的是制定一个决策，这对于简化的模型比较有意义。但在工业界很少会有系统会使用这种方式，当有一系列的决策需要被制定的时候，先选出一个 leader 节点然后让它去协调所有的决策，这样算法会更加简单快速。此外，和其它一致性算法相比，Raft 赋予了 leader 节点更强的领导力，称之为 Strong Leader。比如说日志条目只能从 leader 节点发送给其它节点而不能反着来，这种方式简化了日志复制的逻辑，使 Raft 变得更加简单易懂。2.3 Raft 选主过程2.3.1 节点角色Raft 集群中每个节点都处于以下三种角色之一：  Leader: 所有请求的处理者，接收客户端发起的操作请求，写入本地日志后同步至集群其它节点。  Follower: 请求的被动更新者，从 leader 接收更新请求，写入本地文件。如果客户端的操作请求发送给了 follower，会首先由 follower 重定向给 leader。  Candidate: 如果 follower 在一定时间内没有收到 leader 的心跳，则判断 leader 可能已经故障，此时启动 leader election 过程，本节点切换为 candidate 直到选主结束。2.3.2 任期每开始一次新的选举，称为一个任期（term），每个 term 都有一个严格递增的整数与之关联。每当 candidate 触发 leader election 时都会增加 term，如果一个 candidate 赢得选举，他将在本 term 中担任 leader 的角色。但并不是每个 term 都一定对应一个 leader，有时候某个 term 内会由于选举超时导致选不出 leader，这时 candicate 会递增 term 号并开始新一轮选举。Term 更像是一个逻辑时钟（logic clock）的作用，有了它，就可以发现哪些节点的状态已经过期。每一个节点都保存一个 current term，在通信时带上这个 term 号。节点间通过 RPC 来通信，主要有两类 RPC 请求：  RequestVote RPCs: 用于 candidate 拉票选举。  AppendEntries RPCs: 用于 leader 向其它节点复制日志以及同步心跳。2.3.3 节点状态转换我们知道集群每个节点的状态都只能是 leader、follower 或 candidate，那么节点什么时候会处于哪种状态呢？下图展示了一个节点可能发生的状态转换：接下来我们详细讨论下每个转换所发生的场景。2.3.3.1 Follower 状态转换过程Raft 的选主基于一种心跳机制，集群中每个节点刚启动时都是 follower 身份（Step: starts up），leader 会周期性的向所有节点发送心跳包来维持自己的权威，那么首个 leader 是如何被选举出来的呢？方法是如果一个 follower 在一段时间内没有收到任何心跳，也就是选举超时，那么它就会主观认为系统中没有可用的 leader，并发起新的选举（Step: times out, starts election）。这里有一个问题，即这个“选举超时时间”该如何制定？如果所有节点在同一时刻启动，经过同样的超时时间后同时发起选举，整个集群会变得低效不堪，极端情况下甚至会一直选不出一个主节点。Raft 巧妙的使用了一个随机化的定时器，让每个节点的“超时时间”在一定范围内随机生成，这样就大大的降低了多个节点同时发起选举的可能性。图：一个五节点 Raft 集群的初始状态，所有节点都是 follower 身份，term 为 1，且每个节点的选举超时定时器不同若 follower 想发起一次选举，follower 需要先增加自己的当前 term，并将身份切换为 candidate。然后它会向集群其它节点发送“请给自己投票”的消息（RequestVote RPC）。图：S1 率先超时，变为 candidate，term + 1，并向其它节点发出拉票请求2.3.3.2 Candicate 状态转换过程Follower 切换为 candidate 并向集群其他节点发送“请给自己投票”的消息后，接下来会有三种可能的结果，也即上面节点状态图中 candidate 状态向外伸出的三条线。1. 选举成功（Step: receives votes from majority of servers）当candicate从整个集群的大多数（N/2+1）节点获得了针对同一 term 的选票时，它就赢得了这次选举，立刻将自己的身份转变为 leader 并开始向其它节点发送心跳来维持自己的权威。图：“大部分”节点都给了 S1 选票图：S1 变为 leader，开始发送心跳维持权威每个节点针对每个 term 只能投出一张票，并且按照先到先得的原则。这个规则确保只有一个 candidate 会成为 leader。2. 选举失败（Step: discovers current leader or new term）Candidate 在等待投票回复的时候，可能会突然收到其它自称是 leader 的节点发送的心跳包，如果这个心跳包里携带的 term 不小于 candidate 当前的 term，那么 candidate 会承认这个 leader，并将身份切回 follower。这说明其它节点已经成功赢得了选举，我们只需立刻跟随即可。但如果心跳包中的 term 比自己小，candidate 会拒绝这次请求并保持选举状态。图：S4、S2 依次开始选举图：S4 成为 leader，S2 在收到 S4 的心跳包后，由于 term 不小于自己当前的 term，因此会立刻切为 follower 跟随 S43. 选举超时（Step: times out, new election）第三种可能的结果是 candidate 既没有赢也没有输。如果有多个 follower 同时成为 candidate，选票是可能被瓜分的，如果没有任何一个 candidate 能得到大多数节点的支持，那么每一个 candidate 都会超时。此时 candidate 需要增加自己的 term，然后发起新一轮选举。如果这里不做一些特殊处理，选票可能会一直被瓜分，导致选不出 leader 来。这里的“特殊处理”指的就是前文所述的随机化选举超时时间。图：S1 ~ S5 都在参与选举图：没有任何节点愿意给他人投票图：如果没有随机化超时时间，所有节点将会继续同时发起选举……以上便是 candidate 三种可能的选举结果。2.3.3.3 Leader 状态转换过程节点状态图中的最后一条线是：discovers server with higher term。想象一个场景：当 leader 节点发生了宕机或网络断连，此时其它 follower 会收不到 leader 心跳，首个触发超时的节点会变为 candidate 并开始拉票（由于随机化各个 follower 超时时间不同），由于该 candidate 的 term 大于原 leader 的 term，因此所有 follower 都会投票给它，这名 candidate 会变为新的 leader。一段时间后原 leader 恢复了，收到了来自新leader 的心跳包，发现心跳中的 term 大于自己的 term，此时该节点会立刻切换为 follower 并跟随的新 leader。上述流程的动画模拟如下：图：S4 作为 term2 的 leader图：S4 宕机，S5 即将率先超时图：S5 当选 term3 的 leader图：S4 宕机恢复后收到了来自 S5 的 term3 心跳图：S4 立刻变为 S5 的 follower以上就是 Raft 的选主逻辑，但还有一些细节（譬如是否给该 candidate 投票还有一些其它条件）依赖算法的其它部分基础，我们会在后续“安全性”一章描述。当票选出 leader 后，leader 也该承担起相应的责任了，这个责任是什么？就是下一章将介绍的“日志复制”。3 日志复制3.1 什么是日志复制在前文中我们讲过：共识算法通常基于状态复制机（Replicated State Machine）模型，所有节点从同一个 state 出发，经过一系列同样操作 log 的步骤，最终也必将达到一致的 state。也就是说，只要我们保证集群中所有节点的 log 一致，那么经过一系列应用（apply）后最终得到的状态机也就是一致的。Raft 负责保证集群中所有节点 log 的一致性。此外我们还提到过：Raft 赋予了 leader 节点更强的领导力（Strong Leader）。那么 Raft 保证 log 一致的方式就很容易理解了，即所有 log 都必须交给 leader 节点处理，并由 leader 节点复制给其它节点。这个过程，就叫做日志复制（Log replication）。3.2 Raft 日志复制机制解析3.2.1 整体流程解析一旦 leader 被票选出来，它就承担起领导整个集群的责任了，开始接收客户端请求，并将操作包装成日志，并复制到其它节点上去。整体流程如下：  Leader 为客户端提供服务，客户端的每个请求都包含一条即将被状态复制机执行的指令。  Leader 把该指令作为一条新的日志附加到自身的日志集合，然后向其它节点发起附加条目请求（AppendEntries RPC），来要求它们将这条日志附加到各自本地的日志集合。  当这条日志已经确保被安全的复制，即大多数（N/2+1）节点都已经复制后，leader 会将该日志 apply 到它本地的状态机中，然后把操作成功的结果返回给客户端。整个集群的日志模型可以宏观表示为下图（x ← 3 代表 x 赋值为 3）：每条日志除了存储状态机的操作指令外，还会拥有一个唯一的整数索引值（log index）来表明它在日志集合中的位置。此外，每条日志还会存储一个 term 号（日志条目方块最上方的数字，相同颜色 term 号相同），该 term 表示 leader 收到这条指令时的当前任期，term 相同的 log 是由同一个 leader 在其任期内发送的。当一条日志被 leader 节点认为可以安全的 apply 到状态机时，称这条日志是 committed（上图中的 committed entries）。那么什么样的日志可以被 commit 呢？答案是：当 leader 得知这条日志被集群过半的节点复制成功时。因此在上图中我们可以看到 (term3, index7) 这条日志以及之前的日志都是 committed，尽管有两个节点拥有的日志并不完整。Raft 保证所有 committed 日志都已经被持久化，且“最终”一定会被状态机apply。注：这里的“最终”用词很微妙，它表明了一个特点：Raft 保证的只是集群内日志的一致性，而我们真正期望的集群对外的状态机一致性需要我们做一些额外工作，这一点在《线性一致性与读性能优化》一章会着重介绍。3.2.2 日志复制流程图解我们通过 Raft 动画 来模拟常规日志复制这一过程：如上图，S1 当选 leader，此时还没有任何日志。我们模拟客户端向 S1 发起一个请求。S1 收到客户端请求后新增了一条日志 (term2, index1)，然后并行地向其它节点发起 AppendEntries RPC。S2、S3 率先收到了请求，各自附加了该日志，并向 S1 回应响应。所有节点都附加了该日志，但由于 leader 尚未收到任何响应，因此暂时还不清楚该日志到底是否被成功复制。当 S1 收到2个节点的响应时，该日志条目的边框就已经变为实线，表示该日志已经安全的复制，因为在5节点集群中，2个 follower 节点加上 leader 节点自身，副本数已经确保过半，此时 S1 将响应客户端的请求。leader 后续会持续发送心跳包给 followers，心跳包中会携带当前已经安全复制（我们称之为 committed）的日志索引，此处为 (term2, index1)。所有 follower 都通过心跳包得知 (term2, index1) 的 log 已经成功复制 （committed），因此所有节点中该日志条目的边框均变为实线。3.2.3 对日志一致性的保证前边我们使用了 (term2, index1) 这种方式来表示一条日志条目，这里为什么要带上 term，而不仅仅是使用 index？原因是 term 可以用来检查不同节点间日志是否存在不一致的情况，阅读下一节后会更容易理解这句话。Raft 保证：如果不同的节点日志集合中的两个日志条目拥有相同的 term 和 index，那么它们一定存储了相同的指令。为什么可以作出这种保证？因为 Raft 要求 leader 在一个 term 内针对同一个 index 只能创建一条日志，并且永远不会修改它。同时 Raft 也保证：如果不同的节点日志集合中的两个日志条目拥有相同的 term 和 index，那么它们之前的所有日志条目也全部相同。这是因为 leader 发出的 AppendEntries RPC 中会额外携带上一条日志的 (term, index)，如果 follower 在本地找不到相同的 (term, index) 日志，则拒绝接收这次新的日志。所以，只要 follower 持续正常地接收来自 leader 的日志，那么就可以通过归纳法验证上述结论。3.2.4 可能出现的日志不一致场景在所有节点正常工作的时候，leader 和 follower的日志总是保持一致，AppendEntries RPC 也永远不会失败。然而我们总要面对任意节点随时可能宕机的风险，如何在这种情况下继续保持集群日志的一致性才是我们真正要解决的问题。上图展示了一个 term8 的 leader 刚上任时，集群中日志可能存在的混乱情况。例如 follower 可能缺少一些日志（a ~ b），可能多了一些未提交的日志（c ~ d），也可能既缺少日志又多了一些未提交日志（e ~ f）。注：Follower 不可能比 leader 多出一些已提交（committed）日志，这一点是通过选举上的限制来达成的，会在下一章《安全性》介绍。我们先来尝试复现上述 a ~ f 场景，最后再讲 Raft 如何解决这种不一致问题。场景a~b. Follower 日志落后于 leader这种场景其实很简单，即 follower 宕机了一段时间，follower-a 从收到 (term6, index9) 后开始宕机，follower-b 从收到 (term4, index4) 后开始宕机。这里不再赘述。场景c. Follower 日志比 leader 多 term6当 term6 的 leader 正在将 (term6, index11) 向 follower 同步时，该 leader 发生了宕机，且此时只有 follower-c 收到了这条日志的 AppendEntries RPC。然后经过一系列的选举，term7 可能是选举超时，也可能是 leader 刚上任就宕机了，最终 term8 的 leader 上任了，成就了我们看到的场景 c。场景d. Follower 日志比 leader 多 term7当 term6 的 leader 将 (term6, index10) 成功 commit 后，发生了宕机。此时 term7 的 leader 走马上任，连续同步了两条日志给 follower，然而还没来得及 commit 就宕机了，随后集群选出了 term8 的 leader。场景e. Follower 日志比 leader 少 term5 ~ 6，多 term4当 term4 的 leader 将 (term4, index7) 同步给 follower，且将 (term4, index5) 及之前的日志成功 commit 后，发生了宕机，紧接着 follower-e 也发生了宕机。这样在 term5~7 内发生的日志同步全都被 follower-e 错过了。当 follower-e 恢复后，term8 的 leader 也刚好上任了。场景f. Follower 日志比 leader 少 term4 ~ 6，多 term2 ~ 3当 term2 的 leader 同步了一些日志（index4 ~ 6）给 follower 后，尚未来得及 commit 时发生了宕机，但它很快恢复过来了，又被选为了 term3 的 leader，它继续同步了一些日志（index7~11）给 follower，但同样未来得及 commit 就又发生了宕机，紧接着 follower-f 也发生了宕机，当 follower-f 醒来时，集群已经前进到 term8 了。3.2.5 如何处理日志不一致通过上述场景我们可以看到，真实世界的集群情况很复杂，那么 Raft 是如何应对这么多不一致场景的呢？其实方式很简单暴力，想想 Strong Leader 这个词。Raft 强制要求 follower 必须复制 leader 的日志集合来解决不一致问题。也就是说，follower 节点上任何与 leader 不一致的日志，都会被 leader 节点上的日志所覆盖。这并不会产生什么问题，因为某些选举上的限制，如果 follower 上的日志与 leader 不一致，那么该日志在 follower 上一定是未提交的。未提交的日志并不会应用到状态机，也不会被外部的客户端感知到。要使得 follower 的日志集合跟自己保持完全一致，leader 必须先找到二者间最后一次达成一致的地方。因为一旦这条日志达成一致，在这之前的日志一定也都一致（回忆下前文）。这个确认操作是在 AppendEntries RPC 的一致性检查步骤完成的。Leader 针对每个 follower 都维护一个 next index，表示下一条需要发送给该follower 的日志索引。当一个 leader 刚刚上任时，它初始化所有 next index 值为自己最后一条日志的 index+1。但凡某个 follower 的日志跟 leader 不一致，那么下次 AppendEntries RPC 的一致性检查就会失败。在被 follower 拒绝这次 Append Entries RPC 后，leader 会减少 next index 的值并进行重试。最终一定会存在一个 next index 使得 leader 和 follower 在这之前的日志都保持一致。极端情况下 next index 为1，表示 follower 没有任何日志与 leader 一致，leader 必须从第一条日志开始同步。针对每个 follower，一旦确定了 next index 的值，leader 便开始从该 index 同步日志，follower 会删除掉现存的不一致的日志，保留 leader 最新同步过来的。整个集群的日志会在这个简单的机制下自动趋于一致。此外要注意，leader 从来不会覆盖或者删除自己的日志，而是强制 follower 与它保持一致。这就要求集群票选出的 leader 一定要具备“日志的正确性”，这也就关联到了前文提到的：选举上的限制。下一章我们将对此详细讨论。4 安全性及正确性前面的章节我们讲述了 Raft 算法是如何选主和复制日志的，然而到目前为止我们描述的这套机制还不能保证每个节点的状态机会严格按照相同的顺序 apply 日志。想象以下场景：  Leader 将一些日志复制到了大多数节点上，进行 commit 后发生了宕机。  某个 follower 并没有被复制到这些日志，但它参与选举并当选了下一任 leader。  新的 leader 又同步并 commit 了一些日志，这些日志覆盖掉了其它节点上的上一任 committed 日志。  各个节点的状态机可能 apply 了不同的日志序列，出现了不一致的情况。因此我们需要对“选主+日志复制”这套机制加上一些额外的限制，来保证状态机的安全性，也就是 Raft 算法的正确性。4.1 对选举的限制我们再来分析下前文所述的 committed 日志被覆盖的场景，根本问题其实发生在第2步。Candidate 必须有足够的资格才能当选集群 leader，否则它就会给集群带来不可预料的错误。Candidate 是否具备这个资格可以在选举时添加一个小小的条件来判断，即：每个 candidate 必须在 RequestVote RPC 中携带自己本地日志的最新 (term, index)，如果 follower 发现这个 candidate 的日志还没有自己的新，则拒绝投票给该 candidate。Candidate 想要赢得选举成为 leader，必须得到集群大多数节点的投票，那么它的日志就一定至少不落后于大多数节点。又因为一条日志只有复制到了大多数节点才能被 commit，因此能赢得选举的 candidate 一定拥有所有 committed 日志。因此前一篇文章我们才会断定地说：Follower 不可能比 leader 多出一些 committed 日志。比较两个 (term, index) 的逻辑非常简单：如果 term 不同 term 更大的日志更新，否则 index 大的日志更新。4.2 对提交的限制除了对选举增加一点限制外，我们还需对 commit 行为增加一点限制，来完成我们 Raft 算法核心部分的最后一块拼图。回忆下什么是 commit：  当 leader 得知某条日志被集群过半的节点复制成功时，就可以进行 commit，committed 日志一定最终会被状态机 apply。所谓 commit 其实就是对日志简单进行一个标记，表明其可以被 apply 到状态机，并针对相应的客户端请求进行响应。然而 leader 并不能在任何时候都随意 commit 旧任期留下的日志，即使它已经被复制到了大多数节点。Raft 论文给出了一个经典场景：上图从左到右按时间顺序模拟了问题场景。阶段a：S1 是 leader，收到请求后将 (term2, index2) 只复制给了 S2，尚未复制给 S3 ~ S5。阶段b：S1 宕机，S5 当选 term3 的 leader（S3、S4、S5 三票），收到请求后保存了 (term3, index2)，尚未复制给任何节点。阶段c：S5 宕机，S1 恢复，S1 重新当选 term4 的 leader，继续将 (term2, index2) 复制给了 S3，已经满足大多数节点，我们将其 commit。阶段d：S1 又宕机，S5 恢复，S5 重新当选 leader（S2、S3、S4 三票），将 (term3, inde2) 复制给了所有节点并 commit。注意，此时发生了致命错误，已经 committed 的 (term2, index2) 被 (term3, index2) 覆盖了。为了避免这种错误，我们需要添加一个额外的限制：Leader 只允许 commit 包含当前 term 的日志。针对上述场景，问题发生在阶段c，即使作为 term4 leader 的 S1 将 (term2, index2) 复制给了大多数节点，它也不能直接将其 commit，而是必须等待 term4 的日志到来并成功复制后，一并进行 commit。阶段e：在添加了这个限制后，要么 (term2, index2) 始终没有被 commit，这样 S5 在阶段d将其覆盖就是安全的；要么 (term2, index2) 同 (term4, index3) 一起被 commit，这样 S5 根本就无法当选 leader，因为大多数节点的日志都比它新，也就不存在前边的问题了。以上便是对算法增加的两个小限制，它们对确保状态机的安全性起到了至关重要的作用。至此我们对 Raft 算法的核心部分，已经介绍完毕。下一章我们会介绍两个同样描述于论文内的辅助技术：集群成员变更和日志压缩，它们都是在 Raft 工程实践中必不可少的部分。5 集群成员变更与日志压缩尽管我们已经通过前几章了解了 Raft 算法的核心部分，但相较于算法理论来说，在工程实践中仍有一些现实问题需要我们去面对。Raft 非常贴心的在论文中给出了两个常见问题的解决方案，它们分别是：  集群成员变更：如何安全地改变集群的节点成员。  日志压缩：如何解决日志集合无限制增长带来的问题。本文我们将分别讲解这两种技术。5.1 集群成员变更在前文的理论描述中我们都假设了集群成员是不变的，然而在实践中有时会需要替换宕机机器或者改变复制级别（即增减节点）。一种最简单暴力达成目的的方式就是：停止集群、改变成员、启动集群。这种方式在执行时会导致集群整体不可用，此外还存在手工操作带来的风险。为了避免这样的问题，Raft 论文中给出了一种无需停机的、自动化的改变集群成员的方式，其实本质上还是利用了 Raft 的核心算法，将集群成员配置作为一个特殊日志从 leader 节点同步到其它节点去。5.1.1 直接切换集群成员配置先说结论：所有将集群从旧配置直接完全切换到新配置的方案都是不安全的。因此我们不能想当然的将新配置直接作为日志同步给集群并 apply。因为我们不可能让集群中的全部节点在“同一时刻”原子地切换其集群成员配置，所以在切换期间不同的节点看到的集群视图可能存在不同，最终可能导致集群存在多个 leader。为了理解上述结论，我们来看一个实际出现问题的场景，下图对其进行了展现。图5-1阶段a. 集群存在 S1 ~ S3 三个节点，我们将该成员配置表示为 C-old，绿色表示该节点当前视图（成员配置）为 C-old，其中红边的 S3 为 leader。阶段b. 集群新增了 S4、S5 两个节点，该变更从 leader 写入，我们将 S1 ~ S5 的五节点新成员配置表示为 C-new，蓝色表示该节点当前视图为 C-new。阶段c. 假设 S3 短暂宕机触发了 S1 与 S5 的超时选主。阶段d. S1 向 S2、S3 拉票，S5 向其它全部四个节点拉票。由于 S2 的日志并没有比 S1 更新，因此 S2 可能会将选票投给 S1，S1 两票当选（因为 S1 认为集群只有三个节点）。而 S5 肯定会得到 S3、S4 的选票，因为 S1 感知不到 S4，没有向它发送 RequestVote RPC，并且 S1 的日志落后于 S3，S3 也一定不会投给 S1，结果 S5 三票当选。最终集群出现了多个主节点的致命错误，也就是所谓的脑裂。图5-2上图来自论文，用不同的形式展现了和图5-1相同的问题。颜色代表的含义与图5-1是一致的，在 problem: two disjoint majorities 所指的时间点，集群可能会出现两个 leader。但是，多主问题并不是在任何新老节点同时选举时都一定可能出现的，社区一些文章在举多主的例子时可能存在错误，下面是一个案例：  来源：zhuanlan.zhihu.com/p/27207160图5-3该假想场景类似图5-1的阶段d，模拟过程如下：  S1 为集群原 leader，集群新增 S4、S5，该配置被推给了 S3，S2 尚未收到。  此时 S1 发生短暂宕机，S2、S3 分别触发选主。  最终 S2 获得了 S1 和自己的选票，S3 获得了 S4、S5 和自己的选票，集群出现两个 leader。图5-3过程看起来好像和图5-1没有什么大的不同，只是参与选主的节点存在区别，然而事实是图5-3的情况是不可能出现的。注意：Raft 论文中传递集群变更信息也是通过日志追加实现的，所以也受到选主的限制。很多读者对选主限制中比较的日志是否必须是 committed 产生疑惑，回看下在《安全性》一文中的描述：  每个 candidate 必须在 RequestVote RPC 中携带自己本地日志的最新 (term, index)，如果 follower 发现这个 candidate 的日志还没有自己的新，则拒绝投票给该 candidate。这里再帮大家明确下，论文里确实间接表明了，选主时比较的日志是不要求 committed 的，只需比较本地的最新日志就行！回到图5-3，不可能出现的原因在于，S1 作为原 leader 已经第一个保存了新配置的日志，而 S2 尚未被同步这条日志，根据上一章《安全性》我们讲到的选主限制，S1 不可能将选票投给 S2，因此 S2 不可能成为 leader。5.1.2 两阶段切换集群成员配置Raft 使用一种两阶段方法平滑切换集群成员配置来避免遇到前一节描述的问题，具体流程如下：阶段一  客户端将 C-new 发送给 leader，leader 将 C-old 与 C-new 取并集并立即apply，我们表示为 C-old,new。  Leader 将 C-old,new 包装为日志同步给其它节点。  Follower 收到 C-old,new 后立即 apply，当 C-old,new 的大多数节点（即 C-old 的大多数节点和 C-new 的大多数节点）都切换后，leader 将该日志 commit。阶段二  Leader 接着将 C-new 包装为日志同步给其它节点。  Follower 收到 C-new 后立即 apply，如果此时发现自己不在 C-new 列表，则主动退出集群。  Leader 确认 C-new 的大多数节点都切换成功后，给客户端发送执行成功的响应。上图展示了该流程的时间线。虚线表示已经创建但尚未 commit 的成员配置日志，实线表示 committed 的成员配置日志。为什么该方案可以保证不会出现多个 leader？我们来按流程逐阶段分析。阶段1. C-old,new 尚未 commit该阶段所有节点的配置要么是 C-old，要么是 C-old,new，但无论是二者哪种，只要原 leader 发生宕机，新 leader 都必须得到大多数 C-old 集合内节点的投票。以图5-1场景为例，S5 在阶段d根本没有机会成为 leader，因为 C-old 中只有 S3 给它投票了，不满足大多数。阶段2. C-old,new 已经 commit，C-new 尚未下发该阶段 C-old,new 已经 commit，可以确保已经被 C-old,new 的大多数节点（再次强调：C-old 的大多数节点和 C-new 的大多数节点）复制。因此当 leader 宕机时，新选出的 leader 一定是已经拥有 C-old,new 的节点，不可能出现两个 leader。阶段3. C-new 已经下发但尚未 commit该阶段集群中可能有三种节点 C-old、C-old,new、C-new，但由于已经经历了阶段2，因此 C-old 节点不可能再成为 leader。而无论是 C-old,new 还是 C-new 节点发起选举，都需要经过大多数 C-new 节点的同意，因此也不可能出现两个 leader。阶段4. C-new 已经 commit该阶段 C-new 已经被 commit，因此只有 C-new 节点可以得到大多数选票成为 leader。此时集群已经安全地完成了这轮变更，可以继续开启下一轮变更了。以上便是对该两阶段方法可行性的分步验证，Raft 论文将该方法称之为共同一致（Joint Consensus）。关于集群成员变更另一篇更详细的论文还给出了其它方法，简单来说就是论证一次只变更一个节点的的正确性，并给出解决可用性问题的优化方案。感兴趣的同学可以参考：《Consensus: Bridging Theory and Practice》。5.2 日志压缩我们知道 Raft 核心算法维护了日志的一致性，通过 apply 日志我们也就得到了一致的状态机，客户端的操作命令会被包装成日志交给 Raft 处理。然而在实际系统中，客户端操作是连绵不断的，但日志却不能无限增长，首先它会占用很高的存储空间，其次每次系统重启时都需要完整回放一遍所有日志才能得到最新的状态机。因此 Raft 提供了一种机制去清除日志里积累的陈旧信息，叫做日志压缩。快照（Snapshot）是一种常用的、简单的日志压缩方式，ZooKeeper、Chubby 等系统都在用。简单来说，就是将某一时刻系统的状态 dump 下来并落地存储，这样该时刻之前的所有日志就都可以丢弃了。所以大家对“压缩”一词不要产生错误理解，我们并没有办法将状态机快照“解压缩”回日志序列。注意，在 Raft 中我们只能为 committed 日志做 snapshot，因为只有 committed 日志才是确保最终会应用到状态机的。上图展示了一个节点用快照替换了 (term1, index1) ~ (term3, index5) 的日志。快照一般包含以下内容：  日志的元数据：最后一条被该快照 apply 的日志 term 及 index  状态机：前边全部日志 apply 后最终得到的状态机当 leader 需要给某个 follower 同步一些旧日志，但这些日志已经被 leader 做了快照并删除掉了时，leader 就需要把该快照发送给 follower。同样，当集群中有新节点加入，或者某个节点宕机太久落后了太多日志时，leader 也可以直接发送快照，大量节约日志传输和回放时间。同步快照使用一个新的 RPC 方法，叫做 InstallSnapshot RPC。至此我们已经将 Raft 论文中的内容基本讲解完毕了。《In Search of an Understandable Consensus Algorithm (Extended Version)》 毕竟只有18页，更加侧重于理论描述而非工程实践。如果你想深入学习 Raft，或自己动手写一个靠谱的 Raft 实现，《Consensus: Bridging Theory and Practice》 是你参考的不二之选。接下来我们将额外讨论一下关于线性一致性和 Raft 读性能优化的内容。6 线性一致性与读性能优化6.1 什么是线性一致性？在该系列首篇《基本概念》中我们提到过：在分布式系统中，为了消除单点提高系统可用性，通常会使用副本来进行容错，但这会带来另一个问题，即如何保证多个副本之间的一致性。什么是一致性？所谓一致性有很多种模型，不同的模型都是用来评判一个并发系统正确与否的不同程度的标准。而我们今天要讨论的是强一致性（Strong Consistency）模型，也就是线性一致性（Linearizability），我们经常听到的 CAP 理论中的 C 指的就是它。其实我们在第一篇就已经简要描述过何为线性一致性：  所谓的强一致性（线性一致性）并不是指集群中所有节点在任一时刻的状态必须完全一致，而是指一个目标，即让一个分布式系统看起来只有一个数据副本，并且读写操作都是原子的，这样应用层就可以忽略系统底层多个数据副本间的同步问题。也就是说，我们可以将一个强一致性分布式系统当成一个整体，一旦某个客户端成功的执行了写操作，那么所有客户端都一定能读出刚刚写入的值。即使发生网络分区故障，或者少部分节点发生异常，整个集群依然能够像单机一样提供服务。“像单机一样提供服务”从感官上描述了一个线性一致性系统应该具备的特性，那么我们该如何判断一个系统是否具备线性一致性呢？通俗来说就是不能读到旧（stale）数据，但具体分为两种情况：  对于调用时间存在重叠（并发）的请求，生效顺序可以任意确定。  对于调用时间存在先后关系（偏序）的请求，后一个请求不能违背前一个请求确定的结果。只要根据上述两条规则即可判断一个系统是否具备线性一致性。下面我们来看一个非线性一致性系统的例子。本节例图均来自《Designing Data-Intensive Application》，作者 Martin Kleppmann如上图所示，裁判将世界杯的比赛结果写入了主库，Alice 和 Bob 所浏览的页面分别从两个不同的从库读取，但由于存在主从同步延迟，Follower 2 的本次同步延迟高于 Follower 1，最终导致 Bob 听到了 Alice 的惊呼后刷新页面看到的仍然是比赛进行中。虽然线性一致性的基本思想很简单，只是要求分布式系统看起来只有一个数据副本，但在实际中还是有很多需要关注的点，我们继续看几个例子。上图从客户端的外部视角展示了多个用户同时请求读写一个系统的场景，每条柱形都是用户发起的一个请求，左端是请求发起的时刻，右端是收到响应的时刻。由于网络延迟和系统处理时间并不固定，所以柱形长度并不相同。  x 最初的值为 0，Client C 在某个时间段将 x 写为 1。  Client A 第一个读操作位于 Client C 的写操作之前，因此必须读到原始值 0。  Client A 最后一个读操作位于 Client C 的写操作之后，如果系统是线性一致的，那么必须读到新值 1。  其它与写操作重叠的所有读操作，既可能返回 0，也可能返回 1，因为我们并不清楚写操作在哪个时间段内哪个精确的点生效，这种情况下读写是并发的。仅仅是这样的话，仍然不能说这个系统满足线性一致。假设 Client B 的第一次读取返回了 1，如果 Client A 的第二次读取返回了 0，那么这种场景并不破坏上述规则，但这个系统仍不满足线性一致，因为客户端在写操作执行期间看到 x 的值在新旧之间来回翻转，这并不符合我们期望的“看起来只有一个数据副本”的要求。所以我们需要额外添加一个约束，如下图所示。在任何一个客户端的读取返回新值后，所有客户端的后续读取也必须返回新值，这样系统便满足线性一致了。我们最后来看一个更复杂的例子，继续细化这个时序图。如上图所示，每个读写操作在某个特定的时间点都是原子性的生效，我们在柱形中用竖线标记出生效的时间点，将这些标记按时间顺序连接起来。那么线性一致的要求就是：连线总是按照时间顺序向右移动，而不会向左回退。所以这个连线结果必定是一个有效的寄存器读写序列：任何客户端的每次读取都必须返回该条目最近一次写入的值。  线性一致性并非限定在分布式环境下，在单机单核系统中可以简单理解为“寄存器”的特性。Client B 的最后一次读操作并不满足线性一致，因为在连线向右移动的前提下，它读到的值是错误的（因为Client A 已经读到了由 Client C 写入的 4）。此外这张图里还有一些值得指出的细节点，可以解开很多我们在使用线性一致系统时容易产生的误解：  Client B 的首个读请求在 Client D 的首个写请求和 Client A 的首个写请求之前发起，但最终读到的却是最后由 Client A 写成功之后的结果。  Client A 尚未收到首个写请求成功的响应时，Client B 就读到了 Client A 写入的值。上述现象在线性一致的语义下都是合理的。所以线性一致性（Linearizability）除了叫强一致性（Strong Consistency）外，还叫做原子一致性（Atomic Consistency）、立即一致性（Immediate Consistency）或外部一致性（External Consistency），这些名字看起来都是比较贴切的。6.2 Raft 线性一致性读在了解了什么是线性一致性之后，我们将其与 Raft 结合来探讨。首先需要明确一个问题，使用了 Raft 的系统都是线性一致的吗？不是的，Raft 只是提供了一个基础，要实现整个系统的线性一致还需要做一些额外的工作。假设我们期望基于 Raft 实现一个线性一致的分布式 kv 系统，让我们从最朴素的方案开始，指出每种方案存在的问题，最终使整个系统满足线性一致性。6.2.1 写主读从缺陷分析写操作并不是我们关注的重点，如果你稍微看了一些理论部分就应该知道，所有写操作都要作为提案从 leader 节点发起，当然所有的写命令都应该简单交给 leader 处理。真正关键的点在于读操作的处理方式，这涉及到整个系统关于一致性方面的取舍。在该方案中我们假设读操作直接简单地向 follower 发起，那么由于 Raft 的 Quorum 机制（大部分节点成功即可），针对某个提案在某一时间段内，集群可能会有以下两种状态：  某次写操作的日志尚未被复制到一少部分 follower，但 leader 已经将其 commit。  某次写操作的日志已经被同步到所有 follower，但 leader 将其 commit 后，心跳包尚未通知到一部分 follower。以上每个场景客户端都可能读到过时的数据，整个系统显然是不满足线性一致的。6.2.2 写主读主缺陷分析在该方案中我们限定，所有的读操作也必须经由 leader 节点处理，读写都经过 leader 难道还不能满足线性一致？是的！！ 并且该方案存在不止一个问题！！问题一：状态机落后于 committed log 导致脏读回想一下前文讲过的，我们在解释什么是 commit 时提到了写操作什么时候可以响应客户端：  所谓 commit 其实就是对日志简单进行一个标记，表明其可以被 apply 到状态机，并针对相应的客户端请求进行响应。也就是说一个提案只要被 leader commit 就可以响应客户端了，Raft 并没有限定提案结果在返回给客户端前必须先应用到状态机。所以从客户端视角当我们的某个写操作执行成功后，下一次读操作可能还是会读到旧值。这个问题的解决方式很简单，在 leader 收到读命令时我们只需记录下当前的 commit index，当 apply index 追上该 commit index 时，即可将状态机中的内容响应给客户端。问题二：网络分区导致脏读假设集群发生网络分区，旧 leader 位于少数派分区中，而且此刻旧 leader 刚好还未发现自己已经失去了领导权，当多数派分区选出了新的 leader 并开始进行后续写操作时，连接到旧 leader 的客户端可能就会读到旧值了。因此，仅仅是直接读 leader 状态机的话，系统仍然不满足线性一致性。6.2.3 Raft Log Read为了确保 leader 处理读操作时仍拥有领导权，我们可以将读请求同样作为一个提案走一遍 Raft 流程，当这次读请求对应的日志可以被应用到状态机时，leader 就可以读状态机并返回给用户了。这种读方案称为 Raft Log Read，也可以直观叫做 Read as Proposal。为什么这种方案满足线性一致？因为该方案根据 commit index 对所有读写请求都一起做了线性化，这样每个读请求都能感知到状态机在执行完前一写请求后的最新状态，将读写日志一条一条的应用到状态机，整个系统当然满足线性一致。但该方案的缺点也非常明显，那就是性能差，读操作的开销与写操作几乎完全一致。而且由于所有操作都线性化了，我们无法并发读状态机。6.3 Raft 读性能优化接下来我们将介绍几种优化方案，它们在不违背系统线性一致性的前提下，大幅提升了读性能。6.3.1 Read Index与 Raft Log Read 相比，Read Index 省掉了同步 log 的开销，能够大幅提升读的吞吐，一定程度上降低读的时延。其大致流程为：  Leader 在收到客户端读请求时，记录下当前的 commit index，称之为 read index。  Leader 向 followers 发起一次心跳包，这一步是为了确保领导权，避免网络分区时少数派 leader 仍处理请求。  等待状态机至少应用到 read index（即 apply index 大于等于 read index）。  执行读请求，将状态机中的结果返回给客户端。这里第三步的 apply index 大于等于 read index 是一个关键点。因为在该读请求发起时，我们将当时的 commit index 记录了下来，只要使客户端读到的内容在该 commit index 之后，那么结果一定都满足线性一致（如不理解可以再次回顾下前文线性一致性的例子以及2.2中的问题一）。6.3.2 Lease Read与 Read Index 相比，Lease Read 进一步省去了网络交互开销，因此更能显著降低读的时延。基本思路是 leader 设置一个比选举超时（Election Timeout）更短的时间作为租期，在租期内我们可以相信其它节点一定没有发起选举，集群也就一定不会存在脑裂，所以在这个时间段内我们直接读主即可，而非该时间段内可以继续走 Read Index 流程，Read Index 的心跳包也可以为租期带来更新。Lease Read 可以认为是 Read Index 的时间戳版本，额外依赖时间戳会为算法带来一些不确定性，如果时钟发生漂移会引发一系列问题，因此需要谨慎的进行配置。6.3.3 Follower Read在前边两种优化方案中，无论我们怎么折腾，核心思想其实只有两点：  保证在读取时的最新 commit index 已经被 apply。  保证在读取时 leader 仍拥有领导权。这两个保证分别对应2.2节所描述的两个问题。其实无论是 Read Index 还是 Lease Read，最终目的都是为了解决第二个问题。换句话说，读请求最终一定都是由 leader 来承载的。那么读 follower 真的就不能满足线性一致吗？其实不然，这里我们给出一个可行的读 follower 方案：Follower 在收到客户端的读请求时，向 leader 询问当前最新的 commit index，反正所有日志条目最终一定会被同步到自己身上，follower 只需等待该日志被自己 commit 并 apply 到状态机后，返回给客户端本地状态机的结果即可。这个方案叫做 Follower Read。注意：Follower Read 并不意味着我们在读过程中完全不依赖 leader 了，在保证线性一致性的前提下完全不依赖 leader 理论上是不可能做到的。"
  },
  
  {
    "title": "HotStuff 共识算法详解",
    "url": "/posts/hotstuff/",
    "categories": "Consensus",
    "tags": "hotstuff, consensus",
    "date": "2022-10-27 12:00:00 +0800",
    





    
    "snippet": "1 前言HotStuff提出了一个三阶段投票的BFT类共识协议，该协议实现了safety、liveness、responsiveness特性。通过在投票过程中引入门限签名实现了O(n) 的消息验证复杂度。Hotstuff总结出对比了目前主流的BFT共识协议，构建了基于经典BFT共识实现pipeline BFT共识的模式。HotStuff是尹茂帆等人提出的分布性一致性协议，在该算法中，最多出错...",
    "content": "1 前言HotStuff提出了一个三阶段投票的BFT类共识协议，该协议实现了safety、liveness、responsiveness特性。通过在投票过程中引入门限签名实现了O(n) 的消息验证复杂度。Hotstuff总结出对比了目前主流的BFT共识协议，构建了基于经典BFT共识实现pipeline BFT共识的模式。HotStuff是尹茂帆等人提出的分布性一致性协议，在该算法中，最多出错节点个数为 f &lt;= (n-1)/3 。该算法有两个优点，第一个优点是，相比于之前的算法，HotStuff是基于leader节点的，拥有线性复杂度，可以极大地降低节点出错时，系统的共识消耗。同时，更替leader的低复杂度，鼓励网络频繁更迭leader节点，对于区块链等领域非常有用。第二个优点是该模型隔离了安全性与活跃性，安全性通过节点投票保证，而活跃性则通过Pacemaker保证。学习HotStuff除了阅读本文，还可以阅读原论文《HotStuff: BFT Consensus in the Lens of Blockchain》，或是原论文的中文翻译。Facebook的Libra数字货币也使用的该算法的变种LibraBFT。原论文提出了HotStuff的三种实现形式，分别为简易的HotStuff算法(Basic HotStuff)，链状HotStuff算法(Chained HotStuff)和事件驱动的HotStuff算法(Event-driven HotStuff)。工程上，人们通常使用Event-driven HotStuff算法，但是如果直接去阅读Event-driven HotStuff算法的源代码会不知所云。Event-driven HotStuff算法之所以比较难以理解，是因为它——  使用了Basic HotStuff算法的共识逻辑，特别的，包括leader如何与replica交互形成共识；  运用了Chained HotStuff提出的流水线优化思想，特别的，如何使用流水线并行优化原算法中相似的阶段；  最后Event-driven HotStuff是Chained HotStuff事件驱动形式，特别的，将原始实现中轮询产生的驱动改为由leader节点发出的事件驱动。一方面，从论文的结构上来看，先介绍了前两者，最后才在Implementation章节介绍了Event-driven HotStuff。但另一方面，Event-driven HotStuff又是三者中代码最简单的，也是三者中唯一一个可以在网上找到大量源码进行对照的实现。因此直接上手阅读源码，在遇到困难时再去查阅简单版本的实现也是一个很好地做法（事实上论文作者也推荐直接阅读Event-driven HotStuff）。2 协议内容2.1 协议基础名词解释  BFT: 全称是Byzantine Fault tolerance， 表示系统可以容纳任意类型的错误，包括宕机、作恶等等。  SMR: 全称是State Machine Replication， 一个状态机系统，系统的每个节点都有着相同的状态副本。  BFT SMR protocol: 用来保证SMR中的各个正常节点都按照相同的顺序执行命令的一套协议。  View: 表示一个共识单元，共识过程是由一个接一个的View组成的，每个View中都有一个ViewNumber表示，每个ViewNumber对应一个Leader。  QC(quorum certificate): 表示一个被(n−f)个节点签名确认的数据包及viewNumber。比如，对某个区块的(n−f)个投票集合。  prepareQC: 对于某个prepare消息，Leader收集齐(n−f)个节点签名所生成的证据（聚合签名或者是消息集合），可以视为第一轮投票达成的证据。  lockedQC: 对于某个precommit消息，Leader收集齐(n−f)个节点签名所生成的证据（聚合签名或者是消息集合），可以视为第二轮投票达成的证据。视图状态机复制中单次状态转移在HotStuff中以视图(View)的形式出现，leader节点收集网络中其他节点的信息，发送提案并取得共识的整个过程可以看做是一个视图，视图实际上可以类比于状态机的一次转移，其中包含了这次转移需要执行的用户命令。而整个分布式系统，正是通过一次又一次的视图变换(View-Change)，得以逐轮推进运作。HotStuff是基于View的的共识协议，View表示一个共识单元，共识过程是由一个接一个的View组成。在一个View中，存在一个确定Leader来主导共识协议，并经过三阶段投票达成共识，然后切换到下一个View继续进行共识。假如遇到异常状况，某个View超时未能达成共识，也是切换到下一个View继续进行共识。状态机复制  | State Machine Replication状态机复制(State Machine Replication, SMR)是人们为了解决分布式系统同步问题提出的一种理论框架。为了让一个系统拥有较强的容错能力，人们通常会部署多个完全相同的副本节点，任意一个节点的崩溃都不会影响整个系统的正常工作。在工程上，这些副本节点通常使用状态机复制理论进行同步。副本状态机（SMR, State Machine Replication）指的是状态机由多个副本组成，在执行命令时，各个副本上的状态通过共识达成一致。假如各个副本的初始状态是一致的，那么通过共识机制使得输入命令的顺序达成全局一致，就可以实现各个副本上状态的一致。在SMR中，存在一个Leader节点发送proposal，然后各个节点参与投票达成共识。系统输入为tx，网络节点负责将这些tx，打包成一个block，每个block都包含其父block的哈希索引。分支直观地看，HotStuff中，每一个副本节点都会在自己的内存中维护一个待提交指令的树状结构。每个树叶子节点都包含了一个待提交的指令，树节点到根节点组成了一个分支。未提交的分支之间互相是竞争关系，一轮中只会有一个分支被节点所共识。系统中存在一个唯一的leader被其他所有节点所公认，这个leader会尝试将包含“待执行指令”的提议附加到一个已经被 n-f 个副本投票支持的分支。并尝试就新的分支与其他副本达成共识，共识成功后，整个系统所有节点都会提交并执行这些新的附加操作指令。  值得注意的是HotStuff并不关心leader的选取过程，因此在后续算法中，我们需要默认leader已经由其他模块指定。仲裁证书HotStuff中的投票使用密码学中的仲裁证书(Quorum Certificate, QC)，每一个视图都会关联一个仲裁证书，仲裁证书表明该视图是否获得了足够多副本的认可。仲裁证书本质是副本节点通过自己的私钥签名的一种凭证。如果一个副本节点认同某一个分支，它会使用自己的私钥对该分支签名，创建一个部分证书发送给leader。leader收集到 n-f 个部分证书，可以合成一个仲裁证书，一个拥有仲裁证书的视图意味着获得了多数节点的投票支持。视图的投票总共分为三个步骤，准备阶段prepare, 预提交阶段pre-commit，提交阶段commit。每一次投票都会合成一个仲裁证书，不同阶段的证书从含义上稍微有些区别，在后续章节的阅读时需要注意。网络假设在实际的分布式系统中，由于网络延时、分区等因素，系统不是同步的系统。在异步的网络系统，由FLP原理可知，各个节点不可能达成共识，因此对于分布式系统的分析，一般是基于部分同步假设的。  同步（synchrony）：正常节点发出的消息，在已知的时间间隔内可以送达目标节点，即最大消息延迟是确定。  异步（asynchrony）：正常节点发出消息，在一个时间间隔内可以送达目标节点，但是该时间间隔未知，即最大消息延迟未知。  部分同步（partially synchrony）: 系统存在一个不确定的GST（global stable time）和一个Δ，使得在GST结束后的Δ时间内，系统处于一个同步状态。2.2 Basic HotStuff 三阶段流程2.2.1 Prepare阶段每个View开始时，新的Leader收集由(n−f)个副本节点发送的NEW-VIEW消息，每个NEW-VIEW消息中包含了发送节点上高度最高的prepareQC(如果没有则设为空)。  prepareQC可以看做是对于某个区块(n−f)个节点的投票集合，共识过程中第一轮投票达成的证据Leader从收到的NewView消息中，选取高度最高的preparedQC作为highQC。因为highQC是viewNumber最大的，所以不会有比它更高的区块得到确认，该区块所在的分支是安全的。下图是Leader节点本地的区块树， #71是Leader节点收到的highQC, 那么阴影所表示的分支就是一个安全分支，基于该分支创建新的区块不会产生冲突。Leader节点会在highQC所在的安全分支来创建一个新的区块，并广播proposal，proposal中包含了新的区块和highQC，其中highQC作为proposal的安全性验证。其他节点（replica）一旦收到当前View对应Leader的Proposal消息，Replica会根据会SafeNode-predicate规则检查Proposal是否合法。如果Proposal合法，Replica会向Leader发送一个Prepare-vote（根据自己私钥份额对Proposal的签名）。Replica对于Proposal的验证遵循如下的规则:      为保证Safety，Proposal消息中的区块是从本机lockedQC的区块扩展产生（即m.block是lockedQC.block的子孙区块）          在 HotStuff 中，每个节点都会维护一个 lockedQC，用于表示该节点已经看到的最高的已经被共识过的区块。当一个节点收到一个新的 Proposal 消息时，它需要检查其中的区块是否可以从自己的 lockedQC 扩展得到。      假设一个节点看到的 lockedQC 包含区块 A，它接下来收到了一个 Proposal，包含区块 B，而区块 B 并不能从区块 A 扩展得到。那么，这个节点就可以确定自己之前共识过的区块已经过时了，因为现在有另一个区块链分支已经比之前的更长了。如果这个节点现在继续支持之前的区块 A，那么它就会与其他节点产生分歧，从而导致共识的失败。      因此，HotStuff 协议要求每个节点在收到 Proposal 消息时，必须检查其中的区块是否可以从自己的 lockedQC 扩展得到，以确保所有节点在共识过程中都能够基于同样的区块链分支进行共识，从而保证共识的正确性。            为保证Liveness, 除了上一条之外，当Proposal.highQC高于本地lockedQC中的view_number时也会接收该proposal（说明自己被卡在了一个其他节点早已达成共识的视图，因此可以忽略自身的lockedQC，直接尝试与leader重新同步）。  safety判断规则对比的是lockedQC，而不是第一轮投票的结果，所以即使在上一轮针对A投了prepare票，假如A没有commit，那么下一轮依然可以对A’投票，所以说第一轮投票可以反悔。2.2.2 Pre-commitLeader发出proposal消息以后，等待(n−f)个节点对于该proposal的签名，集齐签名后会将这些签名组合成一个新的签名，以生成prepareQC保存在本地，然后将其放入PRECOMMIT消息中广播给Replica节点。prepareQC可以表明有(n−f)个节点对相应的proposal进行了签名确认。digraph prepare {    rankdir=LR;    Leader -&gt; Replica1 [label=\"PRECOMMIT\"]    Leader -&gt; Replica2     Leader -&gt; Replica3     Leader -&gt; Replica4 }  在PBFT、Tendermint中，签名（投票）消息是节点间相互广播，各个节点都要做投票收集工作，所以对于每轮投票，Replica都需要至少验证(n−f)个签名。  在HotStuff中引入了门限签名方案，Replica利用各自的私钥份额签名，由Leader收集签名，Replica只需要将签名消息发送给Leader就可以。Leader将Replica的签名组装后，广播给Replica。这样HotStuff的一轮投票每个Replica只需要验证一次签名。  在HotStuff中，一轮投票的过程，是通过Replica与Leader的交互完成          replica收到proposal，对其签名后，发送给Leader      Leader集齐签名（投票）后，将签名（投票）组装，广播precommit消息      replica收到Precommit，验证其中签名，验证通过则表示第一轮投票成功。      LibraBFT是基于HotStuff的共识协议，但是并没有采用HotStuff中的门限签名方案当Replica收到Precommit消息时，会对其签名，然后回复给leader。2.2.3 Commitcommit阶段与precommit阶段类似，当Leader收到当前Proposal的（n-f）个precommit-vote时，会将这些投票组合成precommitQC，然后将其放入COMMIT消息中广播。当Replica收到COMMIT消息时，会对其签名commit-vote，然后回复给leader。更为重要的是，在此时，replica锁定在precommitQC上，将本地的lockQC更新成收到的precommitQC.  从Replica发出precommit-vote到Leader集齐消息并发出commit消息，这个过程相当于pbft、tendermint中的第二轮投票。  Replica收到了commit消息，验证成功后，表示第二轮投票达成。此时Replica回复给Leader，并且保存precommitQC到lockedQC.2.2.4 Decide当Leader收到了（n-f）个commit-vote投票，将他们组合成commitQC，广播DECIDE消息。Replica收到DECIDE消息中的commitQC后，认为当前proposal是一个确定的消息，然后执行已经确定的分支上的tx。View-Number加1，开始新的阶段。  Note: 这里也是针对输入做共识，共识后再执行已经确定共识分支上的交易。2.3 Safety2.3.1 Safety性证明2.3.1.1 同一个View下，不会对冲突的区块，产生相同类型的QC证明思路： 反证法，假如在同一个view下，产生了相同类型的QC，而且最多存在f个作恶节点，那么就会有一个诚实节点双投了，这与前提假设矛盾。  Lemma１：　对于任意两个有效的qc1、qc2，假如qc1.type==qc2.type，且qc1.block与qc2.block冲突，那么必然有qc1.viewNumber!=qc2.viewNumber.证明（反证法）：假设 qc1.viewNumber == qc2.viewNumber那么，在相同的 view 中，有 2f+1 个 replica 对 qc1.block 进行签名投票，同样有 2f+1 对 qc2.block 投票。这样的话，就存在一个正常节点在算法流程中投了针对某个消息投了两票，这与算法流程冲突。2.3.1.2 正常Replica不会commit冲突的区块证明思路： 反证法，假如正常节点commit了冲突的区块，我们追踪到最早出现的冲突区块的位置，则这个冲突的位置肯定与两条safety规则相矛盾。证明：1. 根据 Lemma1 ，在相同的view下，正常的replica不会对冲突的区块产生commitQC，所以不会commit冲突的区块。2. 下面证明在不同的view下，正常的replica也不会对冲突的区块产生commit证明（反证法）：假设viewNumber在v1和v2时（v1 &lt; v2），commit了冲突的区块，即存在commitQC_1 = {block1, v1}, commitQC_2={block2, v2}，且block1与block2冲突。为了简化证明，我们同时假设v1与v2之间不存在其他的commitQC了，即commitQC_2是commit_1之后的第一个commitQC.在v1和v2之间,肯定存在一个最小的v_s(v1 &lt; v_s &lt;= v2)，使得v_s下存在有效的prepareQC_s{block_s, v_s},其中block_s与block1冲突.当含有block_s的prepare被广播后，节点会对该消息做safety验证，由于block_s与block1冲突，所以显然，不符合safety规则1.那么是否会符合规则2呢？假如block_s.parent.viewNumber &gt; block_1.viewNumber，那么显然block_s.parent与block_1冲突，所以block_s.parent是更早的与block1冲突的，这与v_s最小矛盾。有2f+1个节点对于block_s的prepare消息投了票,那么这些节点在收到Prepare_s时,会进行safeNode验证,正常情况下,由于block_s与block1冲突,那么正常节点不会投出prepare_vote票,故而根本不会产生prepareQC_s, v_s根本不会存在. 这与上述假定冲突,因此在不同的view下,不可能对相同的block产生commit.2.4 Chained HotStuff在Basic HotStuff中，三阶段投票每一阶段无非都是发出消息然后收集投票，Chained HotStuff 改进了Basic HotStuff协议的效用，同时对其进行了简化。其思想是在每个 prepare 阶段都改变 view，也就是每个 proposal 都有自己的 view 。这减少了消息的种类，并使得决策可以流水线化处理。在Chained HotStuff中，leader 将 prepare 阶段的投票收集到一个 view 中，形成一个genericQC。然后将此 genericQC 传递给下一个 view 中的 leader，实质上是将原本下一阶段（pre-commit）的责任委托给了下一个 leader。然而，下一个leader2 并不会真的执行 pre-commit 阶段，而是会启动一个新的 prepare 阶段，并添加自己的 proposal 。view v+1 的这个 prepare 阶段同时也是 view v 的 pre-commit 阶段。view v+2 的 prepare 阶段同时也是 viewv+1 的 pre-commit 阶段以及 view v 的 commit 阶段。因为所有阶段都具有相同的结构。如图所示，Basic HotStuff 中阶段的流水线嵌入在了Chained HotStuff proposal 链中。Chained HotStuff的 view v1、v2、v3分别作为 cmd1 在 v1 中提出的 prepare、pre-commit、commit 等Basic HotStuff阶段。该命令在 v4 结束时提交。view v2、v3、v4分别作为 cmd2 在 v2 中提出的三个Basic HotStuff阶段。该命令在v5结束时提交。在这些阶段中生成的其他 proposal 以类似的方式继续流水线处理，用虚线框表示。在图中，单箭头表示节点 b 的 b.parent 字段，双箭头表示 b.justify.node。因此，在 Chained HotStuff 中只有两种消息类型，即 new-view 消息和通用阶段 (generic phase) 的 generic 消息。genericQC 在所有逻辑上流水线化的阶段中都起作用。协议简化为如下过程：  Leader节点          等待NewView消息，然后发出Proposal      发出Proposal后，等待其他节点的投票      向下一个Leader发出NewView消息        非Leader节点          等待来自Leader的Proposal消息      收到Leader的Proposal消息后，检查消息中的QC，更新本地的prepareQC、lockedQC等变量，发出投票      向下一Leader发出NewView消息      2.4.1 Dummy Block正常情况下，每个View中都有一个区块产生并集齐签名，但是情况不会总是这么完美，有时不会有新的区块产生。为了保持区块高度与viewNumber的一致，HotStuff中引入了Dummy Block的概念。假如在一个View中，不能达成共识，那么就在为该View添加一个Dummy Block。2.4.2 K-Chain如图，当节点 b∗ 具有引用直接父节点的 QC 时，即 b∗.justify.node = b∗.parent 时，我们称 b* 形成 1-Chain。记 b′′ = b∗.justify.node。在 b* 形成1-Chain 的基础上，若 b′′.justify.node = b′′.parent，则称 b* 形成2-Chain。在此基础上，若 b′′ 形成 2-Chain，则 b* 形成 3-Chain。  一个区块中的QC是对其直接父区块的确认，那么我们称之为1-chain。同理，一个区块b后面没有Dummy block的情况下，连续产生了k个区块，则称这段区块链分支是对区块b的k-chain。令 b = b′.justify.node，b′ = b′′.justify.node，b′′ = b*.justify.node，任何一个节点都可能出现祖先间隙。      如果 b* 形成 1-Chain，则 b′′ 的 prepare 阶段已成功。因此，当副本为 b* 投票时，还需要执行 genericQC ← b*.justify。    如果 b* 形成 2-Chain，则 b′ 的 pre-commit 阶段已成功。因此，副本应更新 lockedQC ← b′′.justify。  如果 b* 形成 3-Chain，则 b 的 commit 阶段已成功，b 成为已提交的决策。  每当一个新的区块形成，节点都会检查是否会形成1-chain，2-chian，3-chain.      1-chain: 有新的prepareQC形成，更新本地的 genericQC    2-chain: 有新的precommitQC形成，更新本地的 lockedQC    3-chian: 有新的commitQC形成，有新的区块分支进入 commit 状态，执行确认的区块分支  Chained HotStuff 协议伪代码：2.4.3 Pacemaker把 HotStuff 抽象成一个事件驱动的协议，可以将 liveness 相关的功能抽离出来，成为单独的 pacemaker 模块。safety 与 liveness 在实现上解耦，safety 是协议的核心保证安全性，liveness 由 pacemaker 保证。  Pacemaker实现如下几部分功能          Leader检查      收集NewView消息，对齐View并更新highQC      3 讨论  BFT类共识算法研究对比： PBFT - Tendermint - HotStuff - Casper - GRANDPA          PBFT： 两阶段投票，每个view有超时，viewchange通过一轮投票来完成，viewchange消息中包含了prepared消息（即达成了第一阶段投票的消息）。      Tendermint: 两阶段投票，一个round中的各个阶段都有超时时间，roundchange通过超时触发（而不是投票），网络节点保存自己已经达成第一阶段投票的消息（即polka消息）。      HotStuff: 三阶段投票，每个view有超时，采用门限签名减小消息复杂度。liveness与safety解耦为两个部分      GRANDPA: 将出块与共识确认分离，用来对已经产生的区块链进行投票确认，两阶段投票，但是投票是针对区块分支（对一个区块投票也相当于对其所有父区块投票），而不是特定区块，各个节点可以针对不同高度的区块投票      "
  },
  
  {
    "title": "IPFS 中的分布式哈希表 DHT",
    "url": "/posts/dht/",
    "categories": "IPFS",
    "tags": "ipfs",
    "date": "2022-10-15 12:00:00 +0800",
    





    
    "snippet": "分布式哈希表 (Distributed Hash Table, DHT) 是一种用于将键映射到值的分布式系统。在 IPFS 中，DHT 是内容路由系统的基本组件，就像分布式的文件索引系统，将用户要查找的内容与存储该内容的节点联系起来。可以将其想象成一个巨大的电话簿，存储着谁拥有什么数据。使用 DHT 映射的键值对分为三种类型：这些类型的含义略有不同，但它们都使用相同的 DHT 协议进行更新和...",
    "content": "分布式哈希表 (Distributed Hash Table, DHT) 是一种用于将键映射到值的分布式系统。在 IPFS 中，DHT 是内容路由系统的基本组件，就像分布式的文件索引系统，将用户要查找的内容与存储该内容的节点联系起来。可以将其想象成一个巨大的电话簿，存储着谁拥有什么数据。使用 DHT 映射的键值对分为三种类型：这些类型的含义略有不同，但它们都使用相同的 DHT 协议进行更新和查找。IPFS 使用改良的 Kademlia。1 KademliaKademlia 算法的目的是在三个系统参数之上构建 DHT：  地址空间 (address space) ：可以唯一标识所有网络节点的方式。在 IPFS 中是从 0 到 2^256-1 的所有数字。  度量 (metric)：对地址空间中的对等点进行排序的标准，IPFS 采用 SHA256(PeerID) 并将其转换为 0 到 2^256-1 之间的整数。  投射方法 (projection)：根据一条记录的record key计算出地址空间中的一个位置，最适合存储该记录的（一个或多个）节点应邻近该位置。 IPFS 采用 SHA256(Record Key)。有了地址空间和节点排序标准，我们就可以将网络视为一个排序列表进行搜索。我们可以将系统变成类似跳表的形式，其中节点知道距离为 1,2,4,8... 的其他节点。这使我们能在在对数于网络规模的时间内搜索列表，即查找时间为 O(log(N)) 。与跳表不同，Kademlia 不稳定，因为节点可以随时加入、离开或重新加入网络。为了应对系统的不稳定特性，Kademlia 节点不仅仅保持与距离为 1,2,4,8... 的节点的链接。对于每个 2 的倍数，它保持最多 K 个链接。在 IPFS中 K = 20 。例如，与其保持单个距离 128 的链接，不如保持 20 个距离在 65 到 128 之间的链接。像 K 这样的网络范围参数的选择不是任意的，而是根据网络中观察到的平均流失率和网络重新发布信息的频率来确定的。系统参数（例如K）的计算旨在最大化网络保持连接的概率，并且在保持理想查询延迟的同时不丢失任何数据，前提是平均流失率观察值保持恒定。这些系统和网络参数驱动着 Kademlia 的两个主要组件的决策：  路由表 (routing table)：跟踪网络中的所有链接；  查找算法 (lookup algorithm)：确定如何遍历这些链接以存储和检索数据。1.1 不可拨节点Kademlia 的一个主要特性是所有节点都可以从小到大排列。该特性的用处在于，当节点 0按顺序寻找节点 55 时，它可以知道自己在逐渐接近目标。但这需要序列中的每个节点都可以相互通信。否则，节点 33 可能会告诉节点 0 想要的内容在无法通信的节点上，这会使网络缓慢、分裂，数据只能被部分节点访问。节点间无法相互通信的两个常见原因是网络地址转换器 (NAT) 和防火墙。节点 X、Y 和 Z 可以连接到 A 但 A 无法连接到 X、Y 的非对称网络很常见。同样，在 NAT 后的节点 A 和 B 无法相互通信也是非常常见的。为了解决这个问题，IPFS 节点忽略了公认无法访问的节点。如果节点怀疑自己不可达也会将自己从网络中过滤掉。为此，需要使用 libp2p 的AutoNAT，它充当分布式 STUN 层 ( session traversal utility for NAT, NAT会话穿越应用程序) ，通知节点其地址及节点是否公开可拨。只有当节点检测到自身公开可拨时，才会从客户端模式（可以查询 DHT 但不响应查询）切换到服务器模式（可以查询及响应查询）。同样，如果服务器发现自己不可拨，将切换回客户端模式。IPFS 在所有公开可拨的 IPFS 节点上暴露速率受限的 AutoNAT 服务。这类请求很少见并且没有明显的开销。2 双DHT许多 IPFS 节点使用公共 DHT 发现和宣传内容，但是，某些节点在隔离网络中运行，例如本地网络或隔离的 VPN。一个所有非公开可拨节点都是客户端的 DHT 会为此类用户带来问题，因为节点都不是公开可拨的。可用于非公用网络节点的 DHT，称为 LAN DHT。与公共的 WAN DHT 完全分离。这两个 DHT 通过使用不同的 DHT 协议名称来区分：            DHT      Path                  WAN      /ipfs/kad/1.0.0              LAN      /ipfs/lan/kad/1.0.0      WAN 和 LAN DHT 之间的主要区别在于节点的接受标准：哪些节点有资格成为路由表或查询的一部分。 WAN DHT 的标准是“看起来像不像公共地址”，LAN DHT 的标准是“看起来像不像非公共地址”。 WAN DHT 节点根据其是否公开可拨决定是否从客户端模式切换到服务器模式，而 LAN DHT 节点始终是服务器，除非设置了 dhtclient 选项。3 路由表路由表是用于决定网络数据传输路径的一组规则。所有支持 IP 的设备，包括路由器和交换机，都会用到路由表。每个 IPFS 节点都维护一个路由表，其中包含指向网络中其他节点的链接。 IPFS 依靠 Kademlia 算法来定义内容是否应该进入路由表：  当我们连接到一个节点时，检查它是否符合添加到我们的路由表的条件。  如果符合条件，则确定新节点与我们的距离，以确定它应该进入哪个桶 (bucket)。  尝试将节点放入桶中。  如果我们无法连接到路由表中的某个节点，则将其从路由表中删除。这里有三个值得注意的属性：条件、桶和刷新/删除节点。3.1 条件可以被添加进路由表的节点满足以下两个条件：  确保节点是 DHT 服务器，且在宣传 DHT 协议 ID，WAN DHT 为 /ipfs/kad/1.0.0，LAN DHT 为 /ipfs/lan/kad/1.0.0。  确保节点 IP 地址与我们预期的范围相匹配。例如，公共 DHT 的成员至少有一个公共范围的 IP 地址，而不是只有 192.168.X.Y 这样的地址。3.2 节点桶桶是最多 20 个具有相似地址的节点的集合。例如，如果节点距离我们在 2^7 到 2^8 之间，地址空间大小为 2^256，则节点进入桶 256-8。如果该桶中节点少于 20 个，则可以将节点添加进该桶。如果该桶已经有 20 个节点，则 IPFS 决定是否可以删除其中的节点。否则，IPFS 不会将节点添加到桶中。3.3 刷新/删除节点为保持路由表准确、及时更新，IPFS 每 10 分钟刷新一次路由表。虽然比必要的频率高，但在 IPFS 获取更多 DHT 网络动态信息的同时确保网络的健康非常重要。路由表刷新的工作方式如下：  遍历所有桶，从桶 0 到包含节点的最高桶。最大可能的桶数上限为 15。          对于每个桶，在 Kademlia 空间中选择一个可以放入该桶的随机地址，并进行查找以找到离该随机地址最近的 K 个节点。这可以确保我们用尽可能多的节点填满每个桶。        此外，在网络中搜索我们自己，以防网络规模和网络分布使得前 15 个桶不足以包含离我们最近的 K 个节点。节点可以出于几个原因从路由表中删除，通常是因为该节点离线或无法访问。每次刷新后，IPFS 都会遍历路由表并尝试连接到我们最近没有查询过的节点。如果节点不活跃或不在线，就会从路由表中被删除。如果节点在一次刷新时的预期可用时限内不可用，也会被删除。该时限为 Log(1/K) * Log(1 - α/K) * refreshPeriod，其中 α 是可以同时查询的节点数。此外， IPFS 将“可用”定义为“响应时间不超过路由表中任意节点响应时间的二倍”。这主要针对缓慢、过载、不可靠或与我们的网络连接不良的节点。4 查找算法查找算法回答了“最接近 X 的 K 个节点是哪些？”。 Kademlia 查找算法的 IPFS 实现工作流程如下：  从路由表中将 K 个最接近 X 的节点加载到查询队列中。  允许最多 10 个并发查询，找到最接近 X 的节点并询问其哪些是最接近 X 的 K 个节点？  当对某节点的查询完成后，将结果添加到查询队列中。  将次近的节点拉出队列并对其进行查询。  只要成功查询到距 X 最近的三个已知节点且没有任何超时或错误，终止查询。  查询完成后，取出最近的 K 个未失败的节点并返回。"
  },
  
  {
    "title": "PBFT 共识算法详解",
    "url": "/posts/pbft/",
    "categories": "Consensus",
    "tags": "pbft, consensus",
    "date": "2022-06-20 12:00:00 +0800",
    





    
    "snippet": "1 系统模型1.1 网络PBFT 工作在异步的分布式系统中，系统中各个节点彼此通过网络连接。 系统运行时，消息的传递允许出现下列情形：  不能正确发送  延迟  重复  乱序1.2 拜占庭错误系统允许错误节点也就是拜占庭节点表现出任意行为，但是需要附加一个限定条件： 节点失效彼此应相互独立，从而大部分或全部节点不会同时失效。在有恶意攻击存在的情况下，可以采取类似于下列措施来保证这个限制的成立...",
    "content": "1 系统模型1.1 网络PBFT 工作在异步的分布式系统中，系统中各个节点彼此通过网络连接。 系统运行时，消息的传递允许出现下列情形：  不能正确发送  延迟  重复  乱序1.2 拜占庭错误系统允许错误节点也就是拜占庭节点表现出任意行为，但是需要附加一个限定条件： 节点失效彼此应相互独立，从而大部分或全部节点不会同时失效。在有恶意攻击存在的情况下，可以采取类似于下列措施来保证这个限制的成立：  各节点运行的服务程序和操作系统的版本尽可能多样化  各节点的管理员帐号和密码不同1.3 消息加密属性1.3.1 使用加密技术的目的  防止身份欺骗、重放攻击  监测错误消息1.3.2 具体使用的加密技术  公钥签名：用于验证消息发送者身份，PBFT 中，实际上只用于 view-change 和 new-view 消息，以及出现错误的情况。其他消息都采用下面将会提到的 MAC（消息认证码）进行认证。这是算法设计中提出的一种优化措施，用于提升算法性能。  MAC：即消息认证码，用于算法正常操作流程中的消息认证  消息摘要：用于检测错误消息1.4 敌手特性算法限定敌手（adversary）可以：      串通拜占庭节点        延迟通信或延迟正常节点  同时，敌手不可以：  无限延迟正常节点的通信  伪造正常节点签名  从消息摘要反推原始消息  让不同消息产生相同摘要2 服务属性2.1 关于副本复制服务PBFT 算法可用于实现确定性的副本复制服务（Replicated service）。 副本复制服务拥有状态（state）和操作（operation）。客户端（client）向服务发起请求，以执行操作，并等待响应。服务由 n 个节点组成。操作可以执行任何计算，只要这些计算始终产生确定性的结果。节点和客户端如果遵循算法的预定步骤执行操作，则被称为正常节点或客户端。2.2 关于 Safety 和 Liveness只要系统中失效节点的个数不超过容错数 ，系统就能提供 safety 和 liveness。2.2.1 SafetySafety 是指系统能保证客户端请求的线性一致性（linearizability），即请求按顺序一次一条地被执行。PBFT 相对于之前的算法如 Rampart 等的一个显著的不同在于，其 Safety 不依赖于同步假设。算法不需限定客户端一定是正常的，允许其发送不合法的请求，原因是各正常节点可以一致性地监测客户端请求的各种操作。并且算法可以通过权限控制的方式对客户端进行限制。2.2.2 Liveness由于算法不依赖于同步提供 Safety，因此必须通过同步假设来提供 Liveness。这里的同步假设是，客户端的请求最终总能在有限的时间内被系统接收到。客户端可能会通过多次重传的方式，发送请求到服务，确保其被服务接收到。PBFT 所依赖的同步假设其实是比较弱的假设，原因是在真实的系统中，网络错误最终总可以修复。2.3 关于算法弹性PBFT 的算法弹性（resiliency）是最优的：假定系统中失效节点最大个数为 f，则系统最少只需要 3f+1 个节点就可以保证 Safety 和 Liveness。  简单证明：  考虑最坏的情况，系统有最大数量的失效节点，即 f 个 (总节点数为 n) 。客户端此时可以接收到的回复个数最坏情况是 n-f ，因为失效节点可能都不回复。  但是由于网络延迟等原因，客户端接收到的 n-f 个请求中，实际上有可能包含有失效节点的回复（有可能是错误的），而另外一些正常节点的回复还未及时收到。 这其中，最坏的情况是，n-f 个结果中，有 f 个是失效节点发送的。  按照 PBFT 算法的定义，客户端需要收到 f+1 个相同的回复，才被当作是正确的结果。  因此 n-f 个结果中，除去 f 个失效节点的结果，即 n-f-f &gt;= f+1, 即 n &gt;= 3f+1，因此 3f+1 是最少需要的节点数量。2.4 关于信息保密性一般情况下，为确保服务的高效性，不能提供容错的信息保密性。可能可以使用 secret sharing scheme 来获得操作参数和部分对操作来说透明的状态的保密性。3 算法主流程3.1 主流程简介3.1.1 相关定义算法是状态机副本复制技术的一种形式：服务被建模为状态机，其状态在分布式系统中的不同副本节点上被复制。每个状态机副本节点保存维护着服务状态，并实现服务的各种操作。假设所有副本节点个数为 n，算法中，每个节点依次编号为 0, 1, …, n-1方便起见，假设系统中的副本节点总数为 3f+1。可以有更多数量的节点，但是这不会使算法的弹性更优，只会使系统的性能降低。系统在称为视图（view）的配置下工作。视图以整数编号，从 0 开始。在一个具体的视图 v 中，通过 p =v mod n，决定出主节点（primary），而其余节点成为副本节点（backup）。当主节点失效时，进行视图变更（view change）。视图的编号是连续递增的。3.1.2 算法主流程简要描述算法主流程可简要描述如下：  客户端通过向主节点发送请求，以调用服务的操作；  主节点向其他所有副本节点广播该请求；  各节点执行客户请求，同时将回复发送到客户端；  客户端收到 f+1 个来自不同节点的相同的回复后，此回复即为本次请求的结果。因为算法基于状态机副本复制技术，所以节点需满足两个条件：  必须是确定性的，即对于给定的状态，以及给定的参数集合，调用相同的操作后，将始终得到相同的结果。  各节点拥有相同的初始状态。在满足上述两个条件的情况下，算法可以保证系统的 Safety 属性：即使存在失效的节点，各正常副本节点仍可以就不同的请求的执行顺序达成总体的一致。3.2 算法主流程接下来详细描述算法主流程。为方便起见，这里省略讨论以下细节：  节点因空间不足导致错误，以及如何恢复；  类似网络的原因等导致的客户端消息的重传。另外，假设消息使用公钥签名进行认证，而不是更高效的 MAC 的方式。算法流程的启动从客户端发送请求开始。3.2.1 客户端操作客户端操作流程示意图如下：客户端向其认为的 Primary 节点发送请求：&lt;REQUEST, o, t, c&gt;。其中：      o 是请求的操作        t 是时间戳        c 代表客户端信息  这里省略了消息的签名，包括下文提到的所有消息都应该有发送方的签名，为了讨论方便，作了省略。相关的几点说明：  请求中的时间戳用于保证请求只被执行一次的语义：所有的时间戳都严格排序，即后发送的请求比先发送的请求拥有更高的时间戳。  每个副本节点向客户端发送回复时，都会包含当前的视图编号。客户端可以通过该视图编号来确定当前的主节点，并且只向其认为的主节点发送请求。  每个副本节点执行完请求后，各自单独地向客户端发送回复，其格式为： &lt;REPLY, v, t, c, i, r&gt; 。其中：          v 是当前的视图编号      t 是请求发送时对应的时间戳      i 是副本节点的编号      r 是请求的操作的执行结果            客户端等待来自 f+1 个不同副本节点的相同回复，即 t 和 r 要相同。如果客户端等到了 f+1 个相同回复，r 即为请求的结果。 之所以该结果是有效的，是因为错误节点的个数最多为 f 个，因此必然至少有一个正常节点回复了正确结果，此结果就是 r。    如果客户端没有及时收到回复，则会将请求广播给所有副本节点。副本节点收到请求后，如果发现已经执行过该请求，就只是将结果重新发送至客户端；否则，它会把请求转发到主节点。如果主节点没有把请求广播给其他节点，则最终会被足够多的副本节点认定为错误节点，从而触发视图变更。在接下的流程讨论中，假定客户端等待一个请求完成后，才发送下一个请求。但是，算法允许客户端异步地发送请求，并且可以保证不同请求按顺序执行。3.2.2 三阶段协议主节点收到客户端请求后，将启动三阶段协议，也就是算法接下来的流程。这三阶段是 pre-prepare，prepare 和 commit。前两阶段，即 pre-prepare 和 prepare 用于保证当前视图中请求被排好序，而后两阶段 prepare 和 commit 保证请求在视图变更后，仍旧是排好序的。3.2.2.1 pre-prepare 阶段pre-prepare 阶段流程示意图如下：pre-prepare 阶段中，主节点组装预准备消息，同时把客户端请求附加在其后：&lt;&lt;PRE-PREPARE, v, n, d&gt;, m&gt;，其中：  v 是当前视图编号，指当前消息当前在哪个视图中被编号和发送  m 是客户端的请求消息  n 是主节点给 m 分配的一个序号（sequence number）  d 是 m 的摘要。注意：请求 m 并没有放在预准备消息中，这样可以使预准备消息变得更简短。有两个好处：  降低通信的负载：在视图变更时，由于各节点收到的预准备消息会被用来证明一个特定的请求确实在特定的视图中被赋予了一个序号，较简短的预准备消息将使数据传输量更少。  有助于传输优化：算法运行中，一方面需要向各节点发送客户端请求，另一方面需要传输协议消息以实现对客户请求的排序。通过对这两者解耦，可以实现对较小的协议消息的传输以及对应于较大的请求的大消息的传输分别进行优化。每个副本节点接收到预准备消息后，会进行如下校验，如果条件都满足的话，就接受该消息，否则什么也不做：  客户端请求和预准备消息的签名都正确；d 和 m 的消息摘要一致；  当前节点的当前视图是 v；  当前节点未曾接受另外一条预准备消息，其包含的视图编号和消息序号都和本条消息相同，但对应的是不同的客户端请求；  预准备消息中的序号 n 位于低水线 h 和高水线 H 之间。这是为了防止恶意主节点随意地选择序号来耗尽序号空间，例如故意选择一个非常大的序号。如果副本节点接受预准备消息，接下来就进入 prepare 阶段，如下节所示。3.2.2.2 prepare 阶段prepare 阶段流程示意图如下：在 prepare 阶段，节点会组装并广播准备消息给其他所有副本节点，同时把预准备和准备消息写入到本地消息日志中。准备消息格式如下： &lt;PREPARE, v, n, d, i&gt;。其中：      i 为节点编号    其余参数和预准备消息中的含义相同。  副本节点（包括主节点）收到其他节点发送过来的准备消息时，会进行校验，如果消息满足下列条件，则会接受准备消息，并写入消息日志中：  签名正确  其视图编号和节点的当前视图编号相同  消息中的序号在 h 和 H 之间对于一个副本节点 i 来说，如果其消息日志中包含如下消息：  客户端请求 m  在视图 v 中将 m 分配序号 n 的预准备消息  2f 个由不同的副本节点发送的、和预准备消息相匹配的准备消息（匹配是指有相同的视图编号、请求序号，以及消息摘要）我们就称 prepared (m, v, n, i) 为 true。算法的预准备和准备阶段用于保证所有的正常副本节点就同一视图中的所有请求的顺序达成一致。具体来说，这两阶段能确保以下不变式：  如果 prepared (m, v, n, i) 为 true，则对任意一个正常的副本节点 j (包含 i) 来说，prepared (m’, v, n, j) 肯定为 false（这里 m’ 是不同于 m 的一个请求）。  简单证明：  因为 prepared (m, v, n, i) 为 true，而错误节点最多为 f 个，所以至少有 f 个正常节点发送了准备消息，再加上主节点，这样至少有 f+1 个节点已经就 m 在视图 v 中被编号为 n 达成了一致。  因此，如果 prepared (m’, v, n, j) 为 true，意味着上述 f+1 个节点中至少有一个节点发送了两个相互矛盾的预准备或准备消息，也就是说，这些消息拥有相同的视图编号和序号，但是对应着不同的请求消息。  但这是不可能的，因为该节点是正常节点，因此 prepared (m’, v, n, j) 一定为 false。对于副本节点 i 来说，prepared (m, v, n, i) 变为 true，则其将进入 commit 阶段，如下节所示。3.2.2.3 commit 阶段commit 阶段流程示意图如下：节点进入 commit 阶段时，副本节点 i 将向其他所有副本节点广播确认消息：&lt;COMMIT, v, n, D(m), i&gt;，其中：      D(m)是 m 的消息摘要    其余参数和预准备消息和准备消息中的含义相同。  对于副本节点来说，当其收到其他节点发来的确认消息的时候，如果消息满足下列条件，则节点则会接受确认消息并写入本地的日志消息中：  签名正确；  消息中的视图编号等于当前节点的视图编号；  消息中的请求序号在 h 和 H 之间。对于副本节点 i 来说，如果：  prepared (m, v, n, i) 为 true  并且已经接受了 2f+1 个来自不同节点的、和 m 对应的预准备消息相匹配（匹配是指它们有相同的视图编号、消息序号，以及消息摘要）的确认消息（可能包含它自己的）我们称 committed-local (m, v, n, i) 为 true。另外，如果至少存在 f+1 个节点，对于其中每一个节点 i 来说，prepared (m, v, n, i) 为 true，我们则称 committed (m, v, n) 为 true。commit 阶段能保证以下不变式：  如果对某个副本节点来说，committed-local (m, v, n, i) 为 true，则 committed (m, v, n) s 也为 true。上述不变式和视图变更协议一起能够保证：  所有正常节点能够就所有本地确认的请求的序号达成一致，即使这些请求是在不同的视图中确认的。另外，该不变式也能保证：任何一个请求如果在一个副本节点被确认，那么它最终也会被至少 f+1 个副本节点确认。对于任何一个副本节点 i 来说，如果 committed-local (m, v, n, i) 为 true，i 的状态反映了所有序号小于 n 的请求顺序执行的结果。此时，它就可以执行 m 所请求的操作。这就保证了所有的正常节点，按相同的顺序执行请求，从而保证算法的安全性。在执行了请求的操作后，每个节点单独地给客户端发送回复。对于同样内容的请求，节点会忽略那些时间戳更早的，以保证请求只被执行一次。此外，算法并不要求消息按顺序投递，因此，节点可以乱序确认请求。这样做没有问题，因为算法只在一个请求对应的预准备、准备和确认消息都收集完全时才会执行该请求。以下是 f=1，即拜占庭节点数为 1 ，总节点数为 4 时，PBFT 算法的运行示意图：容错证明：  设系统节点数为 n ，其中拜占庭节点数为 f ，系统需要根据节点发来的消息做判断。  为了共识正常进行，在收到 n-f 个消息时，就应该进行处理，因为可能有 f 个节点根本不发送消息。  根据收到的 n-f 个消息做判断，判断的原则至少 f+1 个相同结果。  但最坏情况下，收到的 n-f 个消息中可能有 f 个假消息（由于网络延迟等原因，未收到的是 f 个正常节点的消息）  在此情况下仍能正确共识，应保证：n - f - f &gt;= f + 1, 即 f &lt;= (n-1) / 3垃圾回收为保证安全性（safety），需要保证：  对于每个请求来说，在被至少 f+1 个正常节点执行之前，所有相关的消息都必须记录在消息日志中  如果一条请求被执行，节点能够在视图变更中向其他节点证明这一点另外，如果某个副本节点缺失的一些消息正好已经被所有正常节点删除，则需要通过传输部分或全部的服务状态来使节点状态更新到最新。因此，节点需要某种方式来证明状态的正确性。如果每执行一次操作，都生成一个状态证明，代价将会很大。因此可以每执行一定数量的请求后生成一次状态证明，例如：只要节点序号是某个值比如 100 的整数倍时，就生成一次，此时称这个状态为检查点 checkpoint 。如果一个检查点带有相应的证明，我们则称其为稳定检查点 stable checkpoint 。checkpoint 就是当前节点处理的最新请求序号。stable checkpoint 就是大部分节点 （2f+1） 已经共识完成的最大请求序号。stable checkpoint 的作用：就是减少内存的占用。稳定检查点的生成过程：  副本节点 i 生成一个检查点之后，会组装检查点消息，并全网广播给其他所有副本节点，检查点消息格式： &lt;CHECKPOINT, n, d, i&gt;，n 指的是生成目前最新状态的最后一条执行请求的序号，d 是当前服务状态的摘要。  每个副本节点等待并收集 2f+1 个来自其它副本节点的检查点消息（有可能包括自己的），这些消息有相同的序号 n 和摘要 d。这 2f+1 个检查点消息就是该检查点的正确性证明。一旦一个检查点成为稳定检查点后，节点将从本地消息日志中删除所有序号小于或等于 n 的请求所对应的预准备、准备和确认消息。同时，会删除所有更早的检查点和对应的检查点消息。检查点的生成协议可以用来移动低水线 h 和高水线 H高低水线：  h 的值就是最新稳定检查点所对应的稳定消息序号；  高水线 H = h+k，这里 k 要设置足够大，至少要大于检查点的生成周期，比如说：假如每隔 100 条请求生成检查点，k 就可以取 200。  例子：  如果 B 当前的 checkpoint 已经为 1134，而A的 checkpoint 还是 1039 ，假如有新请求给 B 处理时，B会选择等待  等到 A 节点也处理到和 B 差不多的请求编号时，比如 A 也处理到 1112 了，这时会有一个机制更新所有节点的 stabel checkpoint  比如可以把 stabel checkpoint 设置成 1100 ，于是 B 又可以处理新的请求了，如果 L 保持100 不变，这时的高水位就会变成 1100+100=1200 了。视图变更PBFT 算法运行过程中，如果主节点失效了，为了保持系统的活性（liveness），就需要用到视图变更协议。view-change 会有三个阶段，分别是 view-change ， view-change-ack 和 new-view 阶段。从节点认为主节点有问题时，会向其它节点发送 view-change 消息，当前存活的节点编号最小的节点将成为新的主节点。当新的主节点收到 2f 个其它节点的 view-change 消息，则证明有足够多人的节点认为主节点有问题，于是就会向其它节点广播 New-view 消息。注意：从节点不会发起 new-view 事件。对于主节点，发送 new-view 消息后会继续执行上个视图未处理完的请求，从 pre-prepare 阶段开始。其它节点验证 new-view 消息通过后，就会处理主节点发来的 pre-prepare 消息，这时执行的过程就是前面描述的 PBFT 过程。到这时，正式进入 v+1 （视图编号加1）的时代了。视图变更过程中，节点将不再接受其他任何类型的消息，只接受 checkpoint, view-change, 和 new-view 消息。视图变更过程      节点检查到 timer 超期会停止接受消息并多播一个 view-change 消息 &lt;VIEW-CHANGE,v+1,n,C,P,i&gt; 给所有的replica。    消息中的各字段含义：          v+1：要变更到的目标视图编号；      n：对应于节点已知的、最新的检查点 s 的请求序号；      C：包含 2f+1 个检查点消息&lt;CHECKPOINT,n,d,i&gt;的集合。这些检查点消息用于证明稳定检查点 s 的正确性；      P：由 Pm 构成，m 是所有大于 n 的消息的序列号，集合 Pm 是由在 i 中达成 prepared 状态的消息的集合，Pm 的内容包括关于 m 的 pre-prepare 消息和 2f 个 prepare 消息。            新视图中的新主节点将收集来自其他副本节点的 view-change 消息。当其收集到 2f 个有效的 对应新视图 v+1 的 view-change 消息后，将组装并广播 new-view 消息 &lt;NEW-VIEW,v+1,V,O&gt;。    消息中的各字段含义：                  V：有效的 对应于新视图 v+1 的 2f+1 个 view-change 消息（包括主节点自己组装的）的集合；                    O：pre-prepare状态的消息的集合                              O 的确定：            1）根据 V 中各条 view-change 消息中包含的 pre-prepare 和 prepare 消息，主节点先找到最新的稳定检查点，将其对应的消息序号赋值给 min-s;            2）然后从所有 prepare 消息中找到最大的那个消息序号，将其赋值给 max-s;            3）然后，对于 min-s 和 max-s 之间的每一个消息序号 n， 主节点为其组装位于视图 v+1 上的 pre-prepare 消息。每条消息会有两种情况:            ​\ta) 存在 view-change 消息， 其 P 集合中存在一个 Pm 包含的 prepare 消息中包含对应序号为n的消息，则组装 &lt;PRE-PREPARE, v+1, n, d&gt;；            ​\t\t其中，d 是 V 中特定请求消息的摘要，并且该请求在 V 中的一个最新视图上的 pre-prepare 消息中被分配了序号 n 。            ​\tb)  V 中不存在和 n 对应的 prepare 消息。此时主节点模拟一个特殊的空请求（null request）组装 &lt;PRE-PREPARE, v+1, n, D(null)&gt;;            ​\t\t其中，D (null) 为空请求的摘要。空请求和其他请求一样进行三阶段协议，但是其执行就是一个空操作（noop）。                                主节点广播 new-view 消息后，也会把 O 中的消息写入本地消息日志中。同时，如果主节点本地的最新稳定检查点的消息序号落后于 min-s 的话，则会将 min-s 对应的检查点消息写入消息日志，并按之前介绍的垃圾回收机制删除所有的历史消息。  此时，主节点正式变更到新视图 v+1， 开始接收消息。  每个副本节点收到针对 v+1 视图的 new-view 消息后，会进行校验，是否满足以下条件：          签名正确；      其中包含的 view-change 消息是针对 v+1 视图，并且有效；      集合 O 中的信息正确、有效。      之后，在新的视图中，针对所有序号位于 min-s 和 max-s 之间的请求，系统会重新运行三阶段协议。只不过对于每一个副本节点来说，如果当前确认的请求之前已经执行过的话，节点就不再执行该请求。视图变更完成。  可以这样理解，在新的 view 中，节点是在上一轮 view 中各个节点的 prepared 状态基础上进行共识流程的。  发生view转换时，需要的保证的是：如果视图转换之前的消息 m 被分配了序号 n , 并且达到了 prepared 状态，那么在视图转换之后，该消息也必须被分配序号 n (safety特性)。因为达到 prepared 状态以后，就有可能存在某个节点 commit-local。要保证对于 m 的commit-local，在视图转换之后，其他节点的commit-local 依然是一样的序号。视图变更中节点的信息同步视图变更时，由于 new-view 消息中并不包含原始的客户端请求，因此副本节点可能缺失某条客户端请求 m ，或者某个稳定检查点。此时节点可以从其他副本节点同步缺失的信息。例如，假如节点 i 缺失某个稳定检查点 s ，这个检查点在 V 中可以找到相应的证明，也就是 对应的 checkpoint 消息。由于总共有 2f+1 个这样的消息，而错误节点最多 f 个，因此至少有 f+1 个正常节点发送过这样的消息。因此，可以从这些正常节点获取到对应的检查点状态。算法优化减少系统通信量可以采用三种方法减少系统通信量：      向客户端发送回复的消息摘要，而不是回复的原始内容。    客户端指定一个特定的副本节点，从该节点接收完整的回复内容，而只从其他所有节点处接收回复的摘要。通过判读摘要与回复的一致性以及对回复计数，可以确保接收到正确的回复。如果客户端接收不到正确结果，就会按正常流程重新请求系统，并接收不同节点的完整回复。        请求在副本节点 prepared 后，节点即试探性地执行请求，并发送结果。客户端只要收到 2f+1 个匹配的结果，就可以确保该结果的正确性。    也就是说，该请求最终会确认（至少 f+1 个正常副本节点的本地确认）。如果客户端无法得到正确结果，就重新发送请求，系统执行正常流程。一个被试探性执行的请求，有可能在视图变更过程中被中断，并被替换为一个空请求。此时，已经执行过请求的节点可以通过 new-view 消息中的最新的稳定检查点或本地的检查点来更新状态（取决于哪个序号更大）。        针对只读操作，客户端将请求广播到每一个副本节点。各节点判断请求确实为只读且满足条件后，直接执行请求，并发送回复到客户端。客户端收到 2f+1 个相同的回复，即为正确结果；否则客户端之前设置的重发请求定时器将触发超时，使得客户端重发请求，系统执行正常流程。    这种优化的前提条件是，副本节点在执行请求之前，需确保之前所有请求都已确认，并且被执行。  加快消息验证速度使用公钥签名的方式验证消息存在如下不足：      类似于 RSA 这样的签名算法，签名速度比较慢；        其他公钥密码系统，如椭圆曲线公钥密码系统，虽然签名更快，但是验签更慢。  PBFT 算法实际上在正常流程中采用 MAC (Message Authentication Code，消息验证码) 认证消息，因为它具有如下优点，使得算法相对于之前的算法性能提升了一个数量级以上：      MAC 计算速度快；        通过适当优化，可以减少其长度；        通过对 authenticator（vector of MAC）的使用，可以使验证时间为常量，而生成时间只随着节点数量线性增长。  具体来说，PBFT 算法中使用 MAC 和公钥签名的具体场景是：      所有正常操作流程中使用 MAC 验证消息；        视图变更中的消息：view-change, new-view，以及出现错误时，使用公钥签名。这些场景的出现频率相对较低，从而对算法的性能影响较小。  为什么不能是两阶段      Prepared 状态可以证明在节点 i 看来非拜占庭节点在只有请求 m 使用&lt;v, n&gt;上达成一致。    Prepared 是一个局部视角，不是全局一致，即副本 i 看到了非拜占庭节点认可了&lt;m, v, n&gt;，副本 i 无法确定，其他副本也达到Prepared状态。  如果只有少数副本成为 Prepared 状态，然后执行了请求 m，系统就出现了不一致。  现在我们假设，如果共识只有两个阶段，节点 A 中某个序号 n 对应的消息 m 收到了来自其他 2f 个节点的 prepare 消息，进入 prepared 状态，因为没有commit 阶段，节点 A 直接执行 m 并向 client 发送 REPLY。此时网络中只有少数节点跟 A 一样对消息 m 达到 prepared 状态，其他节点仍然在等待足够的 prepare 消息。这时节点们发现已经很久没有收到主节点发来的消息了，所以他们将本地的消息打包，广播了一条 view change 消息。根据原文，view change 过程中节点是无法接收到与 view change 无关的消息的，自然也无法接收prepare。当新视图中主节点准备生成 V 时，它或许可以发现某个序号 n 只在少数节点中达到了 prepared 状态，它可以选择忽略，将其他 2f+1 个节点的 P 打包进 V 里，由此生成 O 并广播。这样新视图里 n 对应的将是一个空消息。当然主节点可以做得更绝，它可以发出 &lt;n，m’&gt; 的 pre-prepare 消息，这样不明就里的其他节点就会被欺骗，最后对 m’ 达成共识。到这一步可以发现，如果没有 commit 阶段，两个视图中 n 可能对应不同的消息 m 和 m’，这就违背了 PBFT 设计的原则，因为 PBFT 要求即使是跨视图，n 与 m 的匹配也应该是唯一的。      PBFT 中 prepare 阶段的算法效果 ( prepared, local view )在 prepare 阶段，在算法正常进行时，每一个合法的节点都将接收到至少2/3的投票数，我们将“一个节点接收到至少2/3的投票数”称为事件 A，显然至少2/3节点都发生了事件 A，但是节点之间不知道彼此之间是否发生了事件 A；        PBFT 中 commit 阶段的算法效果 ( commit-local, local view )在 commit 阶段，每一个节点都会将自己发生了事件 A 的消息广播给其他节点，并收集其他节点关于事件 A 的广播，我们把“收集到至少2/3节点广播事件 A ”称作事件 B，此时，每一个节点都知道至少有2/3节点都发生了事件 A，但是不确定其他节点是否知道事件 B。        PBFT 中 reply 阶段的算法效果 ( commited, global-view )        在 reply 阶段，每一个节点都将事件 B 返回给 client，此时 client 只要收集到至少2/3节点的事件 B 的广播时，就可以判断，系统已经对该 proposal 形成共识，因为事件 B 表示“至少2/3节点都知道有至少2/3节点对proposal进行了投票”，将“收集到至少2/3节点的事件B的广播”称作事件C。  Raft 与 PBFT 的对比            对比点      Raft      PBFT                  适用环境      私链      联盟链              算法通信复杂度      O(N)      O(N^2)              容错      崩溃容错，故障节点 f &lt;= (n-1) / 2      拜占庭容错，拜占庭节点 f &lt;= (n-1) / 3              流程对比      1.  leader 选举（谁快谁当）2. 共识过程 3. 重选 leader机制      1.  leader 选举（按编号轮询）2.共识过程 3. 重选 leader 机制      复杂度分析：      对于 Raft 算法，核心共识过程是日志复制这个过程，这个过程分两个阶段，一个是日志记录，一个是提交数据。    两个过程都只需要领导者发送消息给跟随者节点，跟随者节点返回消息给领导者节点即可完成，跟随者节点之间是无需沟通的。    所以如果集群总节点数为 n，日志记录阶段通信次数为 n-1，提交数据阶段通信次数也为 n-1，总通信次数为 2n-2，因此 Raft 算法复杂度为 O(N) 。        对于 PBFT 算法，核心过程有三个阶段，分别是 pre-prepare （预准备）阶段，prepare （准备）阶段和 commit （提交）阶段。    对于 pre-prepare 阶段，主节点广播 pre-prepare 消息给其它节点即可，因此通信次数为 n-1 ；    对于 prepare 阶段，每个节点如果同意请求后，都需要向其它节点再 广播 prepare 消息，所以总的通信次数为 n*(n-1)，即 n^2-n ；    对于 commit 阶段，每个节点如果达到 prepared 状态后，都需要向其它节点广播 commit 消息，所以总的通信次数也为 n*(n-1) ，即 n^2-n 。    所以总通信次数为 (n-1) + (n^2-n) + (n^2-n) ，即 2n^2-n-1 ，因此 PBFT 算法复杂度为 O(N^2) 。  流程的对比上，对于 leader 选举这块， Raft 算法本质是谁快谁当选，而 PBFT 算法是按编号依次轮流做主节点。对于共识过程和重选 leader 机制这块，为了更形象的描述这两个算法，接下来会把 Raft 和 PBFT 的共识过程比喻成一个团队是如何执行命令的过程，从这个角度去理解 Raft 算法和 PBFT 的区别。一个团队一定会有一个老大和普通成员。对于 Raft 算法，共识过程就是：只要老大还没挂，老大说什么，我们（团队普通成员）就做什么，坚决执行。那什么时候重新老大呢？只有当老大挂了才重选老大。对于 PBFT 算法，共识过程就是：老大向我发送命令时，当我认为老大的命令是有问题时，我会拒绝执行。就算我认为老大的命令是对的，我还会问下团队的其它成员老大的命令是否是对的，只有大多数人 （2f+1） 都认为老大的命令是对的时候，我才会去执行命令。那什么时候重选老大呢？老大挂了当然要重选，如果大多数人都认为老大不称职或者有问题时，我们也会重新选择老大。"
  },
  
  {
    "title": "区块链隐私计算行业分析",
    "url": "/posts/privacy/",
    "categories": "Blockchain",
    "tags": "blockchain, privacy",
    "date": "2022-05-01 12:00:00 +0800",
    





    
    "snippet": "独立思考来自于更多维度的了解信息，本文是对当前区块链生态中隐私问题的概括性介绍，希望给读者一个宏观视角来观察当前加密行业在隐私方面的进展，区块链及其衍生出来周边生态也被熟知为智能合约、去中心化App、Web3.0等，为了统一术语消除歧义，本文用公链一词指代。本文第一部分介绍为什么隐私问题是未来公链大规模应用的一大障碍，以及对当前公链隐私问题的两个思考维度。第二部分探索调研当前业界实现公链隐私...",
    "content": "独立思考来自于更多维度的了解信息，本文是对当前区块链生态中隐私问题的概括性介绍，希望给读者一个宏观视角来观察当前加密行业在隐私方面的进展，区块链及其衍生出来周边生态也被熟知为智能合约、去中心化App、Web3.0等，为了统一术语消除歧义，本文用公链一词指代。本文第一部分介绍为什么隐私问题是未来公链大规模应用的一大障碍，以及对当前公链隐私问题的两个思考维度。第二部分探索调研当前业界实现公链隐私保护的三种主要技术途径：链下零知识证明计算、链上混币交易实现匿名性、秘密分享与同态加密等前沿密码学技术实现链上隐私。最后，展望未来公链隐私方面的发展趋势，希望本文可以为中文去中心化社区的发展做出一份贡献。为什么公链需要隐私？隐私是公链走入大众的一大障碍现代金融系统是一个运行良好的体系，广大民众每天都在使用并且不需要担心资金安全与交易隐私，因为保证用户的财产安全与交易隐私是银行等金融机构的基本义务。当你用银行卡结算买车时，逢年过节给亲戚好友发红包时，在手头不宽裕的时候向银行小额贷款时，完全不会担心这些交易细节被公之于众，在现代银行和信用卡结算网络中，用户的交易数据只存在于相关金融机构与监管部门内部。在理想的状况下不会泄露这些数据，并且监管部门还可以用这些数据追踪不法分子的交易往来，保障社会秩序与金融运转。然而在当前大多数你所听说过的区块链项目中，情况不是这样。即便区块链技术以去中心化、不可篡改性、安全程度高等特性广为人知，然而当前区块链跟现代金融系统比，它甚至不能满足支付、交易等活动中最基本的隐私需求，链上的一切交易都是公开可查询的。可能你听说过区块链是由各种密码学算法组成的分布式系统，这没错，但请不要误以为区块链会保护你的交易数据不被别人看到。这听起来有点违反常理，但切切实实存在。用比特币买披萨的记录会被永远的记录在比特币网络中，可以被任何人查询[1]到。同样的道理，在以太坊上参与的一切去中心化活动，以及所有账户的余额，都可以用Etherscan[2]这个网站轻松查到。这个问题在区块链技术诞生之初就一直存在，即便近年来发展出一些关注隐私的公链项目，但大都因为在功能性和易用性方面缺乏好的设计，在去中心化社区中并没有得到广泛应用。这一切都表明克服公链隐私问题势在必行，隐私成为公链步入大众，进一步广泛应用的一大障碍。公链隐私问题的两个思考维度在一切开始之前，首先要明确的问题是，当谈及公链中的隐私时，我们在谈什么，业界喜欢准确且轮廓清晰的问题描述。接下来从公链的隐私概念以及隐私所在的架构位置这两个维度展开探讨。维度1，匿名性与机密性。宽泛的说，在现代金融交易系统中隐私有两方面：匿名性和机密性。试想一个非盈利的公益组织收到匿名捐款，如果从捐款交易中除了金额以外不能获得任何与捐款人账户有关的信息，这被称为匿名性。另一种场景，当购买商品时别人不知道你购买了什么商品以及你花了多少钱，别人只知道你在购买商品，这被称为机密性。考虑到当今流行大多数区块链支持智能合约，这类链不单是一个账本，而且是一个支持图灵完备编程语言的去中心化状态机。综上所述，可以把思考区隐私的第一个维度一般化的概括为：•  匿名性：别人是否知道交易的发起方/合约的调用方。•  机密性：别人是否知道交易的内容/合约的状态与参数。维度2，链上计算与中介计算。区块链以及诸多加密货币的基本论点之一是能够在没有可信第三方情况下直接进行金融活动。以比特币为代表的加密货币都允许在没有可信第三方的情况下直接支付，以太坊等支持智能合约的区块链进一步使得除了支付以外的通用合约能够在无需可信第三方的情况下安全执行。在本文中，使用术语“链上”来描述那些用户直接作为共识节点参与活动的行为，如支付、交易、借贷等。链上的一些例子包括直接使用比特币[3]或ZCash[4]支付，以及去中心化交易所Uniswap[5]上的交易，合约中抵押资产的杠杆贷款等。但需要注意的是，成为共识节点对计算机的算力、存储、带宽等有较高要求，而用户一般是持有个人计算机或者移动设备的普通人，他们通常不会运行共识节点，而是通过Web3钱包与区块链交互，在这种情况下钱包的RPC提供商就被视为中介。下面这个事实会让很多人感到惊讶，在宣称去中心化的公链生态中，中介无处不在。例如，用户要想参与数字货币交易，需要把资产存入Coinbase[6]交易所，此时中心化交易所就扮演中介的角色，此外还有宣称是去中心化的交易所项目，但其实它们也需要中心化的中介，例如dYdX[7]和DiversiFi[8]都存在中心化的运营商，相比于Coinbase[6]这样的交易所，只是采取链下撮合链上结算的方式，使用STARKs[9]解决了资金安全的信任问题，但其本质上仍然是中介。注意Uniswap[5]链上交易所与上述交易所不同，它的用户不需要与区块链共识网络之外的服务器发生交互。除了交易所之外，当前公链生态中还有另外两类中介，第一类被称为Payment Channel网络，典型的就是比特币闪电支付[10]，另外一类被称为roll-ups[11]，包括Arbitrum[12]，Optimism[13]，Aztec[14]和Scroll[15]等。需要额外说明一点，这些中介可以是中心化的，但也可以被设计成去中心化的，比如roll-ups[11]的设计目标就是成为去中心化的侧链，以此来增加主链的可扩展性，还有zk-roll-ups，社区目前在探讨是否可以通过特别设计的共识机制[16]，来使其成为真正去中心化的Layer2。上述公链生态的两种表现形式，为隐私提供了第二个思考维度。• 链上隐私：交易/合约执行信息是否被永久记录在链上。• 中介隐私：交易/合约执行信息是否披露给代表用户在链上发起交易的第三方实体，例如中心化交易所Coinbase[6]或Layer2中zk-roll-ups节点等。总结一下，现在有了思考公链隐私的两个维度，第一个维度是匿名性与机密性，第二个维度是链上隐私与链下隐私，把这两个维度结合起来，可以把隐私问题细分得到下面待解决的四个子问题：• 链上匿名性：例如根据用户在链上交易所Uniswap中的交易信息，是否可以查询到用户在链上进行的其他活动，比如NFT交易。• 链上机密性：例如用户在链上的交易金额以及执行合约的输入数据是否是公开可查询的。• 链下匿名性：交易所运营商（Coinbase[6]、dYdX[7]等）是否能组织特定用户的交易，比如近期出现的封禁俄罗斯用户交易事件。• 链下机密性：交易所运营商（Coinbase[6]、dYdX[7]等）是否能查看并记录交易内容的具体信息。上述所有问题的解决途径将在接下来探讨。此刻可以确定的是，在存在中介的公链生态中，技术上来看都是这样的情况：用户的数据经由中介后公开的记录在链上。请读者理解这一点后把它记在心中，接下来探讨采用不同的技术，在链上和中介分别可以实现什么样的隐私。这里是一段短的声明：虽然加密行业中多数项目宣称是去中心化的，但链本身及其生态的设计与开发往往是中心化的，例如公链和合约的代码都是开源的，但实际上除了项目开发者，任何其他人都不会仔细阅读这些代码，读者试想你是否知道以太坊数据结构的实现方法，是否了解Uniswap[6]流动性资金池的恒值函数。此外，矿工们为了追求利润，世界上越来越多共识节点集中到几个大型的矿池，这进一步削弱了区块链的去中心化特性，还有链上合约漏洞导致数百万美元损失的新闻屡见不鲜。但本文不会探讨这些问题，主要关注公链生态中的隐私问题，另外本文也不会涉及对隐私与监管、合规关系的探讨，因为一般情况下金融机构不仅有了解客户（Know Your Customer）的监管义务，而且在反洗钱（AML）的要求下，还需要有保存和向有关部门披露交易内容的法律义务，因此隐私与监管、合规的关系是一个大话题，本文仅仅从理论技术层面探讨公链的隐私问题。当前公链如何实现隐私？从技术角度来看支付是最容易实现隐私的功能，以资金盘较大的两条隐私链Zcash[4]和Monero[17]为例，它们分别使用零知识证明数和环签名技术提供链上的匿名性与机密性。此外还有相对更高效的链上隐私支付方案Mimblewimble[18]，不过它的缺点是需要多轮的交互，引入了较大的攻击面。或者是生成不固定地址的方案，例如Quisquis[19]、Anonymous[20]和Zether[21]等。总的来说，链上支付的隐私问题不难解决。然而业界对于除支付外的其他去中心化活动，即智能合约隐私化的技术路线并不明确，例如DeFi领域的绝大多数去中心化应用，目前没有被广泛使用的隐私保护方案。为了完整的介绍链上计算的隐私保护方案，首先要形式化描述链上计算的抽象模型。一切的链上计算都可以抽象成状态机，通过执行函数在不同状态间转移，把它记为函数f(s_t,input)=(s_(t+1),output)，其中input是调用合约的输入，output是合约运行结果，合约计算过程可以用下面这张图表示。可以看出来，对链上计算过程的实施隐私保护，其实就是在不泄露调用者身份和输入的情况下执行智能合约。概括来讲当前业界的解决方案可以分为三大类。• 把计算过程转移到链下，使用零知识证明验证计算的正确性。• 链上计算保持公开透明但隐私调用者的身份。• 使用同态加密算法保护输入和链上状态，只解密输出结果。通过零知识证明实现链下计算隐私零知识证明是上世纪八十年代密码学与理论计算机界重要的科研成果，这篇笔记[22]系统性的概括了零知识证明技术的发展过程，还有这篇预印文[23]介绍了目前被广泛使用的zk-SNARKs背后的数学原理，本文不包括对零知识证明算法的介绍。注意一点，当前区块链中使用的零知识证明技术在学界被称为非交互式的，简称NIZK（Non-Interactive Zero Knowledge），当证明数据的长度与输入数据长度成线性关系时，即空间复杂度为时，被称为简洁证明算法，这两个特性连起来即zk-SNARKs。此外，当前业界还在关注的一些其他特性，例如初始化过程的公开性，即这种证明算法可以在不可信的网络中执行可信的初始化过程；还有初始化过程的全局性，即只需要全局初始化一次，就可以证明任何的图灵完备程序。零知识证明技术在区块链中主要被用于验证链下计算的正确性，例如roll-ups方案中，链下的节点充当中介，它扮演证明算法中的证明者，链上的验证节点扮演证明算法中验证者，可以在不泄露链下合约状态与用户输入的前提下，验证计算执行的正确性，链上的验证节点在验证通过后再更新链上状态。这种方法最早是有学界Zexe[26]提出的，受此启发，业界中如Mina zkApps[24]和Aleo[25]都是采用上述方法。需要注意的是，上述方法看似完美但仍然存在问题，首先计算过程对证明算法中的证明者是公开的，因此无法保护链下计算的隐私，此外在链上缺乏锁定机制的情况下，还可能会导致条件竞争。由于新的状态来自于用户调用合约时刻的状态，这意味着如果有多个用户在缺乏协调机制的情况下，对链上同一个合约状态进行修改，会导致并发条件竞争的问题。举一个具体的例子，假如链上有状态，用户通过链下计算得到状态，接下来是验证过程，注意还没有真正的更新到链上，此刻用户调用合约，把链上状态从转移到并更新到链上，接着用户的新状态被验证结束，验证节点把更新到链上。可以看到这个过程中状态覆写导致用户的更新无效。现在业界的方法是避免状态共享，通过区分不同用户在链上的状态来解决条件竞争问题，这种方法非常适合用户间不存在状态共享的应用场景，例如ZKPass[27]。但对于生态中绝大多数需要共享链上状态的场景，这种方法并不适用，尤其是DeFi场景。此外，在这种技术途径下，负责链下计算的节点或者Layer2/侧链实际上充当了中介的角色，状态对于中介是公开的，在一些场景中，比如基于恒值函数做市的去中心化交易所，恶意的套利者可以通过公开在中介上的交易信息实现front-running攻击。在不涉及本文后面讨论的其他方法前提下，目前没有已知的方法可以对这些中介隐藏信息。综上所述，使用零知识证明的这种途径可以实现链上计算的匿名性和机密性，但不能实现链下或者说中介计算的机密性。公开透明的链上信息也需要隐私从技术角度讲，既然不依赖中介为链上生态添加机密性保护比较困难，那么是否可以只为链上生态添加匿名性。答案是的，例如CoinJoin[28]和Tornado Cash[29]通过混币交易的思想来达成这一目的，这类方法的思路是把多个实际交易合并成一个包含有多个发起者和接受者的大交易，用户通过不断的生成新地址来实现匿名性，但很明显这种匿名性的保障比较弱，已经有观点表示这些链上的一些交易可以被追踪。再回到零知识证明的技术途径，Aztec Connect[30]是以太坊上即将上线的链上”zkzk-rollup“隐私解决方案，它可以被理解成是在主链与zk-rollup链之间的零知识证明跨链方案，这样可以使得链下的zk-rollup合约与链上合约交互式计算，为用户提供了匿名性保护。FLAX[31]是近期的预印文，它通过重新设计合约虚拟机来提供匿名性，主要思想是为代币标准ERC20添加匿名特性，因此在这个链上实现新的支持匿名性的代币门槛很低，开发者甚至不需要了解密码学就可以完成，而且这类代币的特性跟上面所提到的ZCash、Monero、Zether特性基本一致。基于前沿密码实现链上计算隐私能不能用一种方法，把链上状态以及用户输入都加密，只解密链上计算的输出结果。这样便可以在无需中介的前提下，同时实现链上的匿名性与隐私性。本节介绍用来实现这种特性的一些方法，这些方法大体上可以被分为两类，基于可信硬件执行环境的，以及基于若干前沿密码学技术的。一句话概括，这种方法支持状态转移函数在密态内容上执行，用户把状态和输入加密后保存到区块链，只把链上计算的结果公开。这样可以支持很多隐私保护的Defi场景，例如支付、借贷、交易所等，例如在隐私保护的去中心化交易所中，除了把代币价格和流动池可用量公开，其他所有信息都保持加密。首先是使用可信硬件执行环境的方法，目前被业界广泛使用的是Intel SGX[32]硬件，它可以被理解成处理器中的一块区域，在这块区域中的代码和内存都是保密且不可被篡改的，理想状态下，即便是持有该设备的最高权限管理员用户也无法查看或是更改篡改的内容。这一技术途径首先由学界Ekiden提出，接着Oasis[33]和Secret Network[34]这两个公链落地实现，目前具有隐私保护特性的去中心化交易所Secret Swap[35]正是运行在Secret Network[34]链上。但这种技术途径的缺点也比较明显，其隐私性的前提是假设可信硬件执行环境是安全，一旦可信硬件被攻破或者被生产商植入后门，隐私性将不复存在，不幸的是Intel SGX已经被多次发现漏洞[36]，并且硬件漏洞相比软件漏洞更难以修复。另外是使用密码学算法在密态内容上计算的方法。简单介绍下门限秘密分享的概念，的秘密分享就是将秘密分享给个参与方，在允许合谋的前提下，任何不超过个参与方都不能得到秘密的任何信息，至少要个参与方合作才可以恢复出秘密。文献[37]给了一种在共识节点动态加入和退出的情况下，维护门限秘密分享的解决方案，此外文献[38]进一步提升了门限秘密分享在大规模共识网络中的性能。通过把密钥或者签名私钥秘密分享到共识网络，可以构建隐私保护的去中心化交易所[39]，以及简洁轻客户端[40]。ElGamal或Pailliar是当前业界广泛使用的两种加法同态算法，把它们的密钥作为秘密，分享到去中心化网络，可以支持链上的隐私加法运算，据此可以实现基于恒值函数的去中心化的交易所，例如Penumbra[41]正是使用ElGamal的加法同态特性，以及秘密分享的门限特性。通过各秘密分享参与方合作恢复密钥，解密公开特定的信息。此外还有研究使用差分隐私[42]实现基于恒值函数的隐私交易所。最后，全同态加密是实现更通用的链上隐私计算的一项重要手段，自从2009年Genry[43]之后，全同态加密取得了很大的进展，这包括理论算法进步和工程硬件加速，全同态加密落地实践只是时间问题。当然只依靠全同态加密并不能解决当前公链隐私问题，比如NuCypher[44]和smartFHE[45]使用全同态加密技术途径，但解密密钥由中心化节点保存的，这显然削弱了区块链的去中心化特性。因此，若在区块链中使用全同态加密，如何在共识网络中管理密钥是无法绕过的一个话题。和上述方法一样，文献[46]提出使用门限秘密分享算法把密钥分享到共识网络，各个节点在密态内容与密态输入上执行计算，之后共识节点通过交互合作恢复出密钥，解密获得输出结果。总结隐私问题是公链走向大规模应用路上的一大障碍，当谈及公链隐私时，往往有两个思考维度，从隐私的特性来看有匿名性和机密性，从所处的位置来看有链上隐私和中介（链下）隐私。目前业界主要有三种实现公链隐私的技术途径。首先，使用零知识证明技术，尤其是zk-STARKs和zk-SNARKs这两种常被使用的算法框架把计算转移到链下，这种技术途径可以在链上同时实现匿名性与机密性。但需要注意在这种方式中，链下计算的隐私是缺乏保护的，例如Mina zkApps[24]和Aleo[25]。其次，可以在不依赖链下中介且不对区块链架构大改的情况下，为现有的链上生态添加匿名性（不包括机密性），例如CoinJoin，Tornado Cash，Aztec Connect，和FLAX。最后，本文用“基于前沿密码实现链上计算隐私”这一不太正式的称谓，来概括性介绍基于可信执行环境、门限秘密分享、同态加密技术等实现隐私的方法，这一类方案在无需中介的前提下实现了的链上匿名性与机密性。例如使用可信硬件执行环境的Oasis[33]和Secret Network[34]，还有FLAX在功能性、易用性与链上隐私之间上谋得一种平衡，提供了一种新思路。此外使用门限秘密分享实现隐私公链，Shamir门限秘密分享方案可以看做是一种门限加法同态加密方案，基于此可以实现支持加法计算链上的匿名性和机密性隐私，例如隐私去中心化交易所Penumbra[41]，近年来学界持续提出的新门限全同态加密方案，可以考虑用来实现支持通用计算的链上匿名性与机密性。在上述三种技术途径中，虽然目前业界对零知识证明实现隐私方案投入了最大的关注，但实际上第二、三种技术途径也同样需要关注，考虑到第三种技术途径中门限全同态加密技术带来的隐私效果是最佳的，因此业界应该推动对门限全同态加密的长期学术研究和工程开发工作。当然这几类技术途径并不是相互排斥的，它们甚至可以同时出现在系统中，比如公开的主链支持链上计算、零知识证明实现隐私的链下计算、以及门限全同态的另外一条链支持隐私链上计算。参考[1]. https://www.blockchain.com/btc/tx/cca7507897abc89628f450e8b1e0c6fca4ec3f7b34cccf55f3f531c659ff4d79[2].  https://etherscan.io/[3].  https://bitcoin.org/en/[4].  https://z.cash/[5].  https://uniswap.org/[6].  https://www.coinbase.com/[7].  https://dydx.exchange/[8].  https://deversifi.com/[9].  https://starkware.co/[10]. https://lightning.network/[11]. https://vitalik.ca/general/2021/01/05/rollup.html[12]. https://developer.offchainlabs.com/docs/rollup_basics[13]. https://www.optimism.io/[14]. https://aztec.network/[15]. https://scroll.io/[16].https://ethresear.ch/t/proof-of-efficiency-a-new-consensus-mechanism-for-zk-rollups/11988[17]. https://www.getmonero.org/[18]. https://eprint.iacr.org/2018/1039[19]. https://eprint.iacr.org/2018/990[20]. https://eprint.iacr.org/2020/293[21]. https://eprint.iacr.org/2019/191[22]. https://cseweb.ucsd.edu/~mihir/cse208-Wi20/main.pdf[23]. https://arxiv.org/abs/1906.07221[24]. https://docs.minaprotocol.com/en/snapps/how-snapps-work[25]. https://www.aleo.org/[26]. https://eprint.iacr.org/2018/962[27].https://minacrypto.com/2022/02/03/snapps-mina-club-interview-with-zkpass-co-founders/[28]. https://en.bitcoin.it/wiki/CoinJoin[29]. https://tornado.cash/[30].https://medium.com/aztec-protocol/private-defi-with-the-aztec-connect-bridge-76c3da76d982[31]. https://eprint.iacr.org/2021/1249[32].https://www.intel.com/content/www/us/en/developer/tools/software-guard-extensions/overview.html[33]. https://www.oasislabs.com/[34]. https://scrt.network/[35]. https://secretswap.net/[36]. https://arxiv.org/pdf/2006.13598.pdf[37]. https://eprint.iacr.org/2021/339[38]. https://eprint.iacr.org/2020/464[39]. https://anoma.network/blog/ferveo-a-distributed-key-generation-scheme-for-front-running-protection/[40]. https://eprint.iacr.org/2022/087[41]. https://penumbra.zone/[42]. https://eprint.iacr.org/2021/1101[43]. https://dl.acm.org/doi/abs/10.1145/1536414.1536440[44]. https://www.nucypher.com/[45]. https://eprint.iacr.org/2021/133[46]. https://eprint.iacr.org/2017/257"
  },
  
  {
    "title": "权益证明机制下以太坊中一笔交易的完整流程",
    "url": "/posts/002/",
    "categories": "Blockchain, Ethereum",
    "tags": "ethereum",
    "date": "2022-04-16 20:00:00 +0800",
    





    
    "snippet": "      用户使用私钥创建并签署交易。通常由钱包或库（如 ether.js、web3js、web3py 等）处理，但在底层，用户使用以太坊 JSON-RPC API 向节点发出请求。用户定义愿意支付给验证者作为小费的 gas 量，以鼓励其将交易包含在区块中。小费将支付给验证者，而基础费用（Base Fee）将被销毁。    交易提交给以太坊执行客户端（Execution Client），该...",
    "content": "      用户使用私钥创建并签署交易。通常由钱包或库（如 ether.js、web3js、web3py 等）处理，但在底层，用户使用以太坊 JSON-RPC API 向节点发出请求。用户定义愿意支付给验证者作为小费的 gas 量，以鼓励其将交易包含在区块中。小费将支付给验证者，而基础费用（Base Fee）将被销毁。    交易提交给以太坊执行客户端（Execution Client），该客户端验证其有效性，即确保发送者有足够的 ETH 来完成交易，并且使用正确的密钥签署了交易。      如果交易有效，执行客户端将该交易添加到本地内存池（待处理交易列表）中，并通过执行层的 Gossip 网络将交易广播给其他节点。当其他节点侦听到交易时，也将其添加到本地内存池中。高级用户可能会不广播他们的交易，而是将其转发给专门的区块构建器，如 Flashbots Auction。这使他们能够组织新区块中的交易，以实现最大利润（Maximal Extractable Value，MEV）。        网络上的一个节点是当前时隙（Slot）的区块提议者（Block Proposer），之前已经使用 RANDAO 伪随机地选出。这个节点负责构建和广播要添加到以太坊区块链的下一个区块，并更新全局状态。该节点由三部分组成：执行客户端（Execution Client）、共识客户端（Consensus Client）和验证者客户端（Validator Client）。执行客户端将本地内存池中的交易打包成一个“执行负载（Execution Payload）”，并在本地执行这些交易以生成状态变化。这些信息传递给共识客户端，其中执行负载被包装为“信标区块（Beacon Block）”的一部分，该区块还包含有关奖励、惩罚、削减、证明等信息，这些信息使网络能够就链头的区块顺序达成一致。          在工作量证明中，生成区块的时间是由挖矿难度决定的，而在权益证明中，节奏是固定的。 权益证明以太坊中的时间分为 Slot 时隙（12 秒）和 Epoch 时段（32 个时隙）。 在每个时隙中随机选择一位验证者作为区块提议者。 该验证者负责创建新区块并发送给网络上的其他节点。 另外在每个时隙中，都会随机选择一个验证者委员会，通过他们的投票确定所提议区块的有效性。        其他节点在共识层 Gossip 网络上接收新的信标区块，将其传递给自己的执行客户端并重新在本地执行交易，以确保提议的状态变化是有效的。验证者客户端随后证明该区块是有效的，且是其链视图中的逻辑下一区块（意味着它建立在根据分叉选择（Fork Choice）规则定义的具有最大证明权重的链上）。该区块被添加到每个证明过它的节点的本地数据库中。  如果交易已经成为链的一部分，且该链两个检查点之间具有“超级多数链接（Supermajority Link）”，那么可以认为交易“最终确定（Finalized）”，即该交易不能被还原。检查点发生在每个时段的开始，要有超级多数链接，两个检查点必须都被网络上 ETH 总质押量的 66% 证明。            优点      缺点                  质押使个人更容易参与其中保障网络的安全，促进去中心化。验证者节点可以在普通笔记本电脑上运行。质押池让用户可以在没有 32 个 ETH 的情况下质押。      与工作量证明相比，权益证明仍处于起步阶段，并且经过的实践检验较少。              权益质押更加去中心化。规模经济不像适用于工作量证明挖矿那样适用于权益证明。      实现权益证明比实现工作量证明更加复杂。              权益证明的加密经济安全性高于工作量证明。      用户需要运行三种软件才能参与以太坊的权益证明。              发行较少的新 ETH 就可以激励网络参与者。             "
  },
  
  {
    "title": "工作量证明机制下以太坊中一笔交易的完整流程",
    "url": "/posts/001/",
    "categories": "Blockchain, Ethereum",
    "tags": "ethereum",
    "date": "2022-04-14 12:00:00 +0800",
    





    
    "snippet": "      用户编写并使用账户私钥签署交易请求。        用户通过节点将自己的交易请求广播到整个以太坊网络。        每个接收到新交易请求的以太坊网络节点会将此交易添加到其本地的内存池。内存池主要用来存储该节点已收到的、还未被添加到区块链的区块中被承认的所有交易请求。        在某一时间点，挖矿节点以特定方式将几十或上百个交易请求汇总到一个潜在区块中，该方式使节点在不超出区...",
    "content": "      用户编写并使用账户私钥签署交易请求。        用户通过节点将自己的交易请求广播到整个以太坊网络。        每个接收到新交易请求的以太坊网络节点会将此交易添加到其本地的内存池。内存池主要用来存储该节点已收到的、还未被添加到区块链的区块中被承认的所有交易请求。        在某一时间点，挖矿节点以特定方式将几十或上百个交易请求汇总到一个潜在区块中，该方式使节点在不超出区块燃料限制 (block gas limit) 的情况下，尽可能多地收取交易手续费。    之后，挖矿节点将：          验证每个交易请求的有效性（例如：是否有人试图从未经其签名的帐户中转出以太币、请求是否有格式错误等），然后执行交易请求的代码，并改变其本地 EVM 副本的状态。 矿工将每个此类交易请求的交易费用奖励到自己的账户。      一旦在本地 EVM 副本上验证并执行了块中的所有交易请求，就开始为潜在的块生成工作量证明“合法性证书 (certificate of legitimacy)”。            最终，矿工将完成对区块的证书生成，该区块中就包含了我们特定的交易请求。 然后，矿工广播完成的区块，其中包括证书以及新 EVM 状态的校验和 (checksum)。        其他节点接收到该新区块。 他们将验证证书，执行区块中所有的交易（也包括原本由我们的用户所广播的交易），并验证所有交易执行后其新 EVM 状态的校验和是否与矿工区块声明的状态校验和相匹配。 只有这样，这些节点才会将这个块附加到他们区块链的尾部，并接受新的 EVM 状态作为规范状态。        每个节点从存放未完成交易请求的本地内存池中，删除该新区块中的所有交易。        加入网络的新节点按顺序下载所有区块，包括包含我们交易的区块。新节点初始化本地 EVM 副本（以空白状态的 EVM 开始），然后在本地 EVM 副本上执行每个块中的每笔交易，并校验各块的状态校验和。  以上就是从用户发布交易到该交易被成功执行上链的完整流程。每笔交易都只会被挖出（首次被包含在新区块中并被传播）一次，但在推进规范 EVM 状态的过程中，每个参与者都会执行和验证该交易。 这凸显出区块链的核心准则之一：别信任，去验证。"
  }
  
]

